{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 540x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_row = 2\n",
    "num_col = 5\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(10):\n",
    "    ax = axes[i//num_col, i%num_col]\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title('Label: {}'.format(y_train[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(28, 28))  # shape of input\n",
    "z = Flatten()(x)  # 28x28 -> 784\n",
    "z = Dense(units=128, activation='relu')(z)  # dense + ReLU\n",
    "p = Dense(units=10, activation='softmax')(z)  # dense + softmax\n",
    "\n",
    "model = Model(\n",
    "    inputs=x,\n",
    "    outputs=p,\n",
    ")  # build DNN model\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])  # compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3),\n",
    "    ModelCheckpoint(filepath=os.path.join('models', 'DNN', 'test.h5'), save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 30us/sample - loss: 6.4412 - acc: 0.8578 - val_loss: 1.9869 - val_acc: 0.9066\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1855 - acc: 0.9255 - val_loss: 1.0894 - val_acc: 0.9208\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6034 - acc: 0.9417 - val_loss: 0.7853 - val_acc: 0.9329\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3552 - acc: 0.9549 - val_loss: 0.7053 - val_acc: 0.9372\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2473 - acc: 0.9606 - val_loss: 0.5912 - val_acc: 0.9442\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2074 - acc: 0.9645 - val_loss: 0.5562 - val_acc: 0.9467\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1664 - acc: 0.9680 - val_loss: 0.5400 - val_acc: 0.9450\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1544 - acc: 0.9705 - val_loss: 0.4911 - val_acc: 0.9483\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1525 - acc: 0.9696 - val_loss: 0.4656 - val_acc: 0.9494\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1381 - acc: 0.9726 - val_loss: 0.4334 - val_acc: 0.9477\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1471 - acc: 0.9713 - val_loss: 0.4256 - val_acc: 0.9482\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1131 - acc: 0.9738 - val_loss: 0.4210 - val_acc: 0.9493\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1235 - acc: 0.9736 - val_loss: 0.4451 - val_acc: 0.9510\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1191 - acc: 0.9742 - val_loss: 0.4137 - val_acc: 0.9511\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1210 - acc: 0.9741 - val_loss: 0.3891 - val_acc: 0.9542\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1168 - acc: 0.9764 - val_loss: 0.3765 - val_acc: 0.9557\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1129 - acc: 0.9756 - val_loss: 0.3991 - val_acc: 0.9558\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1099 - acc: 0.9762 - val_loss: 0.3823 - val_acc: 0.9552\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.0912 - acc: 0.9793 - val_loss: 0.4205 - val_acc: 0.9542\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1ca874ccd0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, callbacks=callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9566"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see accuracy\n",
    "\n",
    "accuracy_score(y_test, model.predict(x_test).argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this model is 95.66% (in the author's environment.) Pretty good!\n",
    "\n",
    "このモデルの精度は（筆者の環境では） 95.66% です。\n",
    "悪くない！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class DenseModel:\n",
    "    def __init__(self, layers=1, hid_dim=128):\n",
    "        self.input = Input(shape=(28, 28), name='input')\n",
    "        self.flatten = Flatten(name='flatten')\n",
    "        self.denses = OrderedDict()\n",
    "        for i in range(layers):\n",
    "            name = 'dense_{}'.format(i)\n",
    "            self.denses[name] = Dense(units=hid_dim, activation='relu', name=name)\n",
    "        self.last = Dense(units=10, activation='softmax', name='last')\n",
    "    \n",
    "    \n",
    "    def build(self):\n",
    "        x = self.input\n",
    "        z = self.flatten(x)\n",
    "        for dense in self.denses.values():\n",
    "            z = dense(z)\n",
    "        p = self.last(z)\n",
    "        \n",
    "        model = Model(inputs=x, outputs=p)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== layers: 1 ; hid_dim: 1 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 3.0020 - acc: 0.1119 - val_loss: 2.3024 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3024 - acc: 0.1140 - val_loss: 2.3022 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3018 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3012 - acc: 0.1140 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3022 - val_acc: 0.1060\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3022 - val_acc: 0.1060\n",
      "======== layers: 1 ; hid_dim: 2 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 2.5686 - acc: 0.1459 - val_loss: 2.1145 - val_acc: 0.1944\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0936 - acc: 0.1954 - val_loss: 2.0692 - val_acc: 0.2009\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.0566 - acc: 0.2014 - val_loss: 2.0647 - val_acc: 0.1958\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0403 - acc: 0.2044 - val_loss: 2.0469 - val_acc: 0.2036\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.0228 - acc: 0.2076 - val_loss: 2.0326 - val_acc: 0.2062\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0105 - acc: 0.2089 - val_loss: 2.0272 - val_acc: 0.2071\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0007 - acc: 0.2107 - val_loss: 2.0251 - val_acc: 0.2090\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.9949 - acc: 0.2118 - val_loss: 2.0165 - val_acc: 0.2087\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9893 - acc: 0.2134 - val_loss: 2.0116 - val_acc: 0.2118\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9844 - acc: 0.2147 - val_loss: 2.0109 - val_acc: 0.2149\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9805 - acc: 0.2164 - val_loss: 2.0016 - val_acc: 0.2167\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9749 - acc: 0.2186 - val_loss: 1.9971 - val_acc: 0.2176\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9708 - acc: 0.2231 - val_loss: 1.9923 - val_acc: 0.2225\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9578 - acc: 0.2289 - val_loss: 1.9610 - val_acc: 0.2362\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9470 - acc: 0.2350 - val_loss: 1.9423 - val_acc: 0.2442\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9273 - acc: 0.2429 - val_loss: 1.9246 - val_acc: 0.2242\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8949 - acc: 0.2578 - val_loss: 1.8711 - val_acc: 0.2660\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8310 - acc: 0.2689 - val_loss: 1.7997 - val_acc: 0.2874\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7760 - acc: 0.2761 - val_loss: 1.7580 - val_acc: 0.2860\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7548 - acc: 0.2821 - val_loss: 1.7370 - val_acc: 0.2808\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7422 - acc: 0.2824 - val_loss: 1.7321 - val_acc: 0.2837\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7341 - acc: 0.2870 - val_loss: 1.7285 - val_acc: 0.3027\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7277 - acc: 0.2934 - val_loss: 1.7269 - val_acc: 0.2961\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7211 - acc: 0.2937 - val_loss: 1.7167 - val_acc: 0.3063\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7110 - acc: 0.3036 - val_loss: 1.7038 - val_acc: 0.2903\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.7043 - acc: 0.3036 - val_loss: 1.7070 - val_acc: 0.2887\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6995 - acc: 0.3061 - val_loss: 1.7044 - val_acc: 0.2902\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6966 - acc: 0.3058 - val_loss: 1.6933 - val_acc: 0.2999\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6930 - acc: 0.3043 - val_loss: 1.6881 - val_acc: 0.3031\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6925 - acc: 0.3028 - val_loss: 1.6908 - val_acc: 0.2977\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6898 - acc: 0.3036 - val_loss: 1.6869 - val_acc: 0.3090\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6881 - acc: 0.3021 - val_loss: 1.6856 - val_acc: 0.2944\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6877 - acc: 0.3042 - val_loss: 1.6883 - val_acc: 0.3137\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6863 - acc: 0.3086 - val_loss: 1.6867 - val_acc: 0.2973\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6874 - acc: 0.3087 - val_loss: 1.6875 - val_acc: 0.3050\n",
      "======== layers: 1 ; hid_dim: 4 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 3.8407 - acc: 0.1163 - val_loss: 2.3011 - val_acc: 0.1077\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.2935 - acc: 0.1203 - val_loss: 2.2811 - val_acc: 0.1340\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.2248 - acc: 0.1545 - val_loss: 2.0778 - val_acc: 0.2104\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.9577 - acc: 0.2392 - val_loss: 1.8749 - val_acc: 0.2342\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8388 - acc: 0.2636 - val_loss: 1.7852 - val_acc: 0.2912\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7299 - acc: 0.2951 - val_loss: 1.6863 - val_acc: 0.3113\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6568 - acc: 0.3104 - val_loss: 1.6509 - val_acc: 0.3283\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6169 - acc: 0.3177 - val_loss: 1.5929 - val_acc: 0.3283\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5710 - acc: 0.3250 - val_loss: 1.5600 - val_acc: 0.3282\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5385 - acc: 0.3382 - val_loss: 1.5107 - val_acc: 0.3382\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3154 - acc: 0.4875 - val_loss: 1.2955 - val_acc: 0.4978\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2918 - acc: 0.4940 - val_loss: 1.2972 - val_acc: 0.4790\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2817 - acc: 0.4994 - val_loss: 1.2860 - val_acc: 0.4943\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2672 - acc: 0.5044 - val_loss: 1.2478 - val_acc: 0.5102\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2622 - acc: 0.5090 - val_loss: 1.2408 - val_acc: 0.5332\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2515 - acc: 0.5125 - val_loss: 1.2220 - val_acc: 0.5300\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2422 - acc: 0.5270 - val_loss: 1.2277 - val_acc: 0.5397\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2306 - acc: 0.5370 - val_loss: 1.2212 - val_acc: 0.5341\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2207 - acc: 0.5408 - val_loss: 1.2167 - val_acc: 0.5426\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2215 - acc: 0.5410 - val_loss: 1.2124 - val_acc: 0.5428\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2137 - acc: 0.5426 - val_loss: 1.2048 - val_acc: 0.5472\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2126 - acc: 0.5438 - val_loss: 1.2189 - val_acc: 0.5364\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2111 - acc: 0.5480 - val_loss: 1.1951 - val_acc: 0.5508\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2070 - acc: 0.5475 - val_loss: 1.1892 - val_acc: 0.5551\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2059 - acc: 0.5469 - val_loss: 1.1915 - val_acc: 0.5530\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2067 - acc: 0.5489 - val_loss: 1.2035 - val_acc: 0.5493\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2046 - acc: 0.5483 - val_loss: 1.1941 - val_acc: 0.5556\n",
      "======== layers: 1 ; hid_dim: 8 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 3.5598 - acc: 0.1764 - val_loss: 2.0996 - val_acc: 0.1960\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0699 - acc: 0.1987 - val_loss: 2.0624 - val_acc: 0.2028\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0350 - acc: 0.2078 - val_loss: 2.0416 - val_acc: 0.2059\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0179 - acc: 0.2094 - val_loss: 2.0361 - val_acc: 0.2093\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0043 - acc: 0.2093 - val_loss: 2.0235 - val_acc: 0.2074\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9970 - acc: 0.2125 - val_loss: 2.0207 - val_acc: 0.2077\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9914 - acc: 0.2134 - val_loss: 2.0153 - val_acc: 0.2100\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9882 - acc: 0.2149 - val_loss: 2.0172 - val_acc: 0.2121\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9803 - acc: 0.2169 - val_loss: 1.9834 - val_acc: 0.2407\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9027 - acc: 0.2582 - val_loss: 1.8581 - val_acc: 0.2905\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.8193 - acc: 0.2913 - val_loss: 1.8315 - val_acc: 0.2939\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7796 - acc: 0.3068 - val_loss: 1.7667 - val_acc: 0.3174\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7337 - acc: 0.3287 - val_loss: 1.7322 - val_acc: 0.3495\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6931 - acc: 0.3447 - val_loss: 1.6960 - val_acc: 0.3596\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6530 - acc: 0.3506 - val_loss: 1.6390 - val_acc: 0.3596\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.5641 - acc: 0.3932 - val_loss: 1.5163 - val_acc: 0.4196\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.4365 - acc: 0.4360 - val_loss: 1.4108 - val_acc: 0.4297\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3551 - acc: 0.4535 - val_loss: 1.3316 - val_acc: 0.4370\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2683 - acc: 0.4939 - val_loss: 1.1986 - val_acc: 0.5275\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1460 - acc: 0.5500 - val_loss: 1.1049 - val_acc: 0.5997\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0490 - acc: 0.5911 - val_loss: 1.0445 - val_acc: 0.6333\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9859 - acc: 0.6280 - val_loss: 0.9891 - val_acc: 0.6439\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9338 - acc: 0.6514 - val_loss: 0.9314 - val_acc: 0.6641\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8905 - acc: 0.6729 - val_loss: 0.8716 - val_acc: 0.6795\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8544 - acc: 0.6981 - val_loss: 0.8179 - val_acc: 0.7358\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7965 - acc: 0.7642 - val_loss: 0.7555 - val_acc: 0.7753\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7540 - acc: 0.7815 - val_loss: 0.7163 - val_acc: 0.7963\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.7319 - acc: 0.7889 - val_loss: 0.7098 - val_acc: 0.7955\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7174 - acc: 0.7956 - val_loss: 0.7194 - val_acc: 0.7952\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.6935 - acc: 0.8011 - val_loss: 0.7481 - val_acc: 0.7992\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6872 - acc: 0.8038 - val_loss: 0.6954 - val_acc: 0.8087\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6765 - acc: 0.8037 - val_loss: 0.7612 - val_acc: 0.8038\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6684 - acc: 0.8035 - val_loss: 0.7048 - val_acc: 0.8027\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6645 - acc: 0.8057 - val_loss: 0.6915 - val_acc: 0.8102\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6545 - acc: 0.8080 - val_loss: 0.6887 - val_acc: 0.8037\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6497 - acc: 0.8100 - val_loss: 0.6809 - val_acc: 0.8148\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6490 - acc: 0.8100 - val_loss: 0.6471 - val_acc: 0.8124\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6370 - acc: 0.8127 - val_loss: 0.6795 - val_acc: 0.8023\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6399 - acc: 0.8122 - val_loss: 0.6465 - val_acc: 0.8129\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6310 - acc: 0.8157 - val_loss: 0.6298 - val_acc: 0.8201\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6231 - acc: 0.8176 - val_loss: 0.6258 - val_acc: 0.8247\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6180 - acc: 0.8212 - val_loss: 0.6262 - val_acc: 0.8163\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6073 - acc: 0.8229 - val_loss: 0.6027 - val_acc: 0.8296\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6050 - acc: 0.8240 - val_loss: 0.6066 - val_acc: 0.8262\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6025 - acc: 0.8265 - val_loss: 0.6010 - val_acc: 0.8238\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5960 - acc: 0.8269 - val_loss: 0.6048 - val_acc: 0.8321\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5909 - acc: 0.8293 - val_loss: 0.6066 - val_acc: 0.8219\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5799 - acc: 0.8320 - val_loss: 0.6110 - val_acc: 0.8278\n",
      "======== layers: 1 ; hid_dim: 16 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 6.0332 - acc: 0.2161 - val_loss: 1.9664 - val_acc: 0.2982\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8906 - acc: 0.3119 - val_loss: 1.7723 - val_acc: 0.3521\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7051 - acc: 0.3709 - val_loss: 1.6222 - val_acc: 0.4118\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5030 - acc: 0.4545 - val_loss: 1.4167 - val_acc: 0.4813\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3084 - acc: 0.5337 - val_loss: 1.2288 - val_acc: 0.5767\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1350 - acc: 0.5900 - val_loss: 1.1197 - val_acc: 0.6071\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.0228 - acc: 0.6428 - val_loss: 1.0049 - val_acc: 0.6711\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9316 - acc: 0.6763 - val_loss: 0.9303 - val_acc: 0.7158\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8748 - acc: 0.7012 - val_loss: 0.8884 - val_acc: 0.7179\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8174 - acc: 0.7250 - val_loss: 0.8399 - val_acc: 0.7446\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7701 - acc: 0.7459 - val_loss: 0.7801 - val_acc: 0.7628\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7258 - acc: 0.7609 - val_loss: 0.7522 - val_acc: 0.7760\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7006 - acc: 0.7725 - val_loss: 0.7144 - val_acc: 0.7768\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6638 - acc: 0.7868 - val_loss: 0.6991 - val_acc: 0.8062\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6367 - acc: 0.7956 - val_loss: 0.7095 - val_acc: 0.8142\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6144 - acc: 0.8069 - val_loss: 0.6492 - val_acc: 0.8077\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5946 - acc: 0.8132 - val_loss: 0.6608 - val_acc: 0.8110\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5694 - acc: 0.8211 - val_loss: 0.6456 - val_acc: 0.8271\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5613 - acc: 0.8303 - val_loss: 0.6154 - val_acc: 0.8300\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5222 - acc: 0.8453 - val_loss: 0.5416 - val_acc: 0.8605\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4908 - acc: 0.8590 - val_loss: 0.5139 - val_acc: 0.8676\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4697 - acc: 0.8672 - val_loss: 0.4818 - val_acc: 0.8755\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4563 - acc: 0.8722 - val_loss: 0.4657 - val_acc: 0.8696\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4427 - acc: 0.8746 - val_loss: 0.4567 - val_acc: 0.8783\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4237 - acc: 0.8798 - val_loss: 0.4350 - val_acc: 0.8798\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4096 - acc: 0.8835 - val_loss: 0.4253 - val_acc: 0.8817\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3990 - acc: 0.8858 - val_loss: 0.4063 - val_acc: 0.8902\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3886 - acc: 0.8898 - val_loss: 0.3914 - val_acc: 0.8936\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3798 - acc: 0.8925 - val_loss: 0.3996 - val_acc: 0.8938\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3751 - acc: 0.8930 - val_loss: 0.3847 - val_acc: 0.8953\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3680 - acc: 0.8964 - val_loss: 0.3863 - val_acc: 0.8922\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3662 - acc: 0.8953 - val_loss: 0.3923 - val_acc: 0.8915\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3609 - acc: 0.8979 - val_loss: 0.3809 - val_acc: 0.8931\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3584 - acc: 0.8969 - val_loss: 0.3786 - val_acc: 0.8989\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3553 - acc: 0.8994 - val_loss: 0.3734 - val_acc: 0.9000\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3525 - acc: 0.9002 - val_loss: 0.3879 - val_acc: 0.8927\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3519 - acc: 0.9005 - val_loss: 0.3808 - val_acc: 0.8942\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3466 - acc: 0.9023 - val_loss: 0.3797 - val_acc: 0.8947\n",
      "======== layers: 1 ; hid_dim: 32 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 4.2829 - acc: 0.5650 - val_loss: 1.1640 - val_acc: 0.6476\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.0159 - acc: 0.7023 - val_loss: 0.8553 - val_acc: 0.7507\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7717 - acc: 0.7906 - val_loss: 0.7045 - val_acc: 0.8281\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.6300 - acc: 0.8352 - val_loss: 0.5995 - val_acc: 0.8485\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5359 - acc: 0.8618 - val_loss: 0.5442 - val_acc: 0.8738\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4715 - acc: 0.8763 - val_loss: 0.4864 - val_acc: 0.8863\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4185 - acc: 0.8909 - val_loss: 0.4707 - val_acc: 0.8962\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3852 - acc: 0.8978 - val_loss: 0.4153 - val_acc: 0.9043\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3505 - acc: 0.9075 - val_loss: 0.4214 - val_acc: 0.9053\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3317 - acc: 0.9115 - val_loss: 0.4167 - val_acc: 0.9035\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3041 - acc: 0.9156 - val_loss: 0.3841 - val_acc: 0.9143\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2923 - acc: 0.9193 - val_loss: 0.3573 - val_acc: 0.9151\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2738 - acc: 0.9243 - val_loss: 0.3505 - val_acc: 0.9179\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2589 - acc: 0.9281 - val_loss: 0.3445 - val_acc: 0.9247\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2460 - acc: 0.9297 - val_loss: 0.3328 - val_acc: 0.9226\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2315 - acc: 0.9339 - val_loss: 0.3071 - val_acc: 0.9255\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2271 - acc: 0.9349 - val_loss: 0.3059 - val_acc: 0.9288\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2148 - acc: 0.9376 - val_loss: 0.2834 - val_acc: 0.9315\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2084 - acc: 0.9395 - val_loss: 0.2990 - val_acc: 0.9322\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2069 - acc: 0.9403 - val_loss: 0.3122 - val_acc: 0.9312\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1971 - acc: 0.9424 - val_loss: 0.2739 - val_acc: 0.9318\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1945 - acc: 0.9430 - val_loss: 0.2676 - val_acc: 0.9366\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1871 - acc: 0.9453 - val_loss: 0.2566 - val_acc: 0.9376\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1784 - acc: 0.9473 - val_loss: 0.2784 - val_acc: 0.9362\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1791 - acc: 0.9474 - val_loss: 0.2763 - val_acc: 0.9375\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1748 - acc: 0.9485 - val_loss: 0.2593 - val_acc: 0.9379\n",
      "======== layers: 1 ; hid_dim: 64 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 4.2858 - acc: 0.7136 - val_loss: 0.8588 - val_acc: 0.7740\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7283 - acc: 0.8162 - val_loss: 0.6127 - val_acc: 0.8485\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5176 - acc: 0.8630 - val_loss: 0.4671 - val_acc: 0.8840\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4007 - acc: 0.8945 - val_loss: 0.4208 - val_acc: 0.8992\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3260 - acc: 0.9118 - val_loss: 0.3678 - val_acc: 0.9115\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2867 - acc: 0.9234 - val_loss: 0.3501 - val_acc: 0.9179\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2524 - acc: 0.9302 - val_loss: 0.3157 - val_acc: 0.9242\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2222 - acc: 0.9369 - val_loss: 0.3171 - val_acc: 0.9288\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2087 - acc: 0.9419 - val_loss: 0.2718 - val_acc: 0.9360\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1872 - acc: 0.9464 - val_loss: 0.2686 - val_acc: 0.9381\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1829 - acc: 0.9483 - val_loss: 0.2683 - val_acc: 0.9375\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1703 - acc: 0.9511 - val_loss: 0.2354 - val_acc: 0.9423\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1629 - acc: 0.9530 - val_loss: 0.2448 - val_acc: 0.9428\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1544 - acc: 0.9553 - val_loss: 0.2403 - val_acc: 0.9425\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1518 - acc: 0.9567 - val_loss: 0.2279 - val_acc: 0.9463\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1401 - acc: 0.9594 - val_loss: 0.2199 - val_acc: 0.9453\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1394 - acc: 0.9595 - val_loss: 0.2310 - val_acc: 0.9466\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1326 - acc: 0.9618 - val_loss: 0.2183 - val_acc: 0.9483\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1316 - acc: 0.9614 - val_loss: 0.2307 - val_acc: 0.9450\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1309 - acc: 0.9632 - val_loss: 0.2336 - val_acc: 0.9438\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1206 - acc: 0.9644 - val_loss: 0.2131 - val_acc: 0.9488\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1145 - acc: 0.9667 - val_loss: 0.2188 - val_acc: 0.9472\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1165 - acc: 0.9665 - val_loss: 0.2237 - val_acc: 0.9506\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1093 - acc: 0.9678 - val_loss: 0.2566 - val_acc: 0.9464\n",
      "======== layers: 1 ; hid_dim: 128 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 28us/sample - loss: 5.2953 - acc: 0.8579 - val_loss: 1.3413 - val_acc: 0.8985\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8298 - acc: 0.9139 - val_loss: 0.7433 - val_acc: 0.9147\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4127 - acc: 0.9316 - val_loss: 0.5431 - val_acc: 0.9222\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2766 - acc: 0.9454 - val_loss: 0.4310 - val_acc: 0.9375\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1984 - acc: 0.9542 - val_loss: 0.4178 - val_acc: 0.9392\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1686 - acc: 0.9607 - val_loss: 0.4212 - val_acc: 0.9398\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1491 - acc: 0.9617 - val_loss: 0.3836 - val_acc: 0.9437\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1317 - acc: 0.9661 - val_loss: 0.3709 - val_acc: 0.9485\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1296 - acc: 0.9669 - val_loss: 0.3588 - val_acc: 0.9486\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1286 - acc: 0.9671 - val_loss: 0.3665 - val_acc: 0.9463\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1222 - acc: 0.9694 - val_loss: 0.3946 - val_acc: 0.9467\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1105 - acc: 0.9706 - val_loss: 0.3723 - val_acc: 0.9514\n",
      "======== layers: 1 ; hid_dim: 256 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 5.4261 - acc: 0.8868 - val_loss: 1.7500 - val_acc: 0.9252\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9604 - acc: 0.9440 - val_loss: 0.9430 - val_acc: 0.9433\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4652 - acc: 0.9605 - val_loss: 0.7709 - val_acc: 0.9478\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3149 - acc: 0.9689 - val_loss: 0.7159 - val_acc: 0.9473\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2307 - acc: 0.9733 - val_loss: 0.6486 - val_acc: 0.9554\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1883 - acc: 0.9765 - val_loss: 0.5957 - val_acc: 0.9568\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1589 - acc: 0.9793 - val_loss: 0.5785 - val_acc: 0.9598\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1892 - acc: 0.9777 - val_loss: 0.5935 - val_acc: 0.9587\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1693 - acc: 0.9787 - val_loss: 0.5954 - val_acc: 0.9588\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1662 - acc: 0.9802 - val_loss: 0.5620 - val_acc: 0.9620\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1618 - acc: 0.9794 - val_loss: 0.6244 - val_acc: 0.9553\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1606 - acc: 0.9803 - val_loss: 0.5862 - val_acc: 0.9578\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1273 - acc: 0.9824 - val_loss: 0.5219 - val_acc: 0.9608\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1361 - acc: 0.9818 - val_loss: 0.5770 - val_acc: 0.9622\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1504 - acc: 0.9825 - val_loss: 0.5792 - val_acc: 0.9625\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1492 - acc: 0.9826 - val_loss: 0.6141 - val_acc: 0.9597\n",
      "======== layers: 1 ; hid_dim: 512 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 5.5408 - acc: 0.8988 - val_loss: 1.3479 - val_acc: 0.9350\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7414 - acc: 0.9537 - val_loss: 0.8034 - val_acc: 0.9499\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3527 - acc: 0.9669 - val_loss: 0.7327 - val_acc: 0.9513\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2520 - acc: 0.9735 - val_loss: 0.6399 - val_acc: 0.9554\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2019 - acc: 0.9774 - val_loss: 0.7146 - val_acc: 0.9544\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1689 - acc: 0.9795 - val_loss: 0.6373 - val_acc: 0.9594\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1606 - acc: 0.9795 - val_loss: 0.5955 - val_acc: 0.9592\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1689 - acc: 0.9805 - val_loss: 0.5227 - val_acc: 0.9618\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1647 - acc: 0.9800 - val_loss: 0.7496 - val_acc: 0.9555\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2093 - acc: 0.9781 - val_loss: 0.6205 - val_acc: 0.9624\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1936 - acc: 0.9795 - val_loss: 0.5599 - val_acc: 0.9663\n",
      "======== layers: 1 ; hid_dim: 1024 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 5.8299 - acc: 0.9073 - val_loss: 1.2545 - val_acc: 0.9454\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6687 - acc: 0.9590 - val_loss: 0.8732 - val_acc: 0.9478\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3543 - acc: 0.9698 - val_loss: 0.6837 - val_acc: 0.9576\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2206 - acc: 0.9777 - val_loss: 0.6615 - val_acc: 0.9572\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2062 - acc: 0.9777 - val_loss: 0.8086 - val_acc: 0.9499\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2167 - acc: 0.9781 - val_loss: 0.5675 - val_acc: 0.9643\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1965 - acc: 0.9797 - val_loss: 0.6654 - val_acc: 0.9643\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2143 - acc: 0.9804 - val_loss: 0.7563 - val_acc: 0.9577\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2684 - acc: 0.9771 - val_loss: 0.6955 - val_acc: 0.9603\n",
      "======== layers: 2 ; hid_dim: 1 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 2.3016 - acc: 0.1139 - val_loss: 2.3019 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "======== layers: 2 ; hid_dim: 2 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 2.9821 - acc: 0.1716 - val_loss: 2.0789 - val_acc: 0.1991\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0424 - acc: 0.2000 - val_loss: 1.9916 - val_acc: 0.2079\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9834 - acc: 0.2109 - val_loss: 1.9500 - val_acc: 0.2138\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9485 - acc: 0.2241 - val_loss: 1.9228 - val_acc: 0.2508\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9208 - acc: 0.2403 - val_loss: 1.8996 - val_acc: 0.2458\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.8988 - acc: 0.2539 - val_loss: 1.8794 - val_acc: 0.2603\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8760 - acc: 0.2656 - val_loss: 1.8556 - val_acc: 0.2460\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8562 - acc: 0.2529 - val_loss: 1.8435 - val_acc: 0.2513\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.8379 - acc: 0.2495 - val_loss: 1.8310 - val_acc: 0.2409\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.8260 - acc: 0.2508 - val_loss: 1.8178 - val_acc: 0.2578\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8175 - acc: 0.2657 - val_loss: 1.8219 - val_acc: 0.2808\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.8090 - acc: 0.2808 - val_loss: 1.7985 - val_acc: 0.2867\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.8039 - acc: 0.2894 - val_loss: 1.7946 - val_acc: 0.3007\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7997 - acc: 0.2952 - val_loss: 1.7925 - val_acc: 0.3027\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7955 - acc: 0.2987 - val_loss: 1.7904 - val_acc: 0.2975\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7926 - acc: 0.3031 - val_loss: 1.7839 - val_acc: 0.3099\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7894 - acc: 0.3069 - val_loss: 1.7886 - val_acc: 0.2973\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7872 - acc: 0.3081 - val_loss: 1.7825 - val_acc: 0.3139\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7836 - acc: 0.3139 - val_loss: 1.7785 - val_acc: 0.3174\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7806 - acc: 0.3195 - val_loss: 1.7734 - val_acc: 0.3234\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7763 - acc: 0.3216 - val_loss: 1.7702 - val_acc: 0.3259\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7717 - acc: 0.3243 - val_loss: 1.7680 - val_acc: 0.3250\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7649 - acc: 0.3215 - val_loss: 1.7543 - val_acc: 0.3270\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7587 - acc: 0.3148 - val_loss: 1.7452 - val_acc: 0.3069\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7531 - acc: 0.3133 - val_loss: 1.7432 - val_acc: 0.3185\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7470 - acc: 0.3211 - val_loss: 1.7315 - val_acc: 0.3220\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7419 - acc: 0.3286 - val_loss: 1.7267 - val_acc: 0.3196\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7363 - acc: 0.3333 - val_loss: 1.7326 - val_acc: 0.3489\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7317 - acc: 0.3419 - val_loss: 1.7222 - val_acc: 0.3368\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7242 - acc: 0.3467 - val_loss: 1.7221 - val_acc: 0.3537\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7197 - acc: 0.3510 - val_loss: 1.7059 - val_acc: 0.3516\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7127 - acc: 0.3516 - val_loss: 1.6931 - val_acc: 0.3559\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7063 - acc: 0.3448 - val_loss: 1.6903 - val_acc: 0.3715\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6980 - acc: 0.3383 - val_loss: 1.6846 - val_acc: 0.3340\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6897 - acc: 0.3394 - val_loss: 1.6676 - val_acc: 0.3376\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6808 - acc: 0.3445 - val_loss: 1.6617 - val_acc: 0.3668\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6701 - acc: 0.3709 - val_loss: 1.6564 - val_acc: 0.3621\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6602 - acc: 0.3738 - val_loss: 1.6399 - val_acc: 0.3837\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6483 - acc: 0.3798 - val_loss: 1.6373 - val_acc: 0.3804\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6425 - acc: 0.3788 - val_loss: 1.6214 - val_acc: 0.3896\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6371 - acc: 0.3835 - val_loss: 1.6205 - val_acc: 0.3912\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6309 - acc: 0.3896 - val_loss: 1.6165 - val_acc: 0.3873\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6283 - acc: 0.3887 - val_loss: 1.6076 - val_acc: 0.3988\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6261 - acc: 0.3909 - val_loss: 1.6098 - val_acc: 0.3975\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6231 - acc: 0.3934 - val_loss: 1.6021 - val_acc: 0.4070\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6209 - acc: 0.3955 - val_loss: 1.6001 - val_acc: 0.4014\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6189 - acc: 0.3953 - val_loss: 1.6027 - val_acc: 0.4098\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6164 - acc: 0.3990 - val_loss: 1.5969 - val_acc: 0.4015\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6156 - acc: 0.3985 - val_loss: 1.5977 - val_acc: 0.3949\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6136 - acc: 0.3989 - val_loss: 1.5942 - val_acc: 0.4022\n",
      "Epoch 51/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6120 - acc: 0.3984 - val_loss: 1.5958 - val_acc: 0.3950\n",
      "Epoch 52/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6107 - acc: 0.4013 - val_loss: 1.5885 - val_acc: 0.4096\n",
      "Epoch 53/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6089 - acc: 0.4023 - val_loss: 1.5877 - val_acc: 0.4171\n",
      "Epoch 54/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6101 - acc: 0.4035 - val_loss: 1.5866 - val_acc: 0.4083\n",
      "Epoch 55/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 1.6079 - acc: 0.4029 - val_loss: 1.5866 - val_acc: 0.4063\n",
      "Epoch 56/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6064 - acc: 0.4061 - val_loss: 1.5878 - val_acc: 0.4027\n",
      "Epoch 57/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6062 - acc: 0.4077 - val_loss: 1.5811 - val_acc: 0.4168\n",
      "Epoch 58/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6051 - acc: 0.4088 - val_loss: 1.5828 - val_acc: 0.4112\n",
      "Epoch 59/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6036 - acc: 0.4091 - val_loss: 1.5890 - val_acc: 0.3915\n",
      "Epoch 60/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6029 - acc: 0.4088 - val_loss: 1.5803 - val_acc: 0.4132\n",
      "Epoch 61/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6027 - acc: 0.4099 - val_loss: 1.5827 - val_acc: 0.4053\n",
      "Epoch 62/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6026 - acc: 0.4068 - val_loss: 1.5797 - val_acc: 0.4120\n",
      "Epoch 63/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6017 - acc: 0.4099 - val_loss: 1.5849 - val_acc: 0.4008\n",
      "Epoch 64/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6019 - acc: 0.4092 - val_loss: 1.5886 - val_acc: 0.3964\n",
      "Epoch 65/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6017 - acc: 0.4100 - val_loss: 1.5768 - val_acc: 0.4165\n",
      "Epoch 66/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6011 - acc: 0.4120 - val_loss: 1.5761 - val_acc: 0.4116\n",
      "Epoch 67/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6004 - acc: 0.4104 - val_loss: 1.5851 - val_acc: 0.4040\n",
      "Epoch 68/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5998 - acc: 0.4102 - val_loss: 1.5737 - val_acc: 0.4178\n",
      "Epoch 69/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6000 - acc: 0.4100 - val_loss: 1.5748 - val_acc: 0.4111\n",
      "Epoch 70/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5993 - acc: 0.4129 - val_loss: 1.5750 - val_acc: 0.4142\n",
      "Epoch 71/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5981 - acc: 0.4107 - val_loss: 1.5813 - val_acc: 0.4087\n",
      "======== layers: 2 ; hid_dim: 4 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 2.6020 - acc: 0.1608 - val_loss: 2.1237 - val_acc: 0.1911\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0367 - acc: 0.2034 - val_loss: 1.9628 - val_acc: 0.2002\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9159 - acc: 0.2068 - val_loss: 1.8653 - val_acc: 0.2214\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8372 - acc: 0.2232 - val_loss: 1.8047 - val_acc: 0.2338\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7742 - acc: 0.2424 - val_loss: 1.7358 - val_acc: 0.2587\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6910 - acc: 0.2974 - val_loss: 1.6512 - val_acc: 0.3211\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6336 - acc: 0.3366 - val_loss: 1.6043 - val_acc: 0.3490\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6008 - acc: 0.3595 - val_loss: 1.5696 - val_acc: 0.3728\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5711 - acc: 0.3789 - val_loss: 1.5472 - val_acc: 0.3944\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5365 - acc: 0.3991 - val_loss: 1.4925 - val_acc: 0.4026\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.4909 - acc: 0.4201 - val_loss: 1.4644 - val_acc: 0.4409\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.4622 - acc: 0.4369 - val_loss: 1.4289 - val_acc: 0.4408\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.4272 - acc: 0.4536 - val_loss: 1.4048 - val_acc: 0.4703\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3994 - acc: 0.4748 - val_loss: 1.3671 - val_acc: 0.4771\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3664 - acc: 0.4915 - val_loss: 1.3458 - val_acc: 0.5019\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3388 - acc: 0.5074 - val_loss: 1.3167 - val_acc: 0.5134\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.2881 - acc: 0.5284 - val_loss: 1.2046 - val_acc: 0.5655\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1533 - acc: 0.5925 - val_loss: 1.0932 - val_acc: 0.6060\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0659 - acc: 0.6237 - val_loss: 1.0058 - val_acc: 0.6415\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0269 - acc: 0.6387 - val_loss: 0.9794 - val_acc: 0.6615\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0052 - acc: 0.6475 - val_loss: 0.9649 - val_acc: 0.6640\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9910 - acc: 0.6518 - val_loss: 0.9754 - val_acc: 0.6514\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9777 - acc: 0.6579 - val_loss: 0.9647 - val_acc: 0.6627\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9700 - acc: 0.6595 - val_loss: 0.9604 - val_acc: 0.6714\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9624 - acc: 0.6631 - val_loss: 0.9414 - val_acc: 0.6745\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9612 - acc: 0.6629 - val_loss: 0.9402 - val_acc: 0.6702\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9544 - acc: 0.6639 - val_loss: 0.9394 - val_acc: 0.6746\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9467 - acc: 0.6691 - val_loss: 0.9259 - val_acc: 0.6773\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9452 - acc: 0.6683 - val_loss: 0.9273 - val_acc: 0.6746\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9426 - acc: 0.6691 - val_loss: 0.9387 - val_acc: 0.6712\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9400 - acc: 0.6675 - val_loss: 0.9299 - val_acc: 0.6824\n",
      "======== layers: 2 ; hid_dim: 8 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 3.3771 - acc: 0.1156 - val_loss: 2.2353 - val_acc: 0.1469\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.1702 - acc: 0.1685 - val_loss: 2.1281 - val_acc: 0.1895\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0699 - acc: 0.2002 - val_loss: 2.0517 - val_acc: 0.2073\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0199 - acc: 0.2103 - val_loss: 2.0336 - val_acc: 0.2088\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0004 - acc: 0.2126 - val_loss: 2.0195 - val_acc: 0.2112\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9924 - acc: 0.2138 - val_loss: 2.0121 - val_acc: 0.2096\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9873 - acc: 0.2146 - val_loss: 2.0089 - val_acc: 0.2112\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9832 - acc: 0.2188 - val_loss: 1.9983 - val_acc: 0.2194\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9728 - acc: 0.2218 - val_loss: 1.9506 - val_acc: 0.2438\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.2867 - acc: 0.4920 - val_loss: 1.2576 - val_acc: 0.5204\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.2406 - acc: 0.5075 - val_loss: 1.2252 - val_acc: 0.5178\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.2041 - acc: 0.5200 - val_loss: 1.1876 - val_acc: 0.5190\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1790 - acc: 0.5315 - val_loss: 1.1799 - val_acc: 0.5367\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1626 - acc: 0.5346 - val_loss: 1.1534 - val_acc: 0.5404\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1496 - acc: 0.5397 - val_loss: 1.1374 - val_acc: 0.5532\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1379 - acc: 0.5465 - val_loss: 1.1188 - val_acc: 0.5607\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1272 - acc: 0.5519 - val_loss: 1.0927 - val_acc: 0.5704\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1190 - acc: 0.5569 - val_loss: 1.0992 - val_acc: 0.5686\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1142 - acc: 0.5603 - val_loss: 1.1047 - val_acc: 0.5665\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1011 - acc: 0.5715 - val_loss: 1.0716 - val_acc: 0.5898\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0586 - acc: 0.5937 - val_loss: 1.0213 - val_acc: 0.6065\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0272 - acc: 0.6062 - val_loss: 0.9922 - val_acc: 0.6127\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0089 - acc: 0.6105 - val_loss: 0.9942 - val_acc: 0.6116\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0070 - acc: 0.6098 - val_loss: 0.9888 - val_acc: 0.6162\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9972 - acc: 0.6118 - val_loss: 1.0080 - val_acc: 0.6020\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9927 - acc: 0.6127 - val_loss: 0.9660 - val_acc: 0.6165\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9824 - acc: 0.6174 - val_loss: 0.9783 - val_acc: 0.6150\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9670 - acc: 0.6241 - val_loss: 0.9476 - val_acc: 0.6284\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9520 - acc: 0.6331 - val_loss: 0.9450 - val_acc: 0.6389\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9393 - acc: 0.6368 - val_loss: 0.9158 - val_acc: 0.6486\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9300 - acc: 0.6430 - val_loss: 0.9072 - val_acc: 0.6587\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9205 - acc: 0.6513 - val_loss: 0.9281 - val_acc: 0.6473\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9128 - acc: 0.6517 - val_loss: 0.9029 - val_acc: 0.6476\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9116 - acc: 0.6545 - val_loss: 0.9201 - val_acc: 0.6486\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9100 - acc: 0.6531 - val_loss: 0.8768 - val_acc: 0.6633\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9029 - acc: 0.6578 - val_loss: 0.8981 - val_acc: 0.6515\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9011 - acc: 0.6565 - val_loss: 0.8853 - val_acc: 0.6675\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.8960 - acc: 0.6611 - val_loss: 0.8772 - val_acc: 0.6655\n",
      "======== layers: 2 ; hid_dim: 16 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 3.2295 - acc: 0.3402 - val_loss: 1.3252 - val_acc: 0.5298\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1473 - acc: 0.6123 - val_loss: 0.9661 - val_acc: 0.6518\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9149 - acc: 0.6757 - val_loss: 0.8209 - val_acc: 0.6886\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8011 - acc: 0.7220 - val_loss: 0.7467 - val_acc: 0.7431\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7227 - acc: 0.7626 - val_loss: 0.6627 - val_acc: 0.7775\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6440 - acc: 0.7869 - val_loss: 0.6150 - val_acc: 0.7991\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5935 - acc: 0.8046 - val_loss: 0.5707 - val_acc: 0.8185\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5590 - acc: 0.8191 - val_loss: 0.5372 - val_acc: 0.8410\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5329 - acc: 0.8372 - val_loss: 0.5186 - val_acc: 0.8523\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5091 - acc: 0.8503 - val_loss: 0.5293 - val_acc: 0.8560\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4937 - acc: 0.8580 - val_loss: 0.5021 - val_acc: 0.8597\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4803 - acc: 0.8598 - val_loss: 0.4794 - val_acc: 0.8661\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4670 - acc: 0.8655 - val_loss: 0.4710 - val_acc: 0.8712\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4562 - acc: 0.8680 - val_loss: 0.4639 - val_acc: 0.8723\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4410 - acc: 0.8722 - val_loss: 0.4602 - val_acc: 0.8752\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4365 - acc: 0.8735 - val_loss: 0.4477 - val_acc: 0.8779\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4303 - acc: 0.8744 - val_loss: 0.4393 - val_acc: 0.8801\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4282 - acc: 0.8757 - val_loss: 0.4420 - val_acc: 0.8742\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4221 - acc: 0.8771 - val_loss: 0.4703 - val_acc: 0.8673\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4188 - acc: 0.8786 - val_loss: 0.4636 - val_acc: 0.8671\n",
      "======== layers: 2 ; hid_dim: 32 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 3.5720 - acc: 0.5170 - val_loss: 1.0512 - val_acc: 0.6832\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8601 - acc: 0.7472 - val_loss: 0.6572 - val_acc: 0.8295\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5800 - acc: 0.8444 - val_loss: 0.4963 - val_acc: 0.8689\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4583 - acc: 0.8798 - val_loss: 0.4334 - val_acc: 0.8837\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3866 - acc: 0.8949 - val_loss: 0.3733 - val_acc: 0.9026\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3484 - acc: 0.9056 - val_loss: 0.3651 - val_acc: 0.9125\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3210 - acc: 0.9134 - val_loss: 0.3271 - val_acc: 0.9191\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2921 - acc: 0.9205 - val_loss: 0.3042 - val_acc: 0.9250\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2730 - acc: 0.9245 - val_loss: 0.3186 - val_acc: 0.9158\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2613 - acc: 0.9281 - val_loss: 0.2847 - val_acc: 0.9271\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2437 - acc: 0.9330 - val_loss: 0.2894 - val_acc: 0.9285\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2353 - acc: 0.9349 - val_loss: 0.2754 - val_acc: 0.9278\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2232 - acc: 0.9369 - val_loss: 0.2697 - val_acc: 0.9303\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2180 - acc: 0.9385 - val_loss: 0.2917 - val_acc: 0.9268\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2080 - acc: 0.9402 - val_loss: 0.2672 - val_acc: 0.9336\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2006 - acc: 0.9422 - val_loss: 0.2731 - val_acc: 0.9316\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1955 - acc: 0.9430 - val_loss: 0.2610 - val_acc: 0.9354\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1872 - acc: 0.9453 - val_loss: 0.2613 - val_acc: 0.9369\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1878 - acc: 0.9452 - val_loss: 0.2463 - val_acc: 0.9373\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1832 - acc: 0.9466 - val_loss: 0.2340 - val_acc: 0.9397\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1748 - acc: 0.9485 - val_loss: 0.2304 - val_acc: 0.9425\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1679 - acc: 0.9510 - val_loss: 0.2480 - val_acc: 0.9377\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1757 - acc: 0.9480 - val_loss: 0.2425 - val_acc: 0.9376\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1684 - acc: 0.9502 - val_loss: 0.2315 - val_acc: 0.9397\n",
      "======== layers: 2 ; hid_dim: 64 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 3.5661 - acc: 0.7906 - val_loss: 0.8820 - val_acc: 0.8651\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6094 - acc: 0.8849 - val_loss: 0.4988 - val_acc: 0.8972\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3816 - acc: 0.9110 - val_loss: 0.3947 - val_acc: 0.9167\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2935 - acc: 0.9289 - val_loss: 0.3971 - val_acc: 0.9179\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2497 - acc: 0.9362 - val_loss: 0.3697 - val_acc: 0.9255\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2282 - acc: 0.9428 - val_loss: 0.3377 - val_acc: 0.9320\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1955 - acc: 0.9503 - val_loss: 0.3141 - val_acc: 0.9357\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1921 - acc: 0.9505 - val_loss: 0.2966 - val_acc: 0.9404\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1686 - acc: 0.9562 - val_loss: 0.2956 - val_acc: 0.9419\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1636 - acc: 0.9570 - val_loss: 0.2804 - val_acc: 0.9468\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1544 - acc: 0.9597 - val_loss: 0.3446 - val_acc: 0.9405\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1596 - acc: 0.9596 - val_loss: 0.2970 - val_acc: 0.9423\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1537 - acc: 0.9608 - val_loss: 0.3006 - val_acc: 0.9501\n",
      "======== layers: 2 ; hid_dim: 128 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 31us/sample - loss: 4.3217 - acc: 0.8311 - val_loss: 1.0452 - val_acc: 0.8902\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6384 - acc: 0.9193 - val_loss: 0.6228 - val_acc: 0.9196\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3902 - acc: 0.9369 - val_loss: 0.4960 - val_acc: 0.9345\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2726 - acc: 0.9502 - val_loss: 0.4657 - val_acc: 0.9394\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2258 - acc: 0.9563 - val_loss: 0.4286 - val_acc: 0.9421\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1831 - acc: 0.9634 - val_loss: 0.4042 - val_acc: 0.9454\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1727 - acc: 0.9647 - val_loss: 0.4377 - val_acc: 0.9431\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1602 - acc: 0.9678 - val_loss: 0.3973 - val_acc: 0.9532\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1629 - acc: 0.9673 - val_loss: 0.3858 - val_acc: 0.9484\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1512 - acc: 0.9690 - val_loss: 0.4337 - val_acc: 0.9476\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1417 - acc: 0.9725 - val_loss: 0.3886 - val_acc: 0.9529\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1431 - acc: 0.9716 - val_loss: 0.3509 - val_acc: 0.9583\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1166 - acc: 0.9767 - val_loss: 0.3786 - val_acc: 0.9546\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1109 - acc: 0.9778 - val_loss: 0.3956 - val_acc: 0.9530\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1122 - acc: 0.9769 - val_loss: 0.3863 - val_acc: 0.9563\n",
      "======== layers: 2 ; hid_dim: 256 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 2.6508 - acc: 0.8821 - val_loss: 0.8029 - val_acc: 0.9211\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4698 - acc: 0.9426 - val_loss: 0.6508 - val_acc: 0.9298\n",
      "Epoch 3/100\n",
      "42496/48000 [=========================>....] - ETA: 0s - loss: 0.2786 - acc: 0.9592"
     ]
    }
   ],
   "source": [
    "dim_hidden_layres = [2**i for i in range(11)]\n",
    "n_layers = range(1, 4)\n",
    "\n",
    "df_accuracy = pd.DataFrame()\n",
    "\n",
    "for layers in n_layers:\n",
    "    for hid_dim in dim_hidden_layres:\n",
    "        print('========', 'layers:', layers, '; hid_dim:', hid_dim, '========')\n",
    "        model = DenseModel(layers=layers, hid_dim=hid_dim)\n",
    "        model = model.build()\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=3),\n",
    "            ModelCheckpoint(filepath=os.path.join('models', 'DNN', 'model_{}_{}.h5'.format(layers, hid_dim)), save_best_only=True),\n",
    "        ]\n",
    "        model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, callbacks=callbacks, validation_split=0.2)\n",
    "        acc = accuracy_score(y_test, model.predict(x_test).argmax(axis=-1))\n",
    "        \n",
    "        df_accuracy = pd.concat([df_accuracy, pd.DataFrame([[layers, hid_dim, acc]], columns=['layers', 'hid_dim', 'accuracy'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"11\" halign=\"left\">accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hid_dim</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>16</th>\n",
       "      <th>32</th>\n",
       "      <th>64</th>\n",
       "      <th>128</th>\n",
       "      <th>256</th>\n",
       "      <th>512</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.3104</td>\n",
       "      <td>0.5589</td>\n",
       "      <td>0.8267</td>\n",
       "      <td>0.8932</td>\n",
       "      <td>0.9389</td>\n",
       "      <td>0.9472</td>\n",
       "      <td>0.9476</td>\n",
       "      <td>0.9598</td>\n",
       "      <td>0.9638</td>\n",
       "      <td>0.9582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.4070</td>\n",
       "      <td>0.6832</td>\n",
       "      <td>0.6570</td>\n",
       "      <td>0.8609</td>\n",
       "      <td>0.9418</td>\n",
       "      <td>0.9483</td>\n",
       "      <td>0.9573</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>0.9662</td>\n",
       "      <td>0.9645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.6095</td>\n",
       "      <td>0.7897</td>\n",
       "      <td>0.7957</td>\n",
       "      <td>0.9321</td>\n",
       "      <td>0.9489</td>\n",
       "      <td>0.9487</td>\n",
       "      <td>0.9540</td>\n",
       "      <td>0.9691</td>\n",
       "      <td>0.9622</td>\n",
       "      <td>0.9658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy                                                          \\\n",
       "hid_dim     1       2       4       8       16      32      64      128    \n",
       "layers                                                                     \n",
       "1         0.1135  0.3104  0.5589  0.8267  0.8932  0.9389  0.9472  0.9476   \n",
       "2         0.1135  0.4070  0.6832  0.6570  0.8609  0.9418  0.9483  0.9573   \n",
       "3         0.1135  0.6095  0.7897  0.7957  0.9321  0.9489  0.9487  0.9540   \n",
       "\n",
       "                                 \n",
       "hid_dim    256     512     1024  \n",
       "layers                           \n",
       "1        0.9598  0.9638  0.9582  \n",
       "2        0.9690  0.9662  0.9645  \n",
       "3        0.9691  0.9622  0.9658  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accuracy.set_index(['layers', 'hid_dim']).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
