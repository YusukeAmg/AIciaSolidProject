{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAhQAAAEFCAYAAAC7GH5GAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nOy9aXCd2Xnf+Tt3Be6+Yt8IEmA3yVaTvalb3T3aIzszKatKI5dVyUSZ2KMvk0oylcUa16ScySQpVT5kMqlUUtHEjh0nNZYrTmyVLFtS2mq11K3e2N0iRbK5gdhx9xV3X975AJzTFyS44QK47wXPr+oWgcu7nPf945z3eZ/zLMIwDDQajUaj0Wi6wdLrAWg0Go1Go+l/tEGh0Wg0Go2ma7RBodFoNBqNpmu0QaHRaDQajaZrtEGh0Wg0Go2ma7RBodFoNBqNpmuOvEEhhHhVCPFrh/1ezYOjNeoPtE79gdapPziKOvWNQSGEWBRCfK7X47gbQoi/JoRoCSE2Ox6f6vW4DhOzawQghPjfhBAxIUReCPHbQghnr8d02PSDThIhxJ8LIQwhhK3XYzlszK6TEOKMEOJ7QoiUEOKRLWjUBzo5hRD/txBiXQiRFUL8ayGE/SC+q28Mij7hp4ZheDoer/Z6QJqPEEJ8Afg68FlgBpgF/s9ejklzd4QQfxl45AyJPqIB/AHwq70eiOaefB14BjgDzANPAf/HQXxR3xsUQoigEOI7QojktvX1HSHExG0vOy6EeHv7rvSPhRChjvc/L4R4QwiRE0L87FHzKhwGJtLoq8BvGYZxyTCMLPB/AX9tj5915DCRTggh/MBvAn9/r59xVDGLToZhXDUM47eAS10czpHFLDoBfwn4l4ZhZAzDSAL/Evjre/yse9L3BgVbx/DvgWlgCqgA/+q21/xVtk7gGNBk64QihBgH/gT4x0AI+LvAHwohord/iRBialvYqXuM5dy2+++aEOIfPIpu2rtgFo1OAz/r+P1nwLAQIrzH4zpqmEUngH8K/Bsg1s0BHVHMpJPm7phFJ7H96Px9Ytto318Mw+iLB7AIfO4BXncWyHb8/irwjY7fTwF1wAr8OvB7t73/e8BXO977aw84vlngGFt/RE8Al4H/vdfnTWu04303gV/o+N0OGMBMr8+d1mnH+54BPmBru2NmWyNbr8+b1umu339i61LS+3Omddr1e/8x8DoQBUaAt7bn1Oh+n4u+91AIIVxCiH8rhFgSQhSA14CAEMLa8bKVjp+X2LqQRNiyHL+8bd3lhBA54CVg9GHHYRjGgmEYtwzDaBuGcRH4R8D/uNfjOkqYRSNgE/B1/C5/Lu7hs44cZtBJCGEB/jXwtwzDaHZzPEcVM+ikuT8m0umfAO+zZaS/AfwRW/EviT181j3pe4MC+DvASeDjhmH4gP9u+/lOF89kx89TbJ3MFFti/p5hGIGOh9swjG/sw7iM28bwKGMWjS4BT3b8/iQQNwwjvYfPOoqYQScfWx6KbwkhYsA728+vCiFefsjPOqqYQSfN/TGFToZhVAzD+BuGYYwbhjELpIHzhmG09nJQ96LfDAq7EGKg42EDvGztTeW2A1p+c5f3/RUhxCkhhIstz8F/3j6Z/xH4S0KILwghrNuf+aldAmfuixDiF4UQw9s/Pwb8A+CP93ic/YxpNQL+A/Cr298TZCvS+Xf2cpBHALPqlGdrP/ns9uMvbj//NFuu2kcNs+qE2GIAcGz/PiAewTTsbcys07gQYmxbr+fZujbtNpau6TeD4rtsCSQf/xD4F8AgW1bdm8Cf7fK+32PrwhEDBoC/CWAYxgrwS8BvAEm2rMK/xy7nRWwFvmyKuwe+fBa4IIQobY/zv7AVWPaoYVqNDMP4M+CfAT9ky724xAFNrD7AlDoZW8TkY/uzYMuTVN/rwfYxptRpm+ntMcksjwpw9SGP76hgZp2Os7XVUQJ+F/i6YRjf38Mx3hexHbSh0Wg0Go1Gs2f6zUOh0Wg0Go3GhGiDQqPRaDQaTddog0Kj0Wg0Gk3XdGVQCCF+QQhxVQhxQwjx9f0alGZ/0Tr1B1qn/kDr1B9onQ6fPQdlbhfnuAZ8HlhlK1/8K4ZhXN6/4Wm6RevUH2id+gOtU3+gdeoN3fSaeA64YRjGAoAQ4vfZSnO5q2DiEW5xuxcMw9iPwlhapwNG69Qf9EInrdFDkzIM445+FXtA63Sw7KpTN1se4+wsG7q6/dwOhBBfE0K8K4R4t4vv0uwdrVN/oHXqD+6rk9aoK5b26XO0TgfLrjp146HYzdq/w8ozDOObwDdBW4E9QuvUH2id+oP76qQ1MgVapx7QjYdilZ11yCeA9e6GozkAtE79gdapP9A69Qdapx7QjUHxDjAnhDgmhHAAvwJ8e3+GpdlHtE79gdapP9A69Qdapx6w5y0PwzCaQoi/wVaPdivw24ZhXLrP2zSHjNapP9A69Qdap/5A69QbDrWXh96nejj2KSr9odE6PRxap/6gFzppjR6a84ZhPHPYX6p1emh21UlXytRoNBqNRtM12qDQaDQajUbTNd2kjWo0PcFutzMwMIDNZsPv9+P3+7Hb7QQCAQYGBqhWq+RyORqNBvl8nnw+T7PZpFqt0mg0ej18jUajOZJog0LTd7jdboaGhnC5XJw5c4aTJ08SCAQ4ffo0IyMjxGIxLl26RC6X4+rVq1y8eJFSqUQymSSfz/d6+BqNRnMk0QbFXbBYLAixM4ZL/i6EoN1u0263d7y289FJ52vb7TaHGQh7lJDn1eFw4PV68Xg8RKNRJiYmCIVCnDhxgvHxcTweD4VCAY/HQzqdxuv1IoQgl8v1+Ag0D4LU2WKxqLnVbrdptVoAev4cIkIILJatnXGpRyeGYdBqtdT6prV5tHnkDAp5wbdarVit1l1fY7VaGRkZIRqNYrVa1evtdjsulwshBEtLSywuLmK1Wjl27BgjIyO4XC6Gh4cZHBzEMAza7TbNZpO1tTVisRibm5ssLS2RzWYP+aj7H7vdjsfjweFwcOrUKV5++WUCgQDT09NMTEzgcrnweDwAeDweZmdnKZfLeL1eJicnSaVSfP/73yeVSvX4SDT3wmq1EggEcLvdhEIhHn/8cUKhEAsLC1y4cIFyuUylUqFarfZ6qI8Eco653W7GxsaYmJjAZrMpoy+ZTPL++++TTqcplUrkcjllXGgePR4pg0IaExaLBYfDgd1uv8ObAFt3wHNzc5w5cwabzYbD4cBiseDxeAiHw1gsFn784x+Ty+VwOp18/OMf59y5c0QiET72sY8RDodptVo0m01qtRpvvfUW77//PvF4nGKxqA2KPWC32wmFQrjdbs6ePcuXvvQlIpEIg4ODDA4OKqMPwOv1Mj8/j2EYzM/PUyqV2NjY4Pr167z33nv6LsrEWK1WIpEIIyMjnDhxgl/+5V/m2LFj/Lf/9t/IZDIkk0kMw9AGxSERCoU4d+4cIyMjPPvss3ziE5/A6XSqtfTKlSv8u3/377hy5QrxeJzNzU3q9Xqvh63pEX1vUOy2zWCxWLBarcpF1/mzfHg8HgYGBnb9TLvdzsjICENDQ8ozYbVacblcBINBhBAEAgFCoRAOh4NwOEw4HCYUCuH3+/F6vTQaDTWxbLa+P809Q2rncrkIh8P4/X4ikYja8pCGIaC8QvJ9hmHgdDqxWCwUi0UGBwdxOBy0Wi1ardYjY1jIOdA5VwzDoNlsqm0Es2CxWLDb7cpQdDqdOJ1OHA6H+lvY7SZAs38IIdT59nq9RCIRIpEIwWAQn8+n5hRsxTPJ9fFuHl/No0PfX+kcDgeDg4NqwZTeh0gkgtvtxu12Ew6HGRgYwG63Y7fbcTgcTE9PE41Gd12cLBaLmkCd+7jygtVqtTh37pxa6J599lnm5+dxOp3YbDZqtRqbm5tks1lKpRLLy8ssLi6SyWSoVCo9OEv9idVqVV6JqakpPv/5zzM1NcWxY8cIBoM7FjZ5gWw2mwA74lucTicul4tIJMLk5CTVapVMJkOtVtthhBxVBgcHiUajOJ1O9fdfr9fZ2NggnU73eng7sFqthMNhJiYm8Pv9FAoF1tbWyGazpjN+jioDAwMcO3aMUCjEqVOneOmllxgZGWF4eFgZ6u12+46181Ex0DV3p+8NCpvNhsvlwmazqTuxwcFBJiYmCAaDhMNhZmZmcLlc6m7H5XJx+vRppqen7/q5uxka5XKZYrFIvV6n2Wyq73388ceZnJzEMAzq9TqNRoNyuUwmk6FYLJJIJIjFYuTzeWq12kGejiOFxWLB7XYTiUSYnp7mhRde4OTJkypeojNATMaryAAxaVjI9FKn00kgEGBoaIhCoUCpVKLRaBx5YwLA6XQSiURwuVwMDAwwODhIrVajUCiYzqDo3Fp0u92USiWEEGxubmqD4pBwOByMjo4yMTHB/Pw8p0+fZnh4WK2xEsMw1ONRmEea+9OXBoX0QlitVkZHRzl+/Li6cFitVgYGBhgZGcHr9eLz+RgdHVUeCofDoRbVh3WdVqtV1tfXKZVKpNNpUqkUNpuNW7duUS6XabVaVKtVWq0W+XyeZDKp9u/z+Tybm5u6DsI9kO54qdPg4CAzMzNMTk4yMzODz+fD4XDsuoUkjbl6vU65XCabzdJsNhkZGWF0dBS73c7k5CRPPPEEuVwOm81GLpdTnqSjfLGSAa1er1cZY+VyGYfD0euh3YHUf2BgQN0ASM+fvJjpLY+DRW47DQwMqHVWbjdpDga73Y7T6cRqteJ2uxkYGMBisajrmozl6/zbr9Vq5HI5yuWy8pwbhoHD4VBze3NzU12TarXagRt+fWlQOBwOotEoLpeLF154gS9+8YsEg0F1IqWbWwohhZJbInJP/kGRrrxEIsGPf/xjEokE1WqVcrmMEIKLFy/idDppNBoUi0UajQaVSkUFKC0vL7OxsaGCNDV3YrFY1EVDepZCoRB/4S/8BZ555hl8Ph/T09P4fL5d09darRaFQoHNzU1WVlY4f/485XKZT3ziE4TDYTweD5/73Od45plnWF9f50c/+hHr6+vcvHmT8+fPH+mtqMHBQaampgiHw/h8PgKBAIVCgZs3b/Z6aDuQgbUej4dQKITP5yMYDOJ2u3G5XDs0l1uQmv1HeomCwSBer1cZ8dqQOxiEEPh8PnVNm5+fZ3x8HLfbzfj4OC6XC6/XSygU2hGnkslkeP3111lcXKRWqynDIhKJMDw8TKPR4Pr166ytrVEqlYjH4we+zvWlQSG3NdxuNyMjI5w+fZpIJKIs6oPAMAzK5TIbGxusr6/vCLqU1Ot18vk8jUaDWq1GtVql2WySTqd1QaX70JnKOzg4qC4mk5OTKj7F6/XeNTOn1WpRr9epVqvk83lWV1cpFAo8/vjjGIaBzWZjfHyc8fFxfD4ft27dAiCdTh/5YDKHw4Hb7cbv9xMIBFRskLzjMcOFuTNgVN4dy/ksbw40h8NuHorOOdf592KGv51+RwiB0+lUXsTR0VFmZmYIBALMzs6qm4ChoaEd8yAejxOLxdS1plgs0mq1GB8fZ3Jyknq9TiaToVAoABzKOteXs7TValEul4GtuIZqtUqtVlNuoYeh2WyqrYjO4jlerxe32w18VIyqWCyytLTE0tLSDhdT57gqlYoKDmw0GupCp7k3wWCQ+fl5PB4Pk5OTTE9P4/f7mZ2dxe12q+2su2Gz2fD5fNjtdhKJhLoIVSoVEokEbrcbj8eDy+XCarUqg+X2O9+jiMvlYnp6mrGxMdPeZdpsNux2O16vV+3dCyFoNBpkMhmy2Sy5XI5isaiCaTX7hxBCrXnDw8PMzc3x+OOPMzo6qtbUarWqjPYbN26wsbHBysoKN2/eJJVKUSwWdSzFQ+ByuQiFQgwMDHDy5ElOnz6Nx+Ph2LFjjI6OqmDqwcFBAFVDR3reG40GMzMzeDwe5RU3DENlIEpDw+VyEY/HSSQSbG5uHugx9aVBIY2Aer1OsVhUxW6cTudDf1atViORSFAqlajX69RqNSwWCxMTE+rzms0mhmGQyWS4cuUK169fB+60zjuDlDp/15Ps/oyMjPCZz3yGsbEx5ubmeOyxx9RdqtThXhd+Waei3W6TTCYZHBykWCyyubnJ8vIyHo+HqakpXC4XDoeDUChEs9nE7/cfeYPC7/fz+OOPMzMzQyqVIh6P93pId+BwOJSbfW5ujmeeeYZcLselS5fIZDLE43FSqRTZbFbPpwPAYrEQCoXU3e1TTz3FuXPnVLyZrP2RzWZJpVL80R/9Ea+//jrlcplEIkGlUlE3UJoHw+v1Mjc3RyAQ4KWXXuLTn/608iTKAopyGziVSrG6ukqz2WRwcFDFBJ45cwan07njuiM9vZVKBZfLxdjYGDdu3ODy5cskk8kDPaa+NChkuVfZ8KlcLlMul9UFCD4qdy3vfHZLcZKBfIVCgUKhoKxvi8Wi6hx05r1XKhVlvGj2B3l+BwYGCAaDKuc9Go2q+hLwUY0J+a+8qHR6LjrrjMjX1Wo1isUigAqI7ax8ers79ygig7tcLtddt4x6iax7IL1IMmZCBjrXajWVWaUvWPuLjCuTgbvBYJBAIIDP58Pj8WC325XB3Wg0KJVKFItF0uk0sVhMBUFLY0J7ju5P55oXCARUHSNZqE9uNclstXa7TalUIpvN0mg0cLlc6nU+n08ZH7evZUKIHZ93GDdOfWlQyAtFs9lkeXmZ1157jWAwyMTEBKOjozSbTQqFAtVqlZGREc6cOaPKMsv3FwoFyuUyy8vL/Nmf/RnLy8sqS8BisTA2Nsbw8DBut5vp6WmCwSCrq6t6+2Ifkd1CBwcHmZyc5Pjx40xNTRGJRO7446/VaqTTaRUjkUgkEEJw4sQJjh07BqBiVuT/x2IxLBYLlUqFYDCo7sB267dylOnsNWNGLBYLx48f59lnn2V4eJjJyUmEEGoeZzIZSqWS9kwcAG63WwW+Pv/88zz77LOEQiEmJiZUpoHspbK2tsZ7771HIpFgaWmJXC6ntnR1j6J70xkfJNe8U6dO8fnPf56RkRGmp6fxer1YLBZyuRyVSoV8Ps+1a9fI5XLE43EWFxdpNBoq+SAcDvPiiy9y/PhxPB4PkUhkh5e+3W6Ty+VUvZnDyDC8r0EhhPht4H8AEoZhnNl+LgR8C5gBFoFfNgzj0OpJS8+CEIK1tTXefvttvF6vurjU63XW19cpFAqcPn2a2dnZHQZFq9VSJbAXFhZ45ZVXuHTpEs1mUxkU0WhUZRq8+OKLzMzMqGBMM2JGne6HzWYjEAjg9XrVpJqamtrVmq7X66RSKRVwee3aNWWBz8zMYBgGtVptR32FeDxOrVYjm80SjUY5d+4csHt11cOiVzr18pjvh8ViYXp6mk996lOEw2FGRkaAra3GYrG4IzXusOjH+bQXZP+hYDDIk08+yWc/+1kVFN0Zj2YYBrFYjA8++IB0Os3a2hrFYrHnRkS/6NRpUMiKyvPz87z44otqzZMF52TtopWVFf78z/+c1dVVEomEMijklsbExISKG4tGowQCgTsMikKhQCKRODSD4kF8IL8D/MJtz30deMUwjDngle3fDx15Ecnn82SzWRKJBOvr62xsbKggFLnvms/nVf3/drtNsVhU/1cul5V3otFo7Ej7LBQKxONx1tbWSKVSpjUoMLFOtyNTeT0eDyMjIyoIqTPnWmYfyKqjuVyOtbU1lpeXVXCRbEaUTCaJx+MsLS1x8+ZNtdjJyOdcLqe2tGDrAjY4OKjc6zJO45CyPX6HQ9BJLl6ymqvcGjJrvIjValVl1KUOMshZ5tIf8sXrd+iT+dQNDoeDQCCgtjlkuXPpPm+321SrVbXVIYNjTRQY+zuYXCe5veH3+5X3Z3Z2ltHRUVUcsd1uq8KJ8Xic5eVl1tfXVYagjBlsNBpqC1PGt8i4sM5qzvLmWNbZ2dzcVMX+DpL7eigMw3hNCDFz29O/BHxq++ffBV4Ffn0fx/XAZLNZrl69is1mY2lpCZ/PR6vVYnNzU12MRkdHGRsbY3p6mmPHjlGr1bh69SoffPABq6urZDIZ6vW6miDS4KjVamQyGXK5nAryk/vxZsPsOkksFgs+nw+fz8fY2Bhf/OIXeeKJJwiHwwwPDzMwMKDuohuNhppUS0tLfOc732F5eZmBgQE8Hg9Op5P3339flTi/fPkysViMZDLJjRs3KJfLykDJ5XJks1kMw1BejeHhYVZXV5mcnMTpdJLL5Q68xflh6WSz2VQWy9DQkOoxIxs7mQ2r1YrT6WRgYEAZFNVqlZWVFa5du0Y8Hj+UBVHSL/OpWyKRCE888QRDQ0OqzkunUV8ul4nFYhSLRS5fvsx7772nYs7MQD/oZLVa1fVnaGiIz3/+8yoYc2hoCLvdTjKZJJlMkslk+N73vse7775LuVxWhRQ7t+PltuDExASPPfYY8/PzO4pZVatVqtUqqVSKq1evcv78eVWj56DZawzFsGEYGwCGYWwIIYb2cUwPhTx5sFXoQwaz1Go1Wq0Wfr+flZUV2u02fr9fBXQmk0lu3bpFIpFQwV+dSAGBfu4OahqdOpHBRNFolNOnT/Pcc8+pu+nbe3NIl93KygoXLlzgxo0bDA8Pc+zYMVwuF+vr69TrdXK5HG+//TYrKyvUajVKpdIOTe12u0qrkhdbr9erGo7JwjA9qsuw7zrJ4m0yaKuzpoAZ6QyS7QwCzOVyZkpJNOV82itCCJUFMDQ0RDAYVHETEqlBPp9XntpSqdTDUT8QptJJNpOU7d/Pnj3Lxz72MfX/8nqVTqdJJBJ8+OGHvPfee6qeUec6JouODQ0NMTw8zNDQEJFIZMf3yYrN5XJZZYd0Xs8OkgMPyhRCfA342kF/D3zUz6EzfVNmg8hoWcMwVAOiqakpVVXzUeegdZI1QhwOh4qXmJycVMWqAHUHKruByupuq6urxONxFXRZLpdJJpM4nU4qlYratpK9UuTfwD2OVfV98fv9qtdLpVIhlUqZxZW7Kw+qk4xPiUQihEKhOy7WZkBqID0T0oUrjZ52u60yrxqNhql16eQw17y90FlEzu/3MzExwdDQEF6vF/goi84wDPL5PIuLi6RSKVKp1JHKsjlonfx+v6p+eebMGZ544gmi0Sg+nw/YyhqUTQqvXr3K5cuXlVFxt6wZq9VKJBJhbm6OkZERVStJItPmFxYWSCQSJJPJB1oT94u9GhRxIcTotvU3CiTu9kLDML4JfBNACHGgRyTTym6vBSELfMj4B4fDwezsLHa7HZ/PxzvvvKPcwP2yaD0gptFJpki53W7OnDnDU089RTgcVrETMgVY7puXSiUKhQJXrlzh0qVLJBIJstkstVpNBWfKC5LVaqXdbquiYver/dGZ3z02NsYLL7xAKpVic3OzV+Wo910nh8PBxMSE6oUii4OZyaCQfQsGBwfx+/3KWyRdtzJ4Op/PK+9Sj3kgnQ5zzdsLMobIbrczPj7OuXPnGB4eVjdWnVl0a2trvPHGG6yurnL9+vV+MShModP4+Lgq/f/888/z9NNPq4q/sOVR/+CDD8hkMrz99tu88cYblEolUqmUihm6/W/eZrNx/PhxPv3pT+Pz+e7wTrTbbW7cuMGf/umfkk6nuXHjBpubm4dWD2mvq8u3ga9u//xV4I/3Zzjd0VmrQCLTmmQeuzypLpeLYDCo9gw7600cIUyjk81mY2BgALfbTSAQUFk0cptDZu7IWh+bm5tsbm6Sz+dVUKW8S5XdXOX/yzgX6R58kIkjvRSDg4MqpXQvDeP2iX3XSR6bbAjW6Z3YbaHqBTIQUwaOyqDMzjoi0sNokhoHpplP3SC9QjIwWcY0ycC+zqC+UqlEJpMhnU4feqZNF/RUJ9mXyOVyEY1GGRkZUY9IJILdblfBrplMhmQySSwWY319nVgsplKkO//eO72qXq+XSCSi+lfJbdpWq0Wj0VCJBPF4XHXpPSzdHiRt9P9jK8AlIoRYBX4T+AbwB0KIXwWWgS8f5CC7IZfLceHCBVZWVhgYGGB8fFxF9U9MTJDP55menlYXsH7tPGlWnaShNjIywrPPPksgEODcuXPMz88rN7dM8/3ggw+UAZHJZCiXyywuLhKPx3dkaewnsqBPrVbbUUjroDgsnWSTLVljwGq17qh2mM1me5E5sYNQKMSTTz5JOBxmfn4er9er/h6q1aqqJVOv1w99Tpp1Pu0VmWkgUwyfeuophoaGeOqpp1QxQEmtVuPWrVukUikuX77M4uKi6phsAqNuB2bTyev1qrYBZ8+e5bnnniMYDDI8PKzmn1zbrly5wltvvUUsFlMdq2+vNirXT6/Xq7JxhoeH8Xq9DA4Oqt4euVyO5eVlCoUCFy9e5MaNG+Tz+UNPIniQLI+v3OW/PrvPYzkQ0uk07777rkrbOXHihGo6FY1G2dzcZGZmRjX+KhQKfWlQmFEnuVcrS5m//PLLRKNRTpw4wfHjx1Xxonq9zurqKq+88gpra2uqKFWr1VL527Ja4n4j+0c0Go1DiaU5LJ3knUw4HMbr9aq4BBmolclkVDBzr4hGo7zwwgtMTExw6tQp1Uk2k8ko75NM6T6sPWCJGedTN8ggXbfbzczMDL/4i7/I3NwcQ0NDd3RerlQqXL9+nevXr3Pt2jVu3rxJMpk0ZfEqs+nk8/k4e/YsExMTPPnkk7z44os7YlOq1Spra2skk0kuXrzIa6+9xvr6utqWv90rIbd0A4EAx48fJxgMMjo6qrYG5U1QNpvlvffeIxaL8f7773P16lUqlcqhe5T6slLmwyBL9xqGQaFQUEF3w8PDKiAzGAwSjUZVRHOn61xumfSjkdFrZKyCdP/5/X78fj9utxu73a4KF1UqFdLptMpxl02g2u22coEfVFBeZ8Gno7TlJV2kna3eZeZMpVKhVqsdymIjz6vValVbizJANBQKEQwGCQaDKhjTMAwqlYr6G5DblH3iajctsry57NIsO1jKss2AamgoqzTKTpV6/bs/8hzKSpjhcBifz6eyq2TRvVKppAIv0+k0pVKJSqWyY0tPeiUcDofaipIZHYFAAI/Ho+qEyCqlxWKRTCZDJpOhWCz2TLMjb1DIPfd2u82VK1ewWCwMDQ3hdDoZGhrC4/Hw8ssvc/r0aTY2Nrh58yblcplCoaAKity4cUN1eiiyE0sAACAASURBVNM8OA6Hg0gkorpdzs3NEY1G8Xq9CCHI5/O8+uqrXL9+ncXFRS5cuKBc8TIIr7P0r17Uumdzc1OV4pUdew8KaUTYbDai0SiRSESVsg+FQkxPT6tSz4FAAIvFolzBly5dYnl5mWQyqe60zHZ33E/YbDaGh4eZmJjg+PHjjI+Pq7ovMqg5nU6TTqdZX1/n/Pnzaj7q3kX3RhaRs9lshMNhnnjiCR5//HGGh4ex2+20Wi1SqRSJRIKNjQ2++93v8uGHH5JOp8lkMjti+2Ra6MDAACMjI7z44ouMjo4SjUZVc0NZWbNer7O2tqa2OV5//XXW19eJx+M9WysfCYNCehtWV1fZ3NxkbGyM559/nkajgdvt5tSpU7RaLWKxGCMjI2r7Q1bajMVi2qDYA7JXh8/nY2hoiLGxsR1RyaVSiQsXLvDmm2+SSqVYXFw88Ivco4y8+5ftwGu12oF+n/RGSMNyZmaGQCDAU089xfj4OENDQ8zNzakYD+mJWllZ4eLFiyQSCfL5vJmr0/YNVquVYDDI+Pg4IyMjhMNhgsGg8iDJjJpYLMba2hrXr1/nypUrqhaC5t7IJpQ+n4+ZmRlOnjy5w1jL5/Osr6+ztLTEO++8w7vvvrvr58hYF6/Xq7JE5ufnCYfDjI2N7SiHXqlUVEDnzZs3uXz5MhsbGz3t+nrkDQqJdPfK6ply0sg0HtntMBqNUq1WsdlsKgJ6bW1tx3ulRanvmO/NwMAAY2Nj6u5UBhB19tyQQZgyGvkw6FHxKlPQuS97vy0eu91+R3dSu92uFsrOzBHpSZLzRt6xyc/obLbndrt3pGl3Zp3I6PdisXhHcTLNwyNre/h8PsbHx5mZmVGp2rBVuKper1Or1dSaKGPJOmv3aO6P3F6Uf/ed5cszmQy3bt1iY2ODer2uDG2v16sy4OT2iNwumZqaUvVBOov+SWQjxHg8Ti6XU9scvdwefGQMCtiy6OQEeu2110gkEkxMTPCZz3xGWe5DQ0MqhbHRaJBMJhkYGFDlf69evar6RJgkN960hEIhXnrpJebm5jh27BhOp1NNLtns5ubNmywsLKgeKgfF3WIljmL8xL2w2+2qiJc08HZDlkgPBALq3FgsFgKBAOPj46qFsjQOpHERCAQ4efKkep9cVOXWR2dZ/M7Cc53piplMhtXVVbUXrNkbQggikYjq4PvJT36S5557TqVuw9YWWCwWo1Ao8KMf/YhXX32VQqHAysrKXWshaHbSaaTb7XY1L6SR3Wg0+PnPf863v/1t1ZfI4/EwPDzMY489poy9yclJBgYGVKMvt9vN2NiY6vdxe5XbSqXCjRs3VBajnC+91OuRMihkBUYhBOvr62pBlTXSZdBS58XF7XYzOTmpcoNlm/PD7CvQrzgcDkZHR5mZmSEcDquIZJkSmM/nD70vwKNiOOyGDNSUXgN5R7XbAiQbEHk8HvU+gEAgwMjICF6vF4/Ho1ouyxx52dVVbm119mWRAaHLy8vqYiXvgDs9FNITKIPVNHtDus9DoRDRaFT1k5D1PwDVQEq65G/evKk8RPrcPxzyxkQazxJ5E7W0tKSuHQ6HA4/Hw+joqMrgmJubY3BwkOHhYRVTdHsp9M6Cja1Wi3w+TzKZJJfLqVjBXvJIGRSSVqtFOp1Wd0Q//OEPuXbtGsPDw8zMzKg9LNmJ8vjx42o7pNVqkc1mVedL6fHQk28LWc7X7XYzMTGhSj/LC1Or1SKXy7GyskIsFju0gK/Oidj5r2xvLoNBjwpywZEPwzAQQjA+Ps5zzz1HoVBgbGyMeDx+V4NCFuLp9OJ4PB6i0agqRiWNxM6Fcn19XaWlynLphUJBZfQkEgmKxSLz8/NEo1G1hSINDBkUfXsfA82DIQtXORwO5ubm+OQnP6m6XHYakjJtW9bgkRkHh5UBdJToLMRWq9WUB1Bu/T355JN86UtfUkXa2u024XCYmZkZdW0ZHh5Wxkij0VBbs9JI6WxR0Gg02NzcJB6Ps7KyYppYo0fSoJBdLGWzm1gshtfr5ezZs3zmM58hFAoxOTmp8rbPnj1Lq9ViaWkJr9dLKpXi/fffp1gsqoqNOhJ6C5vNxsjICGNjYxw/fpyJiQlGR0dVymC9XieRSHD16lVVye2wuN19axgG5XJZBd+WSqUj496VMUOdPQEsFgsnTpwgGo1Sr9dJp9N3LXxjtVpV46HOOyTprZDGodyykA28yuUyt27dUsGfsViMarXK6uqq2j+WBas+/elP89RTTxEMBpVBIYMDZd0DbVA8PDI40OVy8fTTT/PlL395RwojfOQ5qtfr5PN5stms6kjZT31TzECnMSHTbiuVCk6nU8VHfPKTn+TZZ5/dsQbJLRIZdyHnQLlcVtcTuc0h06plqmi1WlXFrD788EN1c9xrHkmDAlB79kIIUqmUagwlO4sGAgHK5bKy9uWddygUwjAMAoGASn88Sne23SLzpzuDjDoD+2RxFxl0d9CTQE7UzrbYcgGQAYDyDu0o7dfLOKByuawesjW8y+VS8SxOp3PXi0dnnQhAbU10NuBrt9tqHslFrlwuqyBbGStTrVZVKWDZs0Vuf3R6jDpLbR92IaujhPx7l91mg8EgHo9n19fKZnuyRbZJSpz3HZ3zoVwuqzo60qjweDxKg85tPnm+ZSM8GWNUqVRUnSS5HS+RCQJyzpkpC+eRNSgkjUaDbDbL5uYmFy5coFqt4vF4mJ2dVWluZ8+eZWxsDL/fz6lTpyiXy7hcLsLhMJlMhp/85Cdcvny514diGqTFLb0SVqt1R5bM6uoqly9fVhee/aazqNP09DSzs7OMjIwwOjoKbG1zyPoGFy9e5Ec/+hHpdJrFxcUj4+rd3Nzk3XffZXFxkdnZWcrlsrqwyIVNXth3Q94JZTIZ6vU6sViMzc1NFf/SbDbJ5XJks1l1VyYXORkcVqlUVFaUvPt1Op1MTk7i8/mYm5tTxZUajQbFYpFisWgK120/EwqFOHfuHNFolOnp6bvGyRiGQTwe55133iGRSLC2tqaNiT3QWesoFovx/e9/nw8//JDHH3+c559/HrfbjcPhUEHJ0ngrFovE43EqlQrxeFx58EqlEtVqlWg0ytNPP000GgVQ8zYej7O8vMzi4uKhl9a+H4+8QSFTb4QQbG5usrKygtPpVJkJY2NjjI+PMzY2pgLR2u22Ck6LxWIsLCxw5coVPRn56GIuDQppVLRaLarVKqVSiY2NDW7cuKHujA5qDHa7nYmJCZ577jmi0SjRaFRdKGWNgytXrvDmm2+qNsJHRcNSqcTFixcRQrC6uorFYlH7tBMTE9hstvseq1ysisUiV65cIR6PUywWlddBNjTqNEp221bqfN5mszE2Nsbk5CTT09PKFS+DA+Viq9k7fr+f06dPq3Xrbh1mDcMglUpx4cIFVWvnqPz9HzbSqxaPx3n99de5ePEilUqFU6dO7djWkDFCpVKJeDzO5cuX1Tp08eJFFXdUq9XU9qTT6WRgYEDd7CSTSa5du8b6+vqBrJ/d8MgbFBLpfpJpN4VCgXQ6jcvl2pGKI/ePnU4nPp+PcrmMx+PB5XKp9x+Vu9z9ptNlLtNy9/tcdWozMDDA0NAQw8PDhEIhHA6HupvOZrMqhkCO5ajt18tzK7fz5JZCu91+qDbm5XJZXXDK5TL5fF5tbzxsAJ8QQtVFkF1QYctrJD0UZtgL7jdk4J7s2SE7iMqstU5DQW71yYBMGYypYye6R3oghBDE43GuX79OKpVSQbLSsyeLUq2trVEsFtVaJNdEqafMPHQ4HFgsFrV+Sm+g2dYsbVB0IPdt6/U6i4uLKho9l8spkeUjEAjgcDhwuVwcO3aM48ePq7tvXe3xTqRbUF7QZY+A/bx4yKCl4eFhzp49SzAY5OMf/zgvvvgig4ODOJ1OlR737rvvsrKyouqKHGVDMJVK8fbbb6tW4TKW4kGRgZK1Wk0ZgzKa/WHPmcPhYGxsjLm5OUZHR3E4HBiGQTKZ5NKlS8TjcRXHpHlwbDYboVAIt9vN1NQUJ06cUFlWtxuP5XKZpaUlCoUCCwsLKihZbzV1T61WIx6Pqz4dH374oTIG5NaTnDdyC1hmbBQKBaxWK5FIBJ/Px8jICDMzM8zOzqotk85+IDJV1Exog6IDGRkPW9HPuVwOn8+ngsc6F2F5gWo2m0QiEcLhMBaLhWQy2avhm5rOioiynO9+TobOJlQej4fp6WlGRkY4fvw4s7Oz2O128vm8MmZWV1dZWFggmUya0tLfT2RQphmwWq34fD5VDVBGtksXsNxT1nfKD4fFYlGen0AgQCQSUb1TbjcoZGyM7N0hs3M03SODKmGrpfiNGzce6v0OhwOr1YrL5cLj8RAIBAiFQsBH24cysLlarZpu3XrkDQqZxWGz2RgcHNxRCtXpdDIzM0MoFFJbHRIZYVssFpX7/DCyFvoV6eLLZDL7uu/ndDrVQzZ7m52d5eTJk0QiEQYGBkilUjSbTa5fv87a2hrr6+vKA1UoFI6sZ6Jf6DQoZLCs5uFwuVw8/vjjTE9Pq0qlg4ODqnaB7OMiA2wvXbpELBZTtXQ05qCzOFZnaXt5Myb7gqytrZmyds4jb1DY7XYikQiDg4PKFSsLjUQiEYLBoIqUlsgFMJ1OE4vFWFlZYWFhQbmBNXeSy+W4du0aqVRKFRXrFovFgt/vVy2wP/7xjzM1NcXExARPPfUUXq+XZDLJ9evXSafTfPvb3+att95S+/Vyy0Ubgb1FBgdeuXJFNS7TPBx+v58vfOELvPzyy3i9XtVRubP8czqdJpvNcu3aNb773e9y/fp1CoWCNuBMhqxP0Znm3umdWF9f58KFC5RKpUOtMvwgPHIGRWdfAhnA1+leGh4eViVRh4aGcLvdDA4O3vE50niQPT10i+V702q11Hnq9o5IZnHIfHuv16u0kz1ZQqEQLpeLZDJJPp8nnU6zsrLCzZs3tUYmRGZ5yDRTzcPhcDhUi2vZT+L2myC5Zy/LNW9sbOxona0xD52l6G/PmpKNFSuViunmyiNlUMjOe7LHRCQSwe/3Mzs7i8/nIxqNqgYtPp8Pj8eD0+ncYVB0FiGRe9MyOlpfqD5CGm7yX6/Xy/T0tLr4d9NTIxwOMz4+jtvt5uTJk8zNzeH1elUpZyEEa2trtFotzp8/z5tvvkk2m2V1dVVrpDmSyOJHsoDb7fNLtoW/evUqi4uLO3o/6DlhHmRbiEajQTgc7rvtqPsaFEKISeA/ACNAG/imYRj/jxAiBHwLmAEWgV82DMO0vkqZrhYMBvH5fDz77LOqz/zp06fVHa3P58Nms+3I6Ljd0peZIKVSiVKpZIp0KzPpJM9Xp1Hh9/s5fvw4oVCIUCjUlUERiUQ4d+4c4XCYT3ziEzzzzDOq/bzT6WRjY4OLFy+Sy+V47bXX+JM/+ROlU68xk06a3elXjWSKoaRzTarX69y6dYvz588Tj8fJ5/N9X++jX3W6F81mk0QiQTKZZGhoyHQxEvfjQZLRm8DfMQzjceB54H8VQpwCvg68YhjGHPDK9u+mobO4kdzSCAaDDA0NqR4FsnGV3+/f0QxMlouWgTGyRnu5XFYBmMlkkkQiQTqdNovoptZJlpGVHSwDgYCqFdF5rmUnRFk62O/3Ew6HiUQiDA8PMzo6ysjICMPDw0SjUfx+v8q4gY+CZQuFAtlsVpX4NlFEtKl16hWdxdBM0BG2LzTqvEmSaeydweOdN0WdWx5HqItrX+j0sHT29jDBXHgo7uuhMAxjA9jY/rkohLgCjAO/BHxq+2W/C7wK/PqBjPIhkW1f7XY7wWCQqakp3G43J06cYH5+Hq/Xy8zMjApcCgaDql/H7SlWjUaDTCZDpVIhFoupymaLi4vcuHGDzc1NU+zLm0knuSfbeU7sdjtutxuAJ598kkwmQ6FQ4NatW2QyGVXfwDAMFWg5MDDA1NTUHTpFo9EdXWFrtZrqIVGpVFhaWuKdd94hnU6zvLxsqsXTTDqZhc4Lo2EYxGKxno6nXzSy2+088cQTnD59momJCcbGxu56AWq322o+yE6u/U6/6PQwyOaKgUBAbb/3Ew8VQyGEmAHOAW8Bw9uCYhjGhhBiaN9Ht0dkgyqn00k4HGZubo5gMMiTTz7J008/jcvlumfDnE5kUZ98Ps/CwgJvvPEGyWSShYUFrl+/bgo3+u2YQafbSy7LAEohBMeOHaNYLCpDQtamkN0+I5EI4+Pj+Hw+zp49y+zsLG63m7GxMdUB1u/3I4RQbbFlNcdcLsfCwgJXr14lk8morpVmxAw69Rp5By2rm9brdZXqaAbMrJHVamVmZoYXXnhBeVvho63G26tjFgoFVTbdjOtWN5hZp4fBarUSCAQYGxsjEonsaArWDzywQSGE8AB/CPxtwzAKD+qKEUJ8Dfja3oZ3f2QFMiEEdrtdRThPTEyofhvHjx/H7/czNDSEy+ViYGBg19LDsqSpNCI2Nzcpl8ssLi6SzWZZWVkhkUiQzWYpl8umvFCZVSfpdrVYLPh8PsbGxnC73eRyOTwej+p62G631daGy+VibGyMUCjE4OCg0g5QxkcmkyGXy1EsFlXVv/X1dbLZLPl83rT9OcyqUy+RxoVZMKtGsiSz2+1W3URl7BfsrFlQLpfZ3NwkFoup0s5miPnaT8yq016QNShkHaSHKZFvBh7IoBBC2NkS7D8ZhvFftp+OCyFGty3AUSCx23sNw/gm8M3tz9n3v2KbzYbL5cJms6l9+Wg0yqc+9SlOnDhBIBBgfHycwcFB3G63qhwnJ18nm5ubJBIJyuUyP//5z/nwww8pFApcvXpVPS9LRu+l7PBBY2adABXTIrebarUap0+fVuVnq9UqhmEQCoWIRCLKOJQpcDI3u/NOa3V1lY2NDZLJJD/5yU+4efOmqjMht1G0TualMzvKTAaFmTVyuVyEQiECgQDHjh1jfn5erW3SkJA9H5aWlvjwww9JJpMsLy+reWGmbcBuMLNOe8FiseDxeFQl2d2uU2bmQbI8BPBbwBXDMP55x399G/gq8I3tf//4QEZ493EBH1nrco/e7/cTiUQ4fvw4p0+fxuPxEI1G7+k6kguazIUvFousra1x7do1crkcly9fVvu6ZrXszarT7VgsFrxeL16vl1arpdzcshy3YRgEAgGCweCu1rlcMKX3SG5rxONxbt269dClbg+bftGpV5jBqDC7RjabTRkQXq9XBSZ3dpCVjeCKxSIbGxukUil1M3RUUkXNrtNekJ52mRxwFD0ULwL/E3BRCPHB9nO/wZZYfyCE+FVgGfjywQzxI2Q3PZ/Px8zMjLowBYNBHA4H4XBYRTzPzMzg9/tVsGUn8oIkOybGYjFV+ndxcZFyucz169dZXFykUqmoOvcmn4Sm0EkuYolEgmAwSDKZVAGWLpdrxwSRe+cys0M2inI6neqiIu+mqtUqmUyGarXKwsICP/vZz9jc3CSZTJLL5SgUCuTz+YM8tP3CFDqZCSEELpeLaDQKYIZANFNrJL11Mk5MZjnJdU7Ol1qtxvLyMu+9954yvGW32SOCqXXaCzLWJRaLEQgEaDQau8bEmJUHyfL4CXC3W4bP7u9w7o4MtHQ4HIyPj/OFL3yB6elpgsEgIyMjOBwOgsEgfr9f7UFJA+R2K6/VaqnmOLFYjJ/+9KfEYjHW19e5ceOGMjRk9UsZOGhmzKKTPLe1Wg2Xy8Xa2hput5tQKHRH7IrMxoHdXd+y3ketViOTyXDp0iXS6TTnz5/n+9//Pvl8nlarpRZJk6Tv3hOz6GQ2fD4fExMTO7KBeoXZNZIBztJId7lcO/bb6/W6ClS+cuUKP/jBD1Rmx1EKxjS7Tnuh2WySTqcBCAQC1Ov1HR67Xnvv7ocpN2g686etVqsyDGThomAwSDQaZXh4mEAgwNDQEA6HQ8VQ3I1Wq6UCLwuFgjIqEokEsViMZDJJKpWiWq0eqX3Gw0RmbNTrdSqVCvl8nlwuh91uV7nynTnWt08QwzBotVqqA6jMm5fls1OpFKlUimQySbFY7NFRavYTOc9lULXZF81eI8+PLD8v18fO8ybnkWxgKDtgasyNvImqVCrqOnS7R0nOFzNuh5jSoBgYGCAQCOB0OpmYmFD5uNFoFK/XSyQS4cknnyQcDjMwMIDb7VZdQ29HTqx2u83a2horKysUCgU++OADbt68SbFY5NatWzsKIOn69nvH2O5qKAPCfvCDH/DBBx8wNTXFqVOn8Hg8HDt2jKmpqV0vHIZhsLGxwcrKCqVSiYWFBWKxmKpZUSgU2NjYOBJ59BqNRtOJ3PKo1+uqkWImk1FbW7Jmi9/vx2q1UqvVTFXx1JQGhawf4fF4eOKJJ3j++efxer2Mj4+r3NxQKITD4bhvEJc0KGSXtvfff590Os2PfvQjfv7zn+9IFdX9OLpHVuST7tWf/OQnOJ1O5ufnKRaLKgV0YmJiVwu73W6TSCS4ePEi2WyWd955h2vXrlEul0mlUlQqFRWYqel/tDdCo/kIGd8nu1nncjny+bzqKwUf3XBbLBZyuVyPR7wTUxoUNptNlWcOBAKEQiEV0exyuVT6oMViod1uq7zqer1+h7VWqVTU/uHi4iIbGxuqJLNM/TxigUqmQXZkbbfb5HI5YrGYCqqUXqXd3rOwsMDa2hr5fJ5sNqs8R3IbRNO/tNttKpWKCorW8+7hka0ApGu8XC6rAM3d5pSm/5DXs2w2SzKZVBWEZZZcNBrFbreTyWRU/R55U9xLTGlQ+Hw+5ufnGRoa4mMf+xhnzpxRnULtdvuOOhJyf71er7O+vk4sFlNpUdJ9fuHCBbUHn0gkaDQapNNpVepZL2oHQ6vVolQqYbFYWFhYIJPJYLfb+elPf3rXjqOGYaiMDRnrUiqVlJdJ09/U63VWVlZUEOHHPvaxB6pYq/mIarWqOlKura2xtLSE2+1mZGREn8sjRKFQ4N133yWZTHLmzBlV/uDkyZM0Gg3i8Tiw5eWrVCrkcrmer5GmNChkCujw8DDDw8Mqi2M3ZIXFSqXCxsYGt27dUoVd2u02CwsLvPbaa6TTaWq1Wl9kAhwVZIAmoDI1NI82zWaTQqFAMpkkn8/3fAHsR5rNpjLUZRO8VqtFOBzu9dA0+0ilUmFlZYVGo8HQ0BDNZpPBwUGi0Sjz8/N4PB5+/vOfK8OiUCj0eMQmNSg2NzdZWFggl8tRr9fVne3tGIZBuVy+w0PR6XXY2NigXC7TaDS0J0Kj6THNZpNUKgWgqtv6/X6Wlpa4desWuVyObLYvOk33DJkFVS6XuXHjBm+88QYul4sPP/wQn8+nKsVWq1WuX7+ujbY+RTamlA3zksmkSiMNhUI0m02OHTtGo9FgY2ODQqGgtu97tfUhDvOLH7S8qaxTL+tJyOjW3eh0hcuKi53HJNuO92PQpWEYPYlYM0sZ2n5B6/TgyNojcm57vV5sNpvyHsrCaAfhSeyFTgehkRBC1dfxer2qnYCMLZNrXavVolAokMvl+ulm6rxhGM8c9peacS45HA58Ph8DAwO8/PLLfOUrXyESiTA8PEwkEqFQKPD++++zurrKpUuX+M53vkMikaDZbB5GvZFddTKlh2K34EqNRtP/yIZVgOrJonk4pLEgi8jprcSjSavVolwuq0JluVxOhQM4nU7VNbvRaLC+vq7iC3uZOWVKg0Kj0Wg0mkcZmcHYbrdZXFzklVdeIRAIcPbsWXK5HFarFY/Hg9frJZVKEQqFVMpprzrKaoNCo9FoNBqTIYPaG40GN2/eJB6P43K5yGazVCoVwuEwTz/9NBMTE8TjccLhsGpHUCqVtEGh0Wg0Go1mJzIWsN1uk81mVaxEPB7HbreTzWZVnZ5exglqg0Kj0Wg0GhMjiwS2Wi0uXrzIxsYGAwMD/PCHP8Tr9RKLxbh69eqOgo29QBsUGo1Go9GYGMMwaDabNJtNFhcXWVxcBLijIVyv0QaFRqPRaDR9iBmMiE4O26BIAaXtf48aEfb3uKb38bMeFq3Tg6N1OhiOik4pYIn9Px6zoHUyPwdxTLvqdKiFrQCEEO/2onDJQXPUjuuoHY/kqB3XUTseyVE7rqN2PJKjdlxH7XjgcI/pzv7RGo1Go9FoNA+JNig0Go1Go9F0TS8Mim/24DsPg6N2XEfteCRH7biO2vFIjtpxHbXjkRy14zpqxwOHeEyHHkOh0Wg0Go3m6KG3PDQajUaj0XTNoRoUQohfEEJcFULcEEJ8/TC/e78QQkwKIX4ohLgihLgkhPhb28+HhBA/EEJc3/432Oux7hWtk/k5ChqB1qlf0Dr1B73W6dC2PIQQVuAa8HlgFXgH+IphGJcPZQD7hBBiFBg1DOM9IYQXOA98EfhrQMYwjG9s/0EGDcP49R4OdU9onczPUdEItE79gtapP+i1TofpoXgOuGEYxoJhGHXg94FfOsTv3xcMw9gwDOO97Z+LwBVgnK1j+d3tl/0uWyL2I1on83MkNAKtU7+gdeoPeq3TYRoU48BKx++r28/1LUKIGeAc8BYwbBjGBmyJCgz1bmRdoXUyP0dOI9A69Qtap/6gFzodpkEhdnmub1NMhBAe4A+Bv20YRqHX49lHtE7m50hpBFqnfkHr1B/0SqfDNChWgcmO3yeA9UP8/n1DCGFnS6z/ZBjGf9l+Or69fyX3sRK9Gl+XaJ3Mz5HRCLRO/YLWqT/opU6HaVC8A8wJIY4JIRzArwDfPsTv3xeEEAL4LeCKYRj/vOO/vg18dfvnrwJ/fNhj2ye0TubnSGgEWqd+QevUH/Rap0MtbCWE+IvAvwCswG8bhvFPDu3L9wkhxEvAj4GLQHv76d9ga5/qD4ApYBn4smEYmZ4Msku0TubnKGgEWqd+QevUH/RaJ10pU6PRaDQaTdfoSpkajUaj0Wi6RhsUGo1Go9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Go9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Gpa8qpQAAIABJREFUo9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Go9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Go9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Go9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Go9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Go9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Go9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Go9FoukYbFBqNRqPRaLpGGxQajUaj0Wi6RhsUGo1Go9FouubIGxRCiFeFEL922O/VPDhao/5A69QfaJ36g6OoU98YFEKIRSHE53o9jrshhPiqEOK8EKIghFgVQvwzIYSt1+M6TPpAozNCiO8JIVJCCKPX4+kVfaDTrwghrgoh8kKIhBDid4UQvl6P67DpA530fML8OnUihPhzIYRxUNemvjEo+gAX8LeBCPBx4LPA3+3piDS30wD+APjVXg9Ec09eB140DMMPzAI24B/3dkiaXdDzqY8QQvxltubSgdH3BoUQIiiE+I4QIimEyG7/PHHby44LId7evuP5YyFEqOP9zwsh3hBC5IQQPxNCfGov4zAM498YhvFjwzDqhmGsAf8JeHHvR3Z0MJFGVw3D+C3gUheHc2QxkU4rhmGkOp5qASf28llHERPppOfTPTCLTtuf5Qd+E/j7e/2MB6HvDQq2juHfA9PAFFAB/tVtr/mrwF8HxoAm8C8BhBDjwJ+wdfcTYsuj8IdCiOjtXyKEmNoWduoBx/XfoSeaxKwaaXZiGp2EEC8JIfJAEfgS8C+6O7QjhWl00twTM+n0T4F/A8S6OaD7YhhGXzyAReBzD/C6s0C24/dXgW90/H4KqANW4NeB37vt/d8Dvtrx3l/bw1j/Z2AViPT6vGmNdv3+E1t/+r0/Z1qn+45hHPiHwHyvz5vW6a7fr+eTiXUCngE+YGu7YwYwANtBnIu+91AIIVxCiH8rhFgSQhSA14CAEMLa8bKVjp+XADtbsQ7TwJe3rbucECIHvASMdjGeLwLfAH7R2Om2fWQxm0aa3TGjTsbW9uGfAb/fzeccJcyok+ZOzKCTEMIC/GvgbxmG0ezmeB6Eo5CF8HeAk8DHDcOICSHOAu8DouM1kx0/T7EVTJRiS8zfMwzjf9mPgQghfgH4f4H/3jCMi/vxmUcE02ikuSdm1ckGHD+Az+1XzKqTZidm0MnHlofiW0II2PJ+AKwKIb5sGMaPu/z8HfSbh8IuhBjoeNgAL1t7U7ntgJbf3OV9f0UIcUoI4QL+EfCfDcNoAf8R+EtCiC8IIazbn/mpXQJn7osQ4jNsBWJ+yTCMt/d8hP2PmTUSQogBwLH9+4AQwrnXA+1zzKzTX97eFxZCiGngnwCv7PlI+xsz66Tn00eYVac8W/EZZ7cff3H7+aeBtx7+MO9NvxkU32VLIPn4h2wFaw2yZdW9yZZ79HZ+D/gdtgJSBoC/CVvR5MAvAb8BJNmyCv8eu5yX7QVuU9w98OUfAH7gu9uv2xRC/OmejrK/MbNG09tjksGyFeDqQx7fUcHMOp0C3gA22UohvQo8qnfUZtZJz6ePMKVOxhYx+dj+LIC4YRj1vR7s3RDbQRsajUaj0Wg0e6bfPBQajUaj0WhMiDYoNBqNRqPRdE1XBoUQ4hfEVs39G0KIr+/XoDT7i9apP9A69Qdap/5A63T47DmGQmzl0l4DPs9WEad3gK8YhnF5/4an6RatU3+gdeoPtE79gdapN3TjoXgOuGEYxsJ2tOjvsxWVqjEXWqf+QOvUH2id+gOtUw/oprDVODurfK2y1WXzrohHuMXtXjAMQ9z/VfdF63TAaJ36g17opDV6aFKGYdzRr2IPaJ0Oll116sag2G1y3iGKEOJrwNe6+B5Nd2id+gOtU39wX520Rl2xtE+fo3U6WHbVqRuDYpWdZUMngPXbX2QYxjeBb4K2AnuE1qk/0Dr1B/fVSWtkCrROPaCbGIp3gDkhxDEhhAP4FeDb+zMszT6ideoPtE79gdapP9A69YA9eygMw2gKIf4GWy1VrcBvG4Zx6T5v0xwyWqf+QOvUH2id+gOtU2841NLb2q30cOxTENlDo3V6OLRO/UEvdNIaPTTnDcN45rC/VOv00Oyqk66UqdFoNBqNpmu0QaHRaDQajaZrtEGh0Wg0Go2ma7pJG9Vo9h0hBOFwmEgkgtPpJBQK4fF4aDab1Ot1ms0mqVSK1dVVarUazWaTZrPZ62FrNBpNXxCNRpmamsJut6vnqtUqS0tLZLPZrj5bGxQaU2GxWDhx4gQvvvgioVCIs2fPMjs7S7VaJZVKUS6XefPNN/mv//W/ksvl2NzcZHNzs9fD1mg0GtMjhODkyZN86UtfIhgMqufW1tb41re+pQ2Kg0YIgRBC/SwxDIN2u92rYR1ZLBYLXq+XkZERotEos7OzPPbYY1QqFfx+P6VSiVu3buFyuSiXy1gseteuF3TOBfm7fFgsljv+/2GQ86rdbtNutznMTLSjTOc6JjVqt9u0Wq0ej0xzGFitViwWC36/n8nJScLhMIZhYBgGrVaLwcHBrr9DGxQdWCwWrFYrQgh18h0OB+FwGJfLxf/f3pvExnWleb6/G3PEjXkigwxOogaSkiVLHmQr03Ymsspo1IDsTTa6F418QAO1eYvXwFtUoTdv9YBaNd7bJtCNrAc0uruA7Krywi5UllFZmZW2M9OWLMkaKYrzEPM8MYb7FtY5GaQoixKniPD5AQSlIBlxb3xx7v3ON/w/k8mE3W4HYGtri83NTVqtljSK4uWxWCw4HA7sdjsjIyOcO3eOQCCAz+fDMAzMZjNer1emQYLBIK1Wi3q9TqlUUu//MWG1WnE6nXKdaJqGxWLB5/Phcrnw+/2cO3cOn88H8MJ2abVaZDIZSqUS2WyWO3fukM1m1Rp7SYSNxPpxuVx4vV5OnTqF2+1mcXGRmzdvUqvVTvpQFUeIz+fj9OnT+P1+Xn31VcbGxvB4PKRSKbnems3mgV9HORRdCAfCZDJhtVqxWq243W7OnDlDJBLBbrej6zoAN2/elBe6drutLnYHxGq14vF40HWdsbEx5ubm8Pl86LqOYRjyptXpdAiFQoRCIVqtFoVC4aQP/VuF1WrF7/fLdSKc7PHxcaLRKJOTk/zwhz9kYmLipZyAer3OgwcPWF9fZ35+nkQiQaFQUDvpl0REI6xWK+FwmHA4zNjYGO+//z6jo6P8/Oc/Z35+XjkUA47P5+PNN99kYmKCmZkZJicnsdlsZDIZcrkchUJBORQHQYT8zGYzdrtdfhe7L7vdjs1mw+12Mzw8TDgcxmaz4XK5AAgGg/j9fqrVKpVKhXq9fsJnNBiIm5BIKXXfkER6Q0SPutNRisNHOAzC0TabzXg8Hulci4iezWYjGo0SjUaJRCL4fD68Xi/ACzsVdrudQCBArVaTkahsNku9XqdSqagUyAvS7VD4/X6Gh4eJRqMEAgEZsRDrSUWBBgtN03A4HNL2wWCQUCiEx+PBYrFgMplot9tUq1VqtdqhOOzfSofCZDKh6zp2ux2/38/c3JwMr0ciEWw2G36/H6/Xi9Vqxefz4XA4MJvNWK1WOp0O4XCYYDBILpfjxo0bLCwsnPRp9TWtVotqtYphGKRSKZaXl/H7/YyNjeFwOE768L51aJqG2+3G7XbjcrmYmpoiGAwSiUQ4ffo0uq7Lm5XFYiESieD1evF4PAQCAXljetEblMViIRaL4fP5cLvdZLNZzp8/z6NHj7h58ybValV19rwAYnMUCAT4/ve/z7vvvovT6SQUCmG32/H5fHg8Hur1OvV6ne3t7ZM+ZMUh4XA4OH/+PFNTU8Tjca5du0Y8HpcORbPZJJ1O8/jxY9LpNJVK5cCv+a10KDRNw26343a7GRoa4uLFi4yOjhKJRBgbG8PpdBKJRAiFQnvugtvtNpqmYRgGiUSClZUV5VAckE6nIy9mhUKBZDJJq9UiHA6f8JF9O9E0DafTidfrJRgMMjs7SzweJxaLcenSJTwez44iTFFDIf6225F4EafCZDLJyITdbieXyxGPxzGZTMzPz9NsNlUx9AsgNkFut5uLFy/y/vvv02q1KJVKbG9vo+s6TqcTh8MhW7MVg4HVamVqaorLly8zOjrK7OwssVhMFju3222KxSJbW1vkcjkajcaBX3PgHQoR7jObzei6jq7rOBwOhoeHCQQCRKNRxsfHiUQi+P1+PB6PDO+KnJK4gInFKS62oVCIdruNx+PB5XLRbrfVBe8lESkO4Vg0Gg0ajYZ6L08Ik8lEIBBgcnKSQCBAPB5ndHSUcDiMy+XC4XBIR9tsNmM2m4Gnuz8OgtVqJRAIoGma1CMR60vd+F6cbgfQarViGAY2mw2bzSavkYrBwWQy4XA4ZG2auHe1Wi1qtRqlUolcLkcmk6FQKBzKmhp4h0KE+2w2G+fOnWNubg6v18vs7KwMp4vdkNVqxW63o2kajUaDYrEoL17tdhuXy0UgEMBisTA0NITVaiWRSHD9+nUWFhao1+tks1l1sXsJOp0OzWYTwzCoVqvkcjnMZvOheM2KF8dqtTIzM8P3vvc9AoEA58+fZ2hoSNYR7XYgjuJm5Ha7mZ2dZXt7m1wux29/+1tsNpv8jKh8/8thNpulU+jxePB4PFSrVer1+lPRJUX/YjabCYfDTE5O4vf7Zdq+Vquxvr5OLpfj/v373Lx5k0ajoVIez0O0tDmdTpxOJ9FolKmpKQKBAHNzc0xNTe2IOoiLY/cuud1uS0VGs9mMYRgyQhEMBmk2m3g8HhwOB4ZhKF2EAyAiFCL0ur29rSIUJ4TJZJI1LIFAgJGREaLR6HP/TtyMDuPGZLFY8Pv9GIZBIBDA7XZTLpd3KPwpXhxN02QtmNVqxWazYbFYVISiR9itd/SyiAiF2+1G13UsFguaptFsNimXy5RKJfL5PJlM5tBqkgbKoRDVyk6nk3g8LitbJyYmcLvdTE5OMj09ja7r+P1+WekqEDexRqPBnTt3ePDgAe12m3a7TafTYWpqimvXrskFaLfbpXaCyEEqh+LlEM6f1WpF13UCgYBsT1QcP51Oh0KhwMbGBs1mk8nJyad+p91uy+hdOp2mWCwC33xBtFgsuN1uqWfh8XjUjeyYUZ1RvYdIx9vtdoaGhgiFQhQKBRYWFmTb9H5u+pqm4fV6CQQChMNhhoaGCAQCOBwOGo0GhmGwtrbGjRs3yGQybG5uHmpEaqAcCovFgs1mIxgMcu3aNc6cOUMsFuPChQuyRUrXdZlDtFh2nn6j0SCXy1Eqlfjnf/5nPvroI1qtlsw7fuc73+HChQsEAgH59y6XSz6vciheHlGNLjpvhoaGCAaDstBPcbx0Oh2y2SyPHz+mVqtx/vz5p35ne3ubWq1GpVLhzp07LC0tPfU7uy9WTqeTkZER6eyLVm3F8aParnsD4QSMjo7i9/t5++23mZ2dZWFhgZ/97Gc0m03puD/v5q9pGpFIhHPnzhGJRJiYmGBoaIhOp0OtVqNcLvPgwQM+/vhjUqkUa2trhxoFHhiHQmhKiBxvIBAgEonIL9ECuteOV/Rfi7qJQqFAJpMhkUjQ6XRkJKNSqezo1RWOhtBEOKjk8LeZ7veyWwdkLwdN2FoUkplMJqVPcMgYhiFVSN1uN6VSiVKptOPntVqNarVKuVwmm82SSqWA3++A97KHSD8ahoHVapVKs3utG1Go2263ZauoiBYqFIOC6Dr0+Xz4/X4ZWSgUCrhcLiwWi9zY7uca53A48Pv9+P1+XC4XNpuN7e1t6vW6vMflcjny+fyh6ycNjENhMpmIRqOMjIwwMjLCxYsXuXDhAm63W3Zu7HVzarVaVCoVms0mX331Fb/85S/JZDJ8+eWX5PN5NE2Tf684OkTKw2azoeu61DTYK1/ucrkYHh7GbDZTKBTI5/M0m01qtdqhqL0pvk5nrK2t0Ww28Xq9pNNpYrGY/LlhGHLn1Gg0WFlZIZVKPeUc7L4Aigiiy+XiwoUL+Hw+hoeHsdlsUtZeUKvV2NraolKp8PjxY9bX10mn05TLZeU8HgLCVhaL5an0r+L4EAMR33//fQKBAOfOnSMej1Ov1xkaGiKbzVIoFKjX6891pk0mE/F4nLfeeotwOEw8HsfpdFIsFrlx4wabm5vcvn2btbU1SqXSoRc3P9eh0DTtvwJ/AiQNw7jw5LEg8D+BSWAJ+DeGYRxsTNkBMZlMhEIhKeIxOzvL+fPnnxvWa7fbVCoVarUa9+7d48MPPySVSpHP5ykWizLX2+sORb/Y6VmIqIOY6SEKiXY7FEL9LRwOo2kaiUQCj8dDo9Gg2Wz2vEPRL3Zqt9tsbW2RSqWwWq08fPjwKYExET1ot9vy4vQ8RKGY1WqlXC7z9ttvSwEtm822Y602Gg02NjbIZDKsrKyQSCTIZrPHImrVL3Y6KP3uUAyCnUwmE2NjY7z77ruy9sHn81EsFolEIiQSCTnjZj/PFYvFuHLlCoFAgOHhYZxOJ41Gg9u3bzM/P8/S0hKJRIJ6vX7ojvl+PkE/Bf7Vrsf+AvjYMIwzwMdP/n+iiBuNULgUEYnuC1R3B0G1WpUDiNbW1lhaWiKZTFIul6nVajtCsaIAU3SD9Cg/pQ/s9CzETBTRI10sFp85sEY4FN0SwqKKuQ/4KX1iJ+EwNJtN6vU61Wr1qa9arUatVpM53ud9iVSHSGkJDYu91lV3t5V4/mNMbf2UPrHTi9Itsd1dXN4n62c3P2UA7NQtc9+dQu/+et7f22w2HA6H/BIjJboRa/CoZNaf+wkyDOOXmqZN7nr4h8D3nvz7r4BfAH9+iMf1wpjNZkZGRnjllVcIh8NylkA3Yh6AkBwVKmH/8i//wtbWFqurq2xtbUmHQjyv3+8nFArh9Xp7dtH1i52eRavVolwu02g0WFpa4osvviAUCklxo25GR0d57733KJfLMnKUzWapVCo78vy9SD/ZqXueSrlcfuqi1i2vvd+ogc1mY2RkhHA4zPj4OF6vF6fTuaezLqKHhUKBarVKs9mUjv5R0092Oggul4tYLIbZbCabzfadDsWg2Eno8OwWRhS1Ys9zKISekq7rRCIRudESAmbdz3WUXVUve3ccMgxjE8AwjE1N057foH7EmEwm3G43sVgMv9//VD4WkLvfRqNBJpMhnU6zvLzMl19+yfLysrwhdRdedvfyOp3OXo5Q7EXP2elZdAuI5XI52a64Vxjd6/Xi9Xqp1WosLi6yuLgIsKfN+4SetZPYyRyWWJuYGiuUae12+zM1EIQGjJgx0QNTfXvWTi+LzWbD6/VSr9d7Pq37AvSdnbrTh91RuP1GKEQ7tvhyOp0yQtGtDXPUQxWPfLutadqfAX921K8jcr63bt3C7/fT6XTI5/NSAUyMuhaPJZNJ8vk8yWSSbDYrC/p2X7CECIxIefRjnnE/HJednodhGGSzWebn5ymVSrz++uvP7AL4NtIrdnoRRGGz2+3G6/UyPj7O6OgoQ0ND8qK3l33r9TrLy8ssLi6ytbXVNwPBeslG3aHtVqtFs9mUnVGArFkS3QDfpnV20nZyu92Ew2HcbjfxeByfz4eu63Q6HcrlMuVyWXYdPqt4UjgIHo+HiYkJOcBP3KvEFFHRtbi5uUk+nz+UyaJ78bIORULTtNgT7y8GJJ/1i4Zh/AT4CYCmaUe2vWg2m9y6dYvNzU0CgQCrq6uMjY2RzWZZWlqiWq1SKBTkEJRyuSzDqOVyWYaadlfRCqGsPo1Q9Jydnken02F5eZlkMsnIyAjvvPPOSR3KcdJ3dnoRRDpSzAW5evUqY2NjDA8Py7DsXusqn8/z2WefcfPmTUqlUi/IsO/LTr1kI5G2Es5EvV6XGySTyYTdbicUCtHpdAZJD6Qv7BQOh3nnnXeIRqNcuXKFeDyO1WqlUChQLBalAyA6nXY7AaK+z2w2Mzw8zNWrVxkZGeH06dPyXpXL5cjlcjx+/Jg7d+5w9+5dtre3j6x4/WUdig+AHwN/+eT73x3aEb0knU6HYrFIu92mXq+TSCSw2Wyk02lWVlYol8vSoejO1z+Pbn2LPlT06zk77QdR6Ceqk7vpM4duv/SlnZ6F2P2K3ZPNZsPtdssKdjFNVIxRfpYGxfb2Nvl8nlQq1StD9/rSTiJKIYqehU1gp0LtAEVf+8JO3cXlPp9PKjCL2iExX6Ver+/pAOxWFw6FQoTDYTwej9TmEQXuosi9VCodWUEm7K9t9L/zdYFLWNO0NeD/4mtD/bWmaf8BWAF+dCRH9wJ053kNw+DOnTusr69TLpdJp9NS2EOEgPYb8rFYLASDQYaHhwkGgz07R6Bf7PQyiJxfPxWLPYtBthN8LVw1NDSEy+UiEokwNjaGy+ViYmKC0dFRdF1nYmKCQCAgRXu6EeFeMWhPOBLHbftBsZNwIGq1Gul0mo2NDXRdl+2E/U6/2UlEhcSAyXPnzsn0n0hR3Lt3jwcPHshIbXfXYTcul4u5uTn5PLOzs0SjUVlfVi6X+eSTT7hx4wYbGxukUqkjX0v76fL4d8/40Q8O+VgOjGgvq1QqZLNZTCaT9MxhZz5xv2+q1WolFAoRj8dl10Ev0k922i/CkeguIup3p2IQ7dSNy+VienqacDjM3Nwc3/3ud/F6vQSDQQKBgGxvE3UTu3fFItIoQrVCzOe4nYpBsZPYPFWrVZLJJMvLy4RCIQKBwEA4FP1mJ5PJJCe9Dg8Pc/78eSYnJ2WKolKpcOvWLf7pn/6JXC7H1tbWM+sndF3n1Vdf5fz580xMTPDKK6/g9/tli3cymeQXv/gFf/u3fyvlEo46ytebPZAHQDgN3/TGCalT4Rx8Uxhd13X5AejO9e6eiilUA3sgLKtQHArdvfCi3UyIjz1rzYRCIaLRKOFwWLaviRHZTqdzR589/N5BFJ0cjUaDdDpNJpMhl8udWIRi0Oi+Lh5VQZ7i2YjPvcPhkC2dQjFWTKquVqtUKhWKxSL5fJ5yubxnZMJms2Gz2fB4PAQCAYLBID6fT0Y+ms2mLOYslUpUKpVjWz8D51DsB5vNxunTp4nFYnJ2xLPqI4LBINPT03Jqm8ViwTAMisUi2WyWRCLBysoKKysrMkKiOBx2C7Com8rxoWmanLshUhai7iEajT6lminw+/3MzMzg9/sJBAIMDQ3JIkARleh2yoW2xPz8PPfu3aNcLvP48WMSiQSpVEp2dyjbK/qV7i6nSCTC9773Paanp5mYmCAWi+FwOFhfX5fy9Xfv3mVpaUk62N1YLBampqaYnJwkFovxxhtvMDc3h67rOBwOOp0Oq6urXL9+nWQyydbW1rGunW+lQ2G1WonFYszOzsqCy70GexmGgdfrJRaLEQgE0HVd/k61WiWdTpNKpUgmkzI/pSIUh0/3glA3luPDbrfjdrvx+/2cOXOG0dFRIpEIp0+fRtf1Pf/G6/UyNTWF1+t9rq26o3xra2t8/vnn5PN5Hjx4wPr6Oo1G40hb3BSK40A4536/n1gsxtWrV3nttdfQdV2mAYvFIo8ePSKZTLK6ukoikdhz/ZjNZoaGhpidnSUWi3HmzBmmp6dlNLHVapFKpbhz5w7pdJpc7ngVxwfWoRA7oW51MLfbjc/nw+12c+bMGU6dOiXDuSKXuzsUKyaXut1u7Ha7nMKYSqXkB+CoK2cViuNCONhWq5WxsTE5UvnUqVMMDw/LsePPilB0S6A/r+5FtGzXajUymQxbW1uyZU5UtisH/WgRKax+neXRq4hRELquY7PZOHXqFBMTEwwPDxOJROR8qN2dUPV6neHhYeLxOI1Gg0KhwPb2tkyZCCGySCRCMBjEbrejaRqNRoNarUa9Xmdra4utrS2pr3ScDKRDIdppRAFMKBTC4XAwNzfHpUuX8Hq9XLhwgfHx8acK/7ovgoZhYDabcblcWK1WOp2OTGvcvHmTDz74gGw2y+PHj1WeVzEQuFwumdr4wQ9+wHvvvSe7AsT0V4fD8cybj5gNsR9qtRpra2sUCgW++uorfvOb38gOD1GPpKITR4vFYsHlckn5c8XBEU5CJBLhzJkz+P1+3nvvPa5duya7n9xut0y3G4aBz+djbGwMn89HqVQiFAqRTCb58ssvSafTct3pus7k5CSvvPIKPp9PjpgoFos8ePCAQqHA7373Oz777DMpjnWcDKxDIeoi7HY7uq6j6zojIyOcO3cOv9/P9PQ0o6OjT/3d7l1Vt5PQLQOcyWRYXFwkn8/LCIVC0e9YLBYp5BaPx5mbm5Ph2v10BbyITkir1ZJy99lsllQqRaVSOcjhK14Qk8mExWKRKpkDqvNybIj3UAgiRiIRQqEQp0+f5uLFi08VNItCWVFkqWkaQ0NDMirhdDpllN1ms2G32/F4PIRCITwej9QTaTQaZLNZ0uk0iUSCRCJx7NEJGBCHotuAYu7GzMwMkUgEXdcJh8PY7XYmJyeJx+M4HA7MZjPVapVWq0WpVKLVauFyufD7/U8Vj+31WsKjdLvdNJtNWUmrHAvFoLLfm83uC+az/s7pdDI8PIyu64yNjRGPx3dIDSuOHl3XGR0dxW634/P5Tvpw+h673S5TGnNzc7z11lsEg0FGR0d3aOlomiaVS9vtNiaTSdbpdTodwuEw8Xgci8VCOp2W0QmXy8XFixeJRCI70h2JRILPP/+cjY0NlpaWTkymvu8diu5ohJgKOjIywo9+9CMuXbqEw+GQ8r7dIaZ2uy1DQqurq1QqFYaHh6Un2N0r343IZYXDYWZmZshkMhQKBdLptAzRKqdCMQi8zI51r7951npwu91MTU3RaDR4+PAhZ8+eJZvNsrCwoByKY8Ln83H69GmpTaEiFAejW4fl7bff5k//9E/x+/04HI6not+i6LjVaqHrOrFYDPh6mnK73aZUKnH+/HlKpRJOp1Pex8LhMIFAQNbzVasITaaHAAAamklEQVRVlpaW+Id/+AcWFhZoNBpHJq39PPreoRAhO6vVitvtJhgMEg6HGRoakg6CMKaoKG+323LqaLlcJpfLUS6XZbTBYrHQ6XR2TGoTCAfG6XTi8/lot9tSo2L3pDjF4bOXgycKyxQHp3v2Q71ep1wu0+l0ZN2E+Hw/q/Nmt0PRPTVRDKUS30WRmdCFsdvt/Spx3xcI6e3uTY9IC+/W2VG8GOIzLdKDoVCIYDCI3++XkR+xtprNppTXLhQKtNttLBYLHo9HpjdEJDwcDssaFyFV73a7sdlsMrLR/dyiiFmIOh73vahvr8Ki8MXv98sil7fffpvXXntNtrm53W6y2Sx3796V6pmZTIbt7W1SqRTFYpFWq0W1WqXT6fDaa68RCoVkscvuIiVRcGa1WmXrXDablUpmlUqFjY0Nmf5QFeoHp7tYdvfiMJvNBAIBxsfHZXGZ4mAIieZyucyvfvUrMpkMHo+H6elpQqEQ5XKZVColiyZ3O9B7RScsFgsjIyMMDQ3hdruZnJwkGAwe96l9q2m1WqTTaZaWlmg2m5w9e/akD2lgEDIE4lr0/vvvyw4pceNvtVpy4vWNGzdYX1+nXq9TLBYxDIPZ2VnOnz+PrusEg0HpNITDYZrNpoycCycc2KE6Ozs7y49//GOSySS3b9/miy++kNGL4xyq15cORbeCn8fjYWxsjGAwyHe/+13+6I/+SLaJaprG+vo6t2/fJpVKsbq6ysrKCpVKhdXVVVKplIw2WK1WrFYrr7/+umz52Z3/FR4owOTkJOPj42SzWVZWVsjlcmQyGfL5/A65YMXLI977Z0lvm0wmvF4vIyMjtFqtZ7YyKvaP6GIymUz87ne/4+HDh/h8Pi5fvszo6CipVIr5+Xmp4refXK3dbufSpUvMzs7uGA4mUBG9o6fdbpPL5VhbW8NqtfbC5NaBwWKxEI1GmZyc5MyZM7zzzjucPn16h4CbEKlKpVL8+te/5ssvv2R7e5tarSYlt30+Hz6fT9YBWq1WAoEAsHfkr3vIm3D4q9Uqf/M3f8PS0hLFYpFms6kciudhtVrldDbRsytygMKLEzf1YrFIJpMhlUqRy+UoFArU63UajQbtdhu73S7DSuFwGKfTKUf7AjKCIS6yoh1VqP8JL1IMQSoUCrjdbqrVKqVSSYYXd180hdeqLqYvjxCM8Xq9eL1e2dutalkOhoiubW9vU61WMZvNZDIZLBYL2WyWfD4vh+ztx6EQXVGJRAL42mlRtjleDMOQKd9ms7lnmkqE29X6eTHExlZEt4Wacq1WkwX/Ytrn1tYWiURC1k7U63U0TSOTybC5uUmtVsPv98uJoaL2b3fkT0RsuzdaYkS9UJ9ttVrHvqntK4dCvLHBYJDXX3+dSCTCzMwMb775Jl6vl+HhYelMbGxskM/nuX37Np999hmbm5tS21xED3RdZ3x8nD/+4z9mYmKCU6dOMTk5KacgdjodSqUSt2/fZnNzU3aB2O12hoeHGR0dxeVy8dZbb3H27Fny+Tz37t0jn8+zvLzM7du3qdVqey7iarVKJpM5seKZfqB7/oDICXZjMpmkPoLL5WJ8fJyVlRVqtZpcsIqXQ8wW2N7eplwuU61WcTgcNBoNKpXKDkf5eTces9lMvV5naWmJU6dOceXKFRVyP2YMw6BcLu+YEQG/LzIXejt+v19OqlRjBPaHzWZjenqat956S0YUcrkcS0tL3Lp1S6pgLi0tUa1WWV9fl/chUQdx/fp18vk8wWBQOuu6rsuOkedRrVZZXV2VUah0Ok2lUjl2G/aVQyE8aZfLxdjYGBMTE8zNzXH58mXcbrf8vU6nQ6FQIJlMsrm5ycrKipTyrdfrmEwmdF3HbrcTDAa5cOECs7OzciJid4FlvV5nfX2dx48f4/F45Nhft9uNYRhYrVbGx8cZHx+nUCig6zr5fB6HwyFVNIWC2W6OWxa1n9lLF0TTNNxuN263W4YMPR4P8LXQi+JgiF1OvV6nVCod6LnE0C+z2ay0Jk4AsYOtVqsyeisQKWSbzYbT6ZS7a8X+MJvNhEIhxsfHZaq8VquRTCa5d+8e2WyWL7/8krt37+4ZMdA0jbW1Ner1uuweHB8fp9Pp7LvWaHt7m3w+L7sOK5WK0qH4JqxWK0NDQ/j9fsbGxjh79izxeJxIJCJ7etPpNNlslmKxyL1790gkEjx69Eh65CJNYbfbGR8fZ2hoiKmpKYaGhmQRZr1ep1arsbW1RTKZJJvNcufOHZaXl9F1nUQigcPhIJ/Pk8vlZFuqy+Wi2WzicDgIBoNMTU1Rq9Wo1WoyDCUqfDudDsvLy2SzWZXL/Aba7bYsJHM6nTKltRdWq5VoNMrExIS0m9ph9QbCgRey3c+yoeJoeVYkqbvwea/wuuKbERvYra0tAB4/foxhGCwsLMhahueJH3aLV7lcLhl1FR1PmUyGjY0NWYux+9qWTCZ5+PAh+Xyera2tE1OY7XmHQnzQnU4nr7zyCnNzc8Tjcb7//e8zMjIilcfEHPnr16+TzWa5ffs2GxsblMtlkskkzWaTUChEOBzG7/fzgx/8gEuXLhEIBOR0RBEqr1ar/PKXv+Szzz6jUChw7949KX8qZLhHR0elsNW5c+cYHx/H4/EwPj4uVQZff/31HWFhIaJVq9X45JNPuH//Pvl8/qTf4p5le3ubhYUFPv30U6LRKK+++iqhUGjP33U6nZw7dw6TycSDBw9YXl5WO+EewWQyEYlEmJqaYmJiYkc0UXE8dKcPn5VXFyMLRAukYn80m03W19f56quvKJfLPHr0iEKhQD6fJ5FISAGrb3IorFYruq7LWgwheWC1WqVz8tFHH5HP58lms+RyuR12FMMqG40GuVxOCVs9C1EsZLPZ8Pv9DA8PE41GZZ+v8NiE9KgYiiIGpIjCR9Fv7fF48Pl8RKNRWUjpdDplzYTIH4quEJE6yWazsm1UFN10Oh28Xq9UOBMfGOF4CCUzQbPZlANbfD6f6rd/Dp1Oh0qlQj6fx+l0fuMiMZvNches67p6b1+Q7tbcwyrGE88pWnpFSkpphpwMImf/rHZfMSRstzy04pvpdDpUq1WKxSLZbJb19XWy2azUmXheYWR3QawQVbTb7bJTsVuSQDQYZLPZHc8rWlCbzaZs6T4JenpliwErw8PDBINBLl++zJUrV/B6vTgcDprNJmtra9y/f59CocD169d58OAB9Xodi8VCLBaTeup2u514PC4jCefPn5ea6fPz83JK2+LiIuVymVu3brG8vCzrLoAdoiS5XE6KWlWrVR4+fEgwGGRra0s6O7FYbIeWhSjw3Nra4u7duwfOSw86nU5HCo/puq6KLA8ZccMXtUR2u106cKJC/CDOhd/vl9oTV69e5fLlywSDQSKRyCGehWI/tFotkskk29vbuFwuUqkU0WhUCv/ZbDbi8TivvvoqmUxGipopnk+z2WR5eZl6vU6lUiGVSsmC5m9aP92ifJFIhNOnTxONRmVaUDgqrVaLzc1NFhYWSKVSckJv93O3Wi1qtdqJD9R7rkOhadoY8P8Bw0AH+IlhGP+vpmlB4H8Ck8AS8G8MwzjUKkMxKOXixYtEo1HeeOMN3nzzTeD3N/eVlRU+/vhjMpmMrKS12WyMjIwQjUYZHR3l8uXLBAIBJiYmmJ6exmazyUW0sbHB3bt3WVtb4/Hjx9y4cUNWQ+fz+R0CVaKVDn7fCqdpGvPz81LVbH19nVAoxNTUFK+88sqOCt1kMslHH33EvXv35JjmQ3yvTsxOR4WIUKTTaTwez0A4FL1kJ5EzF0XOXq93x1Ahoar4sgSDQc6fP08wGOTdd9/lnXfewW6397wAWS/Z6LBotVqyLszlcrG1tcXw8LBst7bb7UxMTPDGG2+wvr7O4uIiGxsbJ33Y30iv2Gl7e5vFxUVWVlbkWIe9ish3IwphbTYb0WiUmZkZQqEQoVAIu90ulZzFZvfBgwckk8k9o4j7eb3jYD+JshbwfxqGMQu8BfzvmqbNAX8BfGwYxhng4yf/P1RE7YTY8bvdbqlUKXJ8IlQnclCi8CsSiRCNRnekR/x+P263G5fLJW9WpVKJTCZDJpMhl8tRLBalB7hbprYb4Qm2Wi05i16oceZyOTn1TaReRP+x0MIQ3uQhcmJ2Oiq6Q4mijU2EbOHpYjLRfSPqXHo0F3yidhLpQ4fDgc/nIxwOE4lEpFS93+9/qfeuW8vA4XDIwrJwOEw4HMbn8+FyueRgvm722356jAzcWgKkpLrI6QsVRRGJajabe3aB9DA9Y6d2uy3lAfaSp98Li8Ui6yZ8Pp+U6RbRie3tbZm+z+fzbG9vy3uSSFt1p696Yf08N0JhGMYmsPnk3yVN0+4Bo8APge89+bW/An4B/PlhHpzJZGJ8fJx33nlHFqrAzhvJ8PAwV69epVqtyvZMIXjl8XjQdV16fMIhaTQa3L9/n9XVVdbW1vjFL37B2toaxWKRdDotBWBelEqlwsLCgox23Lx5c0e+uFarsbq6KkPKhxmaOkk7HRWiKFNIQV+7dk0O2nG73TvyvC6Xi9nZWcbGxgD45JNPaDQaVKtVKY3eC5yknTRNw+v1EgqF0HWdmZkZxsbG8Pl8zMzM4PV6+dWvfsXm5qbMAe9HJ0Uo9olaiZGREXRd59KlS/zhH/4hgUCAsbExKRjXLcYjnHJxsRQX5JNkENdSN7VajZWVFVwuF/F4HJ/PR6vV4s6dO/z93/89xWKRZDJ50of5XPrdTn6/nwsXLhAIBLh69SpvvvmmdMQrlQrLy8t88MEHLC4uMj8/3xcD816ohkLTtEngMvAbYOiJQTEMY1PTtOhhH5ymaUSjUS5cuIDf798RKhW70kAgwLlz52RbqChsCYfD6Lq+5/MKEZCbN2+ytrbGrVu3WF9fP/D8jUajwebm5kv//WFx3HY6KkTuUIiKZTIZyuUymqY9ZVtRI6NpGhsbGwSDQdLptBwE1ysORTcnYSdd1+W0witXrsgL2tmzZ/F6vWSzWXlBe5GWZovFgs1mw+PxEI/HCQQCzM3NceXKFfx+Pzab7anZON0Ohdg999pwvUFZS92IdO3GxgYul0tOp1xeXuaLL7440WmVL0s/2klM2x0aGuLs2bOcOXNG1jGJkeSffvopt2/flpGjXmffDoWmaW7gZ8B/NAyjuN8qYE3T/gz4s5c7vN+Hknbnz4VDIToqOp3OjgploVAphHnEjUWI9Dx69Ii1tTWSyaSUAu6lC9nLclJ2Omra7bYM09pstj1ttdck0l7lJOwkhulNTU3JgkmRShQTDsUYZYvFQqlU2lfrrdlslg6/GMwXCASIxWKyWl3YQojFtVotWQBaq9VYX18nn89TLBZ7Rj9kUNfSbsRa2h1C7xf6yU4iRW8ymXC73XIytpg02m63ZctpOp2Waal+cfD25VBommbla4P9N8Mw/teThxOapsWeeIAxYM8YmWEYPwF+8uR5XvhTKqR+hYqb0+mUuVoAj8fz1CCvRqPB+vo6xWKRfD7P5uamlDxdXV2lWq2ytrYmq5nz+fyJh1kPg5O001EjWm63trZk9093Oqn7AtjrF8OTspPZbObMmTP8yZ/8CYFAgMnJSYaGhrBYLDgcDjRNIxaLce3aNYrFopx587z302azMT4+TjQalVosHo8Ht9stZxt0r83NzU2KxSIbGxvcunVLSuSLDq1eEHsb5LW0F2JD1W63+2qOR7/ZSTgSdrudsbExrly5IteOkKhfWFhgfn6excVFOTKiu3asl9lPl4cG/BfgnmEY/7nrRx8APwb+8sn3vzuKAxTFKSLaID7o4gIlohK7W2iq1aocV766ukq5XGZhYYGHDx9Sr9fJ5XJyzPhJttkcFidtp6OmO0Ih2rF299H3w0XwJO2kaRo+n4/x8XGCwSDRaBSfz7fjd1wuF7FYDI/HI9fd87Db7Zw+fZpYLIbb7WZ0dFSmpHbbpFtbJJlMsrS0JMPvvTJ/ZdDX0rPotyhtP9pJRNVFHZgoihZib2LEeSKRkBGKXonY7Yf9RCi+A/x74LamaV8+eew/8bWx/lrTtP8ArAA/OuyDa7fbLCws8POf/xyv1ys9OVEda7FYZA623W7L8eGi+LFQKEilzHq9TjKZpFAo7BD/6KcF9BxOzE7HQa1WY3l5WYYMRb6xD+lpO3m9Xqanp2k0GvseTy5kz8XgPFEr0W63pW6LWIv5fJ7PP/9cphsXFhYoFovkcrleWos9baPDpHuCsphHIbrV+mCeR9/ZSdd1rly5wsTEBOfOnZO1fmIDnM/n+eqrr7hx44Yc8NVP7KfL41+AZyWlfnC4h7OTdrvN7du3ZUXy7Ows8Xhc7qJEQZEoYrlz5w4PHz6UExLFiHLRGtU9MnzAnIkTtdNxUKlUuH//PqlUSk54fVbRbS/T63aKRCL4/f4XauXszgsLsR74faRQTBpdWVlha2uLDz/8kHv37sk2xe5NQS/Q6zY6TEwmk3TMR0ZGmJmZIZvN0mw2e96h6Ec7+f1+/uAP/oDvfOc7+Hw+RkdHsdlsJJNJ5ufnSSaT/PrXv+bTTz+V9X/9RE8rZQJSKaxarZJMJuXwFFGMKcLg9XqdRCLB5uamVA0TLWi9cqFSvDztdptKpYLdbpfjfTudjmwfVjwf48k8GTEATwwZEk6AcAz2K439LIEdUUBWrVYplUoyxZhOp0mn01L3RXR2DJJj3w+IqITVat1R3yI0SkSBruLw6dbL0XVdar6IOU9ikFi/TkvueYdCdHmICZ2ZTAa73c7Dhw+xWq2yd73dbpNMJimXy/JC1U/FRYpvptlsks/n5QTSVCqFYRjout7zyou9QqfT4dGjR3z44Yf4/X7m5uYYGxtD13XGx8dfOOIjHJPuYj4xU0eo+y0sLMg2bTGsTzj9/ZazHxScTidTU1PMzc0RjUYxmUxsb29TKpVIp9NSRElx+DQaDRYXF/H7/YyOjuL1ejGbzeRyOR49ekQqlaJQKJz0Yb40Pe9QiDYmMdFNeM67PegeVNtTHCLCgxfqcblcTk5GFJ0/im+m0+mwtLTE9vY2Xq9XDsKLRqNEIpEXcigMw2B7e5tisbjD6S8WiywuLlIoFHj06BFffPEFpVKJRCIhoxKDlm7sN5xOJ/F4nOnpabl2RBtvNpulXC73TZtiv9FoNFhdXcXhcNDpdJiZmcHpdJLP5+WGuZ9nqPS8Q9GNchi+vQin0mQykcvl5E0rnU7j8/l2OBTLy8s7pLrVZ+b3NJtNKQ6WSCTQdZ1ms8nIyAjNZhOn0ymntYp0UqfTkUWawnkQRdDdyrLNZpNKpSK7qpLJJMViUdY4qZtUbyA650R7rtlsfkrWWa2Zo6HVaskOJ13XefjwIR6Ph9XVVTmWoZ+jQ33lUCi+vbTbbSm09Nvf/pb19XU5WGe3AmMmk+Hx48dUKhWVo+/CMAwZ5Umn0xQKBW7evEk0GmV1dZVoNMr09DSXL1/G5XLJ97bRaLCyskI2m6VYLLK+vi47qUTEQ9QxiXonseMtFAovLWWvOBrq9TobGxtytlEwGJTTRZvNplozR0ilUuHWrVs8evQIt9vNP/7jP2K1Wkkmk2xtbdFsNvt6CrVyKBR9gdglA6yurrK6unrCR9SfdGtLFItFTCYT0WgUp9PJ0NAQNpuNmZkZWbAHX++qstksiUSCVCrFw4cPKZfLPHr0iPn5eRqNhiy+VPQ+rVaLYrFIJpOh3W5js9loNBqyK05FKI6OZrPZE+MZjgrlUCgU31JE+3S9Xmdzc5NarSZlskUnlc1mk8OkREh2Y2ODWq0mlWZF/YSiPyiXy9y/f59isYiu6/h8PprNpozqiZSWQvGiaMfpifaLDG2vYBjGiVQaKju9GP1sp+6R4xaLBbvdjq7rUlNC07Qd83S6CzCFQFy/6LqchJ16cS1ZrVY5Jlu0DIt0WLlcll07J2TPLwzDeP24X7QX7dTj7GknFaFQKL7FCG2Kfq4sV7wYzWaTdDp90oehGEB6dxyjQqFQKBSKvkE5FAqFQqFQKA6McigUCoVCoVAcGOVQKBQKhUKhODDKoVAoFAqFQnFglEOhUCgUCoXiwBx322gaqDz5PmiEOdzzmjjE53pRlJ32j7LT0TAodkoDyxz++fQKyk69z1Gc0552OlZhKwBN0z4/CeGSo2bQzmvQzkcwaOc1aOcjGLTzGrTzEQzaeQ3a+cDxnpNKeSgUCoVCoTgwyqFQKBQKhUJxYE7CofjJCbzmcTBo5zVo5yMYtPMatPMRDNp5Ddr5CAbtvAbtfOAYz+nYaygUCoVCoVAMHirloVAoFAqF4sAcq0Ohadq/0jTtgaZpjzRN+4vjfO3DQtO0MU3T/knTtHuapt3RNO3/ePJ4UNO0n2uaNv/ke+Ckj/VlUXbqfQbBRqDs1C8oO/UHJ22nY0t5aJpmBh4CfwisAb8D/p1hGHeP5QAOCU3TYkDMMIzrmqZ5gC+Afw38b0DWMIy/fPKBDBiG8ecneKgvhbJT7zMoNgJlp35B2ak/OGk7HWeE4k3gkWEYjw3D2Ab+B/DDY3z9Q8EwjE3DMK4/+XcJuAeM8vW5/NWTX/srvjZiP6Ls1PsMhI1A2alfUHbqD07aTsfpUIwCq13/X3vyWN+iadokcBn4DTBkGMYmfG1UIHpyR3YglJ16n4GzESg79QvKTv3BSdjpOB0KbY/H+rbFRNM0N/Az4D8ahlE86eM5RJSdep+BshEoO/ULyk79wUnZ6TgdijVgrOv/cWDjGF//0NA0zcrXxvpvhmH8rycPJ57kr0QeK3lSx3dAlJ16n4GxESg79QvKTv3BSdrpOB2K3wFnNE2b0jTNBvxb4INjfP1DQdM0DfgvwD3DMP5z148+AH785N8/Bv7uuI/tkFB26n0Gwkag7NQvKDv1Bydtp2MVttI07Y+A/wcwA//VMIz/+9he/JDQNO27wK+A20DnycP/ia/zVH8NjAMrwI8Mw8ieyEEeEGWn3mcQbATKTv2CslN/cNJ2UkqZCoVCoVAoDoxSylQoFAqFQnFglEOhUCgUCoXiwCiHQqFQKBQKxYFRDoVCoVAoFIoDoxwKhUKhUCgUB0Y5FAqFQqFQKA6McigUCoVCoVAcGOVQKBQKhUKhODD/P1q/J0wmhjMrAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 540x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_row = 2\n",
    "num_col = 5\n",
    "\n",
    "# plot images\n",
    "fig, axes = plt.subplots(num_row, num_col, figsize=(1.5*num_col,2*num_row))\n",
    "for i in range(10):\n",
    "    ax = axes[i//num_col, i%num_col]\n",
    "    ax.imshow(x_train[i], cmap='gray')\n",
    "    ax.set_title('Label: {}'.format(y_train[i]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(28, 28))  # shape of input\n",
    "z = Flatten()(x)  # 28x28 -> 784\n",
    "z = Dense(units=128, activation='relu')(z)  # dense + ReLU\n",
    "p = Dense(units=10, activation='softmax')(z)  # dense + softmax\n",
    "\n",
    "model = Model(\n",
    "    inputs=x,\n",
    "    outputs=p,\n",
    ")  # build DNN model\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])  # compile model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=3),\n",
    "    ModelCheckpoint(filepath=os.path.join('models', 'DNN', 'test.h5'), save_best_only=True),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 31us/sample - loss: 4.4174 - acc: 0.8581 - val_loss: 1.1078 - val_acc: 0.8808\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6663 - acc: 0.9026 - val_loss: 0.5716 - val_acc: 0.9073\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3501 - acc: 0.9285 - val_loss: 0.4796 - val_acc: 0.9208\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2421 - acc: 0.9434 - val_loss: 0.4246 - val_acc: 0.9248\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1833 - acc: 0.9539 - val_loss: 0.3822 - val_acc: 0.9368\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1597 - acc: 0.9579 - val_loss: 0.3789 - val_acc: 0.9355\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1484 - acc: 0.9612 - val_loss: 0.3462 - val_acc: 0.9419\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1338 - acc: 0.9652 - val_loss: 0.3158 - val_acc: 0.9477\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1273 - acc: 0.9654 - val_loss: 0.3807 - val_acc: 0.9406\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1353 - acc: 0.9650 - val_loss: 0.3348 - val_acc: 0.9493\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1244 - acc: 0.9681 - val_loss: 0.3319 - val_acc: 0.9455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f1da4360790>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train\n",
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, callbacks=callbacks, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9479"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see accuracy\n",
    "\n",
    "accuracy_score(y_test, model.predict(x_test).argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of this model is 94.79% (in the author's environment.) Pretty good!\n",
    "\n",
    "このモデルの精度は（筆者の環境では） 94.79% です。\n",
    "悪くない！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class DenseModel:\n",
    "    def __init__(self, layers=1, hid_dim=128):\n",
    "        self.input = Input(shape=(28, 28), name='input')\n",
    "        self.flatten = Flatten(name='flatten')\n",
    "        self.denses = OrderedDict()\n",
    "        for i in range(layers):\n",
    "            name = 'dense_{}'.format(i)\n",
    "            self.denses[name] = Dense(units=hid_dim, activation='relu', name=name)\n",
    "        self.last = Dense(units=10, activation='softmax', name='last')\n",
    "    \n",
    "    \n",
    "    def build(self):\n",
    "        x = self.input\n",
    "        z = self.flatten(x)\n",
    "        for dense in self.denses.values():\n",
    "            z = dense(z)\n",
    "        p = self.last(z)\n",
    "        \n",
    "        model = Model(inputs=x, outputs=p)\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== layers: 1 ; hid_dim: 1 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 2.3027 - acc: 0.1138 - val_loss: 2.3018 - val_acc: 0.1061\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3011 - val_acc: 0.1065\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.2912 - acc: 0.1210 - val_loss: 2.2093 - val_acc: 0.1506\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.1874 - acc: 0.1694 - val_loss: 2.1329 - val_acc: 0.1814\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.1305 - acc: 0.1887 - val_loss: 2.0865 - val_acc: 0.2006\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0829 - acc: 0.2017 - val_loss: 2.0559 - val_acc: 0.2040\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0588 - acc: 0.2113 - val_loss: 2.0389 - val_acc: 0.2166\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0343 - acc: 0.2201 - val_loss: 2.0122 - val_acc: 0.2204\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0021 - acc: 0.2276 - val_loss: 1.9914 - val_acc: 0.2218\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9765 - acc: 0.2361 - val_loss: 1.9549 - val_acc: 0.2384\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.9338 - acc: 0.2451 - val_loss: 1.9065 - val_acc: 0.2349\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8728 - acc: 0.2627 - val_loss: 1.8240 - val_acc: 0.2792\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.8126 - acc: 0.2777 - val_loss: 1.7870 - val_acc: 0.2810\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7899 - acc: 0.2786 - val_loss: 1.7719 - val_acc: 0.2748\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7786 - acc: 0.2788 - val_loss: 1.7677 - val_acc: 0.2930\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7700 - acc: 0.2789 - val_loss: 1.7552 - val_acc: 0.2875\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7656 - acc: 0.2794 - val_loss: 1.7464 - val_acc: 0.2921\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.7580 - acc: 0.2831 - val_loss: 1.7525 - val_acc: 0.2822\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7547 - acc: 0.2857 - val_loss: 1.7404 - val_acc: 0.2779\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7509 - acc: 0.2869 - val_loss: 1.7366 - val_acc: 0.2937\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.7508 - acc: 0.2848 - val_loss: 1.7453 - val_acc: 0.2760\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7478 - acc: 0.2878 - val_loss: 1.7355 - val_acc: 0.2791\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7451 - acc: 0.2875 - val_loss: 1.7354 - val_acc: 0.2808\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.7415 - acc: 0.2900 - val_loss: 1.7380 - val_acc: 0.2933\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7414 - acc: 0.2891 - val_loss: 1.7277 - val_acc: 0.2974\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7372 - acc: 0.2935 - val_loss: 1.7345 - val_acc: 0.2808\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.7363 - acc: 0.2941 - val_loss: 1.7294 - val_acc: 0.2871\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7347 - acc: 0.2951 - val_loss: 1.7170 - val_acc: 0.3086\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.7292 - acc: 0.2978 - val_loss: 1.7166 - val_acc: 0.3001\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.7252 - acc: 0.2983 - val_loss: 1.7178 - val_acc: 0.2772\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7183 - acc: 0.2997 - val_loss: 1.7044 - val_acc: 0.2936\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7146 - acc: 0.2957 - val_loss: 1.7032 - val_acc: 0.2948\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.7086 - acc: 0.3016 - val_loss: 1.6960 - val_acc: 0.3075\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7051 - acc: 0.2981 - val_loss: 1.6940 - val_acc: 0.3063\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.7025 - acc: 0.2979 - val_loss: 1.6919 - val_acc: 0.3031\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6997 - acc: 0.3013 - val_loss: 1.6858 - val_acc: 0.2981\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6985 - acc: 0.2989 - val_loss: 1.6999 - val_acc: 0.2853\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6941 - acc: 0.2993 - val_loss: 1.6803 - val_acc: 0.2989\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6936 - acc: 0.2943 - val_loss: 1.6996 - val_acc: 0.2629\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6905 - acc: 0.2961 - val_loss: 1.6819 - val_acc: 0.2808\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6890 - acc: 0.2946 - val_loss: 1.6768 - val_acc: 0.2956\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6885 - acc: 0.2939 - val_loss: 1.6732 - val_acc: 0.2988\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6858 - acc: 0.2929 - val_loss: 1.6757 - val_acc: 0.2929\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6846 - acc: 0.2925 - val_loss: 1.6694 - val_acc: 0.2953\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6852 - acc: 0.2923 - val_loss: 1.6740 - val_acc: 0.3036\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6857 - acc: 0.2910 - val_loss: 1.6723 - val_acc: 0.2837\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6826 - acc: 0.2882 - val_loss: 1.6732 - val_acc: 0.2931\n",
      "======== layers: 1 ; hid_dim: 2 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 2.4942 - acc: 0.1142 - val_loss: 2.3024 - val_acc: 0.1075\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.2923 - acc: 0.1195 - val_loss: 2.2691 - val_acc: 0.1223\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 2.2515 - acc: 0.1437 - val_loss: 2.1954 - val_acc: 0.1594\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.1686 - acc: 0.1761 - val_loss: 2.1489 - val_acc: 0.1850\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.1231 - acc: 0.1893 - val_loss: 2.1132 - val_acc: 0.1830\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.1027 - acc: 0.1943 - val_loss: 2.0973 - val_acc: 0.1864\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 2.0813 - acc: 0.1980 - val_loss: 2.0703 - val_acc: 0.1944\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 2.0572 - acc: 0.2086 - val_loss: 2.0229 - val_acc: 0.2377\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.9380 - acc: 0.2440 - val_loss: 1.8749 - val_acc: 0.2458\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.8414 - acc: 0.2559 - val_loss: 1.8080 - val_acc: 0.2585\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7810 - acc: 0.2780 - val_loss: 1.7462 - val_acc: 0.2838\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6816 - acc: 0.3219 - val_loss: 1.6610 - val_acc: 0.3410\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.6041 - acc: 0.3479 - val_loss: 1.6133 - val_acc: 0.3582\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.5517 - acc: 0.3702 - val_loss: 1.5176 - val_acc: 0.4173\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.5019 - acc: 0.4065 - val_loss: 1.4876 - val_acc: 0.4183\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.4568 - acc: 0.4288 - val_loss: 1.4677 - val_acc: 0.4473\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.4351 - acc: 0.4350 - val_loss: 1.4777 - val_acc: 0.4403\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.4081 - acc: 0.4468 - val_loss: 1.4185 - val_acc: 0.4507\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3946 - acc: 0.4549 - val_loss: 1.4157 - val_acc: 0.4572\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.3758 - acc: 0.4590 - val_loss: 1.4172 - val_acc: 0.4377\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.3595 - acc: 0.4666 - val_loss: 1.3499 - val_acc: 0.4837\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3312 - acc: 0.4787 - val_loss: 1.3491 - val_acc: 0.4642\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3218 - acc: 0.4830 - val_loss: 1.3425 - val_acc: 0.4618\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3138 - acc: 0.4823 - val_loss: 1.3371 - val_acc: 0.4817\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.3052 - acc: 0.4902 - val_loss: 1.3234 - val_acc: 0.4780\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2941 - acc: 0.4912 - val_loss: 1.3096 - val_acc: 0.4929\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2835 - acc: 0.4991 - val_loss: 1.2860 - val_acc: 0.5088\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2700 - acc: 0.5041 - val_loss: 1.2875 - val_acc: 0.4970\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2584 - acc: 0.5048 - val_loss: 1.2495 - val_acc: 0.5206\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.2434 - acc: 0.5107 - val_loss: 1.2415 - val_acc: 0.5208\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2309 - acc: 0.5219 - val_loss: 1.2156 - val_acc: 0.5192\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2033 - acc: 0.5409 - val_loss: 1.1739 - val_acc: 0.5592\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1623 - acc: 0.5782 - val_loss: 1.1329 - val_acc: 0.5817\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.1390 - acc: 0.5966 - val_loss: 1.1020 - val_acc: 0.6140\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.1269 - acc: 0.6065 - val_loss: 1.1434 - val_acc: 0.5936\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1152 - acc: 0.6150 - val_loss: 1.1227 - val_acc: 0.6047\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.1099 - acc: 0.6177 - val_loss: 1.0873 - val_acc: 0.6288\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.1043 - acc: 0.6174 - val_loss: 1.0900 - val_acc: 0.6321\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0976 - acc: 0.6212 - val_loss: 1.0795 - val_acc: 0.6348\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.0940 - acc: 0.6251 - val_loss: 1.0726 - val_acc: 0.6374\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.0901 - acc: 0.6270 - val_loss: 1.0883 - val_acc: 0.6192\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0876 - acc: 0.6275 - val_loss: 1.0636 - val_acc: 0.6378\n",
      "Epoch 43/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.0886 - acc: 0.6278 - val_loss: 1.0739 - val_acc: 0.6391\n",
      "Epoch 44/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.0869 - acc: 0.6276 - val_loss: 1.0617 - val_acc: 0.6436\n",
      "Epoch 45/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0849 - acc: 0.6271 - val_loss: 1.0548 - val_acc: 0.6382\n",
      "Epoch 46/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.0808 - acc: 0.6309 - val_loss: 1.0723 - val_acc: 0.6328\n",
      "Epoch 47/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0807 - acc: 0.6300 - val_loss: 1.0486 - val_acc: 0.6484\n",
      "Epoch 48/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0822 - acc: 0.6285 - val_loss: 1.1075 - val_acc: 0.6102\n",
      "Epoch 49/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0811 - acc: 0.6289 - val_loss: 1.0698 - val_acc: 0.6302\n",
      "Epoch 50/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.0792 - acc: 0.6318 - val_loss: 1.0496 - val_acc: 0.6478\n",
      "======== layers: 1 ; hid_dim: 4 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 2.9244 - acc: 0.1114 - val_loss: 2.3064 - val_acc: 0.1053\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3027 - acc: 0.1137 - val_loss: 2.3035 - val_acc: 0.1056\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3014 - acc: 0.1138 - val_loss: 2.3029 - val_acc: 0.1057\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1139 - val_loss: 2.3026 - val_acc: 0.1058\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3026 - val_acc: 0.1058\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3025 - val_acc: 0.1058\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3025 - val_acc: 0.1058\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3026 - val_acc: 0.1058\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3027 - val_acc: 0.1058\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3026 - val_acc: 0.1058\n",
      "======== layers: 1 ; hid_dim: 8 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 28us/sample - loss: 4.3154 - acc: 0.1468 - val_loss: 2.1665 - val_acc: 0.1562\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.0335 - acc: 0.2320 - val_loss: 1.9270 - val_acc: 0.2674\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 1.8771 - acc: 0.2797 - val_loss: 1.8549 - val_acc: 0.2878\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7873 - acc: 0.3080 - val_loss: 1.8102 - val_acc: 0.3222\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.7185 - acc: 0.3324 - val_loss: 1.7575 - val_acc: 0.3324\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6524 - acc: 0.3568 - val_loss: 1.6577 - val_acc: 0.3628\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5684 - acc: 0.3820 - val_loss: 1.5534 - val_acc: 0.3919\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5007 - acc: 0.3946 - val_loss: 1.4899 - val_acc: 0.3973\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.4553 - acc: 0.4062 - val_loss: 1.4886 - val_acc: 0.4225\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.4210 - acc: 0.4139 - val_loss: 1.4570 - val_acc: 0.4053\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3913 - acc: 0.4242 - val_loss: 1.4131 - val_acc: 0.4204\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3524 - acc: 0.4342 - val_loss: 1.3826 - val_acc: 0.4272\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.2673 - acc: 0.4756 - val_loss: 1.1831 - val_acc: 0.5283\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1214 - acc: 0.5436 - val_loss: 1.1204 - val_acc: 0.5521\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0540 - acc: 0.5744 - val_loss: 1.0650 - val_acc: 0.5993\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9958 - acc: 0.6051 - val_loss: 1.0244 - val_acc: 0.6135\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9540 - acc: 0.6251 - val_loss: 0.9827 - val_acc: 0.6290\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9187 - acc: 0.6478 - val_loss: 0.9650 - val_acc: 0.6513\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8826 - acc: 0.6708 - val_loss: 0.8707 - val_acc: 0.6885\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8370 - acc: 0.7057 - val_loss: 0.8478 - val_acc: 0.7133\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7959 - acc: 0.7389 - val_loss: 0.8103 - val_acc: 0.7365\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7496 - acc: 0.7630 - val_loss: 0.7359 - val_acc: 0.7749\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7131 - acc: 0.7768 - val_loss: 0.7352 - val_acc: 0.7751\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6863 - acc: 0.7840 - val_loss: 0.6738 - val_acc: 0.7923\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6743 - acc: 0.7850 - val_loss: 0.6680 - val_acc: 0.7896\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6599 - acc: 0.7898 - val_loss: 0.6818 - val_acc: 0.7785\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6525 - acc: 0.7910 - val_loss: 0.6655 - val_acc: 0.7842\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6450 - acc: 0.7945 - val_loss: 0.6558 - val_acc: 0.7792\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6300 - acc: 0.8025 - val_loss: 0.6109 - val_acc: 0.8091\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.6141 - acc: 0.8061 - val_loss: 0.6135 - val_acc: 0.8076\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6097 - acc: 0.8080 - val_loss: 0.6071 - val_acc: 0.8038\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5952 - acc: 0.8135 - val_loss: 0.5756 - val_acc: 0.8169\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5882 - acc: 0.8167 - val_loss: 0.6031 - val_acc: 0.8129\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5802 - acc: 0.8208 - val_loss: 0.5503 - val_acc: 0.8322\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.5707 - acc: 0.8266 - val_loss: 0.5580 - val_acc: 0.8292\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.5689 - acc: 0.8298 - val_loss: 0.5506 - val_acc: 0.8339\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5578 - acc: 0.8359 - val_loss: 0.5313 - val_acc: 0.8414\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5538 - acc: 0.8369 - val_loss: 0.5395 - val_acc: 0.8418\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.5450 - acc: 0.8423 - val_loss: 0.5613 - val_acc: 0.8341\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.5414 - acc: 0.8426 - val_loss: 0.5354 - val_acc: 0.8428\n",
      "======== layers: 1 ; hid_dim: 16 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 4.2057 - acc: 0.2180 - val_loss: 1.9181 - val_acc: 0.2714\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8082 - acc: 0.3034 - val_loss: 1.6728 - val_acc: 0.3332\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5851 - acc: 0.3896 - val_loss: 1.4758 - val_acc: 0.4457\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.3032 - acc: 0.5191 - val_loss: 1.1635 - val_acc: 0.5943\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0378 - acc: 0.6409 - val_loss: 0.9480 - val_acc: 0.6819\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.8755 - acc: 0.6972 - val_loss: 0.8673 - val_acc: 0.7113\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.7904 - acc: 0.7192 - val_loss: 0.7673 - val_acc: 0.7224\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7270 - acc: 0.7385 - val_loss: 0.7486 - val_acc: 0.7455\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6819 - acc: 0.7581 - val_loss: 0.7049 - val_acc: 0.7772\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6488 - acc: 0.7790 - val_loss: 0.6761 - val_acc: 0.7940\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6071 - acc: 0.8150 - val_loss: 0.6173 - val_acc: 0.8219\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5587 - acc: 0.8365 - val_loss: 0.5687 - val_acc: 0.8404\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5217 - acc: 0.8516 - val_loss: 0.5279 - val_acc: 0.8518\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4825 - acc: 0.8631 - val_loss: 0.4857 - val_acc: 0.8689\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4470 - acc: 0.8765 - val_loss: 0.4402 - val_acc: 0.8821\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4081 - acc: 0.8862 - val_loss: 0.4228 - val_acc: 0.8839\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3845 - acc: 0.8929 - val_loss: 0.4044 - val_acc: 0.8912\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3644 - acc: 0.8980 - val_loss: 0.3790 - val_acc: 0.8950\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3528 - acc: 0.9003 - val_loss: 0.3701 - val_acc: 0.8989\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3501 - acc: 0.9006 - val_loss: 0.3554 - val_acc: 0.9028\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3414 - acc: 0.9028 - val_loss: 0.3760 - val_acc: 0.8945\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3364 - acc: 0.9045 - val_loss: 0.3567 - val_acc: 0.9013\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3341 - acc: 0.9037 - val_loss: 0.3659 - val_acc: 0.9022\n",
      "======== layers: 1 ; hid_dim: 32 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 3.7594 - acc: 0.5383 - val_loss: 1.1067 - val_acc: 0.6827\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0092 - acc: 0.7209 - val_loss: 0.8511 - val_acc: 0.7723\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7758 - acc: 0.7977 - val_loss: 0.7159 - val_acc: 0.8108\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6316 - acc: 0.8361 - val_loss: 0.6272 - val_acc: 0.8403\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5532 - acc: 0.8526 - val_loss: 0.5587 - val_acc: 0.8684\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.4867 - acc: 0.8695 - val_loss: 0.5223 - val_acc: 0.8734\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4414 - acc: 0.8838 - val_loss: 0.4719 - val_acc: 0.8878\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3983 - acc: 0.8937 - val_loss: 0.4181 - val_acc: 0.9018\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3649 - acc: 0.9024 - val_loss: 0.4059 - val_acc: 0.9002\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3365 - acc: 0.9103 - val_loss: 0.3630 - val_acc: 0.9115\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.3107 - acc: 0.9165 - val_loss: 0.3370 - val_acc: 0.9122\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2996 - acc: 0.9204 - val_loss: 0.3438 - val_acc: 0.9114\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2796 - acc: 0.9250 - val_loss: 0.3183 - val_acc: 0.9199\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2605 - acc: 0.9294 - val_loss: 0.2930 - val_acc: 0.9242\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2510 - acc: 0.9317 - val_loss: 0.2890 - val_acc: 0.9302\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2395 - acc: 0.9353 - val_loss: 0.2781 - val_acc: 0.9285\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2333 - acc: 0.9352 - val_loss: 0.2624 - val_acc: 0.9342\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2260 - acc: 0.9389 - val_loss: 0.2805 - val_acc: 0.9270\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2196 - acc: 0.9400 - val_loss: 0.2672 - val_acc: 0.9362\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2147 - acc: 0.9408 - val_loss: 0.2745 - val_acc: 0.9302\n",
      "======== layers: 1 ; hid_dim: 64 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 5.1221 - acc: 0.7773 - val_loss: 1.0029 - val_acc: 0.8190\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.7679 - acc: 0.8445 - val_loss: 0.6869 - val_acc: 0.8755\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5355 - acc: 0.8830 - val_loss: 0.5678 - val_acc: 0.8910\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4180 - acc: 0.9026 - val_loss: 0.4843 - val_acc: 0.9116\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3513 - acc: 0.9145 - val_loss: 0.4374 - val_acc: 0.9137\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2998 - acc: 0.9250 - val_loss: 0.4002 - val_acc: 0.9243\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2583 - acc: 0.9336 - val_loss: 0.3924 - val_acc: 0.9252\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2381 - acc: 0.9376 - val_loss: 0.3581 - val_acc: 0.9277\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2055 - acc: 0.9444 - val_loss: 0.3348 - val_acc: 0.9357\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1941 - acc: 0.9461 - val_loss: 0.3235 - val_acc: 0.9406\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1758 - acc: 0.9511 - val_loss: 0.3155 - val_acc: 0.9427\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1652 - acc: 0.9535 - val_loss: 0.3159 - val_acc: 0.9386\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1628 - acc: 0.9540 - val_loss: 0.2840 - val_acc: 0.9439\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1479 - acc: 0.9571 - val_loss: 0.2902 - val_acc: 0.9450\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1441 - acc: 0.9601 - val_loss: 0.2640 - val_acc: 0.9480\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1319 - acc: 0.9615 - val_loss: 0.2600 - val_acc: 0.9492\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1315 - acc: 0.9629 - val_loss: 0.2718 - val_acc: 0.9463\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1281 - acc: 0.9631 - val_loss: 0.2824 - val_acc: 0.9472\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1211 - acc: 0.9646 - val_loss: 0.2434 - val_acc: 0.9513\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1163 - acc: 0.9663 - val_loss: 0.2482 - val_acc: 0.9491\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1145 - acc: 0.9670 - val_loss: 0.2550 - val_acc: 0.9503\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1132 - acc: 0.9678 - val_loss: 0.2740 - val_acc: 0.9503\n",
      "======== layers: 1 ; hid_dim: 128 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 5.8222 - acc: 0.8407 - val_loss: 1.2115 - val_acc: 0.8727\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7120 - acc: 0.8913 - val_loss: 0.5661 - val_acc: 0.8992\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3822 - acc: 0.9188 - val_loss: 0.4691 - val_acc: 0.9171\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.2710 - acc: 0.9350 - val_loss: 0.3916 - val_acc: 0.9275\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2131 - acc: 0.9440 - val_loss: 0.3558 - val_acc: 0.9337\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1796 - acc: 0.9516 - val_loss: 0.3728 - val_acc: 0.9325\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1589 - acc: 0.9566 - val_loss: 0.3453 - val_acc: 0.9393\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1507 - acc: 0.9585 - val_loss: 0.3270 - val_acc: 0.9446\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1402 - acc: 0.9613 - val_loss: 0.3191 - val_acc: 0.9440\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1304 - acc: 0.9629 - val_loss: 0.3090 - val_acc: 0.9428\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1331 - acc: 0.9646 - val_loss: 0.3083 - val_acc: 0.9477\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1313 - acc: 0.9653 - val_loss: 0.2860 - val_acc: 0.9498\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1175 - acc: 0.9672 - val_loss: 0.2588 - val_acc: 0.9494\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1255 - acc: 0.9659 - val_loss: 0.2533 - val_acc: 0.9525\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1088 - acc: 0.9703 - val_loss: 0.2568 - val_acc: 0.9469\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1074 - acc: 0.9711 - val_loss: 0.2702 - val_acc: 0.9506\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1045 - acc: 0.9718 - val_loss: 0.2892 - val_acc: 0.9492\n",
      "======== layers: 1 ; hid_dim: 256 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 4.5086 - acc: 0.8916 - val_loss: 1.3706 - val_acc: 0.9302\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8232 - acc: 0.9455 - val_loss: 0.8250 - val_acc: 0.9406\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4142 - acc: 0.9585 - val_loss: 0.6479 - val_acc: 0.9500\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2681 - acc: 0.9677 - val_loss: 0.5589 - val_acc: 0.9492\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1954 - acc: 0.9734 - val_loss: 0.5332 - val_acc: 0.9528\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1761 - acc: 0.9746 - val_loss: 0.5736 - val_acc: 0.9528\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1570 - acc: 0.9774 - val_loss: 0.5337 - val_acc: 0.9536\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1491 - acc: 0.9783 - val_loss: 0.5236 - val_acc: 0.9532\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1554 - acc: 0.9771 - val_loss: 0.5381 - val_acc: 0.9554\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1537 - acc: 0.9782 - val_loss: 0.5059 - val_acc: 0.9539\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1363 - acc: 0.9796 - val_loss: 0.5136 - val_acc: 0.9578\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1394 - acc: 0.9791 - val_loss: 0.5272 - val_acc: 0.9615\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1383 - acc: 0.9815 - val_loss: 0.5037 - val_acc: 0.9631\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1326 - acc: 0.9820 - val_loss: 0.5954 - val_acc: 0.9614\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1193 - acc: 0.9829 - val_loss: 0.5229 - val_acc: 0.9643\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1461 - acc: 0.9807 - val_loss: 0.5482 - val_acc: 0.9649\n",
      "======== layers: 1 ; hid_dim: 512 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 4.9620 - acc: 0.8987 - val_loss: 1.1509 - val_acc: 0.9392\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6910 - acc: 0.9533 - val_loss: 0.6953 - val_acc: 0.9502\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3165 - acc: 0.9671 - val_loss: 0.6479 - val_acc: 0.9542\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2244 - acc: 0.9742 - val_loss: 0.6362 - val_acc: 0.9542\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1978 - acc: 0.9767 - val_loss: 0.5821 - val_acc: 0.9571\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1881 - acc: 0.9778 - val_loss: 0.6071 - val_acc: 0.9593\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1892 - acc: 0.9784 - val_loss: 0.5651 - val_acc: 0.9617\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1974 - acc: 0.9783 - val_loss: 0.6683 - val_acc: 0.9582\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1827 - acc: 0.9801 - val_loss: 0.5188 - val_acc: 0.9625\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1761 - acc: 0.9808 - val_loss: 0.7230 - val_acc: 0.9535\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1627 - acc: 0.9815 - val_loss: 0.6319 - val_acc: 0.9576\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 17us/sample - loss: 0.1929 - acc: 0.9804 - val_loss: 0.6482 - val_acc: 0.9643\n",
      "======== layers: 1 ; hid_dim: 1024 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 5.0823 - acc: 0.9092 - val_loss: 1.1500 - val_acc: 0.9442\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6215 - acc: 0.9594 - val_loss: 0.9482 - val_acc: 0.9470\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3361 - acc: 0.9699 - val_loss: 0.6292 - val_acc: 0.9582\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2349 - acc: 0.9761 - val_loss: 0.6268 - val_acc: 0.9575\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2166 - acc: 0.9767 - val_loss: 0.5906 - val_acc: 0.9573\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1860 - acc: 0.9791 - val_loss: 0.7466 - val_acc: 0.9560\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2084 - acc: 0.9782 - val_loss: 0.5351 - val_acc: 0.9638\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1942 - acc: 0.9793 - val_loss: 0.6601 - val_acc: 0.9600\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2455 - acc: 0.9779 - val_loss: 0.8333 - val_acc: 0.9573\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2249 - acc: 0.9779 - val_loss: 0.7367 - val_acc: 0.9637\n",
      "======== layers: 2 ; hid_dim: 1 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 2.3016 - acc: 0.1133 - val_loss: 2.3019 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "======== layers: 2 ; hid_dim: 2 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 2.9498 - acc: 0.1132 - val_loss: 2.3019 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3046 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3035 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3021 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "======== layers: 2 ; hid_dim: 4 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 2.9994 - acc: 0.1114 - val_loss: 2.3033 - val_acc: 0.1058\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3030 - acc: 0.1139 - val_loss: 2.3027 - val_acc: 0.1059\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3017 - acc: 0.1140 - val_loss: 2.3024 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3013 - acc: 0.1140 - val_loss: 2.3023 - val_acc: 0.1060\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3010 - acc: 0.1140 - val_loss: 2.3025 - val_acc: 0.1060\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3009 - acc: 0.1140 - val_loss: 2.3025 - val_acc: 0.1060\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.3009 - acc: 0.1140 - val_loss: 2.3026 - val_acc: 0.1060\n",
      "======== layers: 2 ; hid_dim: 8 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 3.6541 - acc: 0.1198 - val_loss: 2.2686 - val_acc: 0.1368\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 2.1087 - acc: 0.2145 - val_loss: 1.9418 - val_acc: 0.2647\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.8575 - acc: 0.2951 - val_loss: 1.7381 - val_acc: 0.3061\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.6946 - acc: 0.3204 - val_loss: 1.6498 - val_acc: 0.3152\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6173 - acc: 0.3354 - val_loss: 1.5938 - val_acc: 0.3418\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.5620 - acc: 0.3570 - val_loss: 1.4933 - val_acc: 0.4012\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.4312 - acc: 0.4453 - val_loss: 1.3756 - val_acc: 0.4659\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3162 - acc: 0.5031 - val_loss: 1.2353 - val_acc: 0.5412\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1959 - acc: 0.5494 - val_loss: 1.1364 - val_acc: 0.5633\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.1151 - acc: 0.5839 - val_loss: 1.0879 - val_acc: 0.5774\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0612 - acc: 0.6079 - val_loss: 1.0334 - val_acc: 0.6109\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0282 - acc: 0.6143 - val_loss: 1.0154 - val_acc: 0.6140\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9888 - acc: 0.6854 - val_loss: 0.9809 - val_acc: 0.6888\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9480 - acc: 0.7074 - val_loss: 0.9301 - val_acc: 0.7143\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9191 - acc: 0.7147 - val_loss: 0.8747 - val_acc: 0.7308\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.8980 - acc: 0.7206 - val_loss: 0.8835 - val_acc: 0.7264\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.8680 - acc: 0.7305 - val_loss: 0.8359 - val_acc: 0.7440\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.8441 - acc: 0.7430 - val_loss: 0.8081 - val_acc: 0.7628\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.8212 - acc: 0.7527 - val_loss: 0.7992 - val_acc: 0.7613\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7906 - acc: 0.7616 - val_loss: 0.7674 - val_acc: 0.7713\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7735 - acc: 0.7699 - val_loss: 0.7985 - val_acc: 0.7632\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7623 - acc: 0.7754 - val_loss: 0.7170 - val_acc: 0.7958\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7367 - acc: 0.7824 - val_loss: 0.6909 - val_acc: 0.8016\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6988 - acc: 0.7969 - val_loss: 0.6397 - val_acc: 0.8133\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6611 - acc: 0.8104 - val_loss: 0.6228 - val_acc: 0.8232\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6370 - acc: 0.8183 - val_loss: 0.5946 - val_acc: 0.8253\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6160 - acc: 0.8223 - val_loss: 0.6047 - val_acc: 0.8260\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6008 - acc: 0.8258 - val_loss: 0.5914 - val_acc: 0.8226\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5899 - acc: 0.8260 - val_loss: 0.5874 - val_acc: 0.8192\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5765 - acc: 0.8305 - val_loss: 0.5607 - val_acc: 0.8407\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5698 - acc: 0.8381 - val_loss: 0.5644 - val_acc: 0.8367\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5598 - acc: 0.8396 - val_loss: 0.5364 - val_acc: 0.8475\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5504 - acc: 0.8459 - val_loss: 0.5522 - val_acc: 0.8406\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5470 - acc: 0.8430 - val_loss: 0.5763 - val_acc: 0.8350\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5436 - acc: 0.8442 - val_loss: 0.5589 - val_acc: 0.8320\n",
      "======== layers: 2 ; hid_dim: 16 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 3.4227 - acc: 0.3596 - val_loss: 1.4753 - val_acc: 0.4940\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.2609 - acc: 0.5798 - val_loss: 1.0824 - val_acc: 0.6473\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.9900 - acc: 0.6761 - val_loss: 0.9012 - val_acc: 0.7180\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.7969 - acc: 0.7534 - val_loss: 0.7303 - val_acc: 0.7789\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6658 - acc: 0.8100 - val_loss: 0.6438 - val_acc: 0.8311\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5693 - acc: 0.8412 - val_loss: 0.5338 - val_acc: 0.8556\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.5052 - acc: 0.8599 - val_loss: 0.4831 - val_acc: 0.8709\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4636 - acc: 0.8702 - val_loss: 0.4515 - val_acc: 0.8794\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4360 - acc: 0.8782 - val_loss: 0.4689 - val_acc: 0.8777\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4132 - acc: 0.8852 - val_loss: 0.4088 - val_acc: 0.8900\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4028 - acc: 0.8881 - val_loss: 0.3862 - val_acc: 0.8946\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3794 - acc: 0.8940 - val_loss: 0.4038 - val_acc: 0.8941\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3705 - acc: 0.8954 - val_loss: 0.3741 - val_acc: 0.8960\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3589 - acc: 0.9004 - val_loss: 0.3511 - val_acc: 0.9033\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3524 - acc: 0.9019 - val_loss: 0.3395 - val_acc: 0.9062\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3366 - acc: 0.9057 - val_loss: 0.3549 - val_acc: 0.9065\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3266 - acc: 0.9090 - val_loss: 0.3389 - val_acc: 0.9074\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3194 - acc: 0.9105 - val_loss: 0.3285 - val_acc: 0.9107\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3148 - acc: 0.9103 - val_loss: 0.3345 - val_acc: 0.9108\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3036 - acc: 0.9139 - val_loss: 0.3487 - val_acc: 0.9051\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3004 - acc: 0.9156 - val_loss: 0.3347 - val_acc: 0.9099\n",
      "======== layers: 2 ; hid_dim: 32 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 3.6156 - acc: 0.4343 - val_loss: 1.2966 - val_acc: 0.5232\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 1.0874 - acc: 0.6187 - val_loss: 0.9110 - val_acc: 0.6733\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.8090 - acc: 0.7267 - val_loss: 0.7418 - val_acc: 0.7503\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.6368 - acc: 0.8067 - val_loss: 0.5869 - val_acc: 0.8397\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.4840 - acc: 0.8687 - val_loss: 0.4406 - val_acc: 0.8883\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.3963 - acc: 0.8955 - val_loss: 0.3829 - val_acc: 0.9057\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3507 - acc: 0.9070 - val_loss: 0.3614 - val_acc: 0.9093\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3214 - acc: 0.9148 - val_loss: 0.3409 - val_acc: 0.9150\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3077 - acc: 0.9193 - val_loss: 0.3419 - val_acc: 0.9190\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2998 - acc: 0.9224 - val_loss: 0.3277 - val_acc: 0.9236\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.2743 - acc: 0.9275 - val_loss: 0.3001 - val_acc: 0.9256\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2700 - acc: 0.9291 - val_loss: 0.3225 - val_acc: 0.9234\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2676 - acc: 0.9299 - val_loss: 0.2897 - val_acc: 0.9283\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2547 - acc: 0.9318 - val_loss: 0.2978 - val_acc: 0.9275\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2422 - acc: 0.9339 - val_loss: 0.2716 - val_acc: 0.9350\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2356 - acc: 0.9370 - val_loss: 0.2678 - val_acc: 0.9325\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2331 - acc: 0.9372 - val_loss: 0.3048 - val_acc: 0.9296\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2271 - acc: 0.9390 - val_loss: 0.2717 - val_acc: 0.9332\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2152 - acc: 0.9426 - val_loss: 0.2784 - val_acc: 0.9327\n",
      "======== layers: 2 ; hid_dim: 64 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 3.2090 - acc: 0.7628 - val_loss: 0.6806 - val_acc: 0.8420\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5562 - acc: 0.8660 - val_loss: 0.4550 - val_acc: 0.8849\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3855 - acc: 0.8970 - val_loss: 0.3642 - val_acc: 0.9101\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3090 - acc: 0.9165 - val_loss: 0.3414 - val_acc: 0.9190\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2671 - acc: 0.9262 - val_loss: 0.3252 - val_acc: 0.9217\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2417 - acc: 0.9329 - val_loss: 0.3133 - val_acc: 0.9275\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2139 - acc: 0.9399 - val_loss: 0.3145 - val_acc: 0.9312\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1969 - acc: 0.9441 - val_loss: 0.2924 - val_acc: 0.9293\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1919 - acc: 0.9471 - val_loss: 0.2998 - val_acc: 0.9349\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1794 - acc: 0.9505 - val_loss: 0.2945 - val_acc: 0.9365\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1716 - acc: 0.9514 - val_loss: 0.2989 - val_acc: 0.9394\n",
      "======== layers: 2 ; hid_dim: 128 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 2.9724 - acc: 0.8506 - val_loss: 0.8787 - val_acc: 0.9018\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5839 - acc: 0.9212 - val_loss: 0.5754 - val_acc: 0.9242\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3401 - acc: 0.9441 - val_loss: 0.4606 - val_acc: 0.9308\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2505 - acc: 0.9532 - val_loss: 0.4270 - val_acc: 0.9406\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1939 - acc: 0.9613 - val_loss: 0.3836 - val_acc: 0.9443\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1711 - acc: 0.9643 - val_loss: 0.4337 - val_acc: 0.9382\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1510 - acc: 0.9677 - val_loss: 0.3494 - val_acc: 0.9517\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1476 - acc: 0.9693 - val_loss: 0.3894 - val_acc: 0.9548\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1600 - acc: 0.9678 - val_loss: 0.3490 - val_acc: 0.9527\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1367 - acc: 0.9727 - val_loss: 0.3328 - val_acc: 0.9550\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1220 - acc: 0.9751 - val_loss: 0.3715 - val_acc: 0.9506\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1281 - acc: 0.9748 - val_loss: 0.3530 - val_acc: 0.9550\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1143 - acc: 0.9769 - val_loss: 0.4154 - val_acc: 0.9513\n",
      "======== layers: 2 ; hid_dim: 256 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 3.0501 - acc: 0.8769 - val_loss: 0.6949 - val_acc: 0.9241\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4578 - acc: 0.9400 - val_loss: 0.5226 - val_acc: 0.9351\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2524 - acc: 0.9594 - val_loss: 0.4259 - val_acc: 0.9383\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1984 - acc: 0.9653 - val_loss: 0.4134 - val_acc: 0.9489\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1490 - acc: 0.9718 - val_loss: 0.3617 - val_acc: 0.9529\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1346 - acc: 0.9735 - val_loss: 0.3775 - val_acc: 0.9488\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.1479 - acc: 0.9737 - val_loss: 0.3720 - val_acc: 0.9515\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1198 - acc: 0.9762 - val_loss: 0.3452 - val_acc: 0.9550\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1128 - acc: 0.9784 - val_loss: 0.2926 - val_acc: 0.9628\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.1038 - acc: 0.9789 - val_loss: 0.3470 - val_acc: 0.9579\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0984 - acc: 0.9795 - val_loss: 0.2892 - val_acc: 0.9586\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0949 - acc: 0.9810 - val_loss: 0.3068 - val_acc: 0.9570\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0842 - acc: 0.9814 - val_loss: 0.2658 - val_acc: 0.9647\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0758 - acc: 0.9833 - val_loss: 0.2684 - val_acc: 0.9647\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0772 - acc: 0.9831 - val_loss: 0.3099 - val_acc: 0.9610\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0854 - acc: 0.9817 - val_loss: 0.2840 - val_acc: 0.9652\n",
      "======== layers: 2 ; hid_dim: 512 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 29us/sample - loss: 2.9686 - acc: 0.8960 - val_loss: 0.6200 - val_acc: 0.9355\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.3428 - acc: 0.9527 - val_loss: 0.4809 - val_acc: 0.9469\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2087 - acc: 0.9648 - val_loss: 0.3668 - val_acc: 0.9529\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.1448 - acc: 0.9734 - val_loss: 0.3444 - val_acc: 0.9571\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1330 - acc: 0.9755 - val_loss: 0.3485 - val_acc: 0.9589\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1455 - acc: 0.9744 - val_loss: 0.3332 - val_acc: 0.9563\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0973 - acc: 0.9805 - val_loss: 0.3066 - val_acc: 0.9581\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0951 - acc: 0.9807 - val_loss: 0.2899 - val_acc: 0.9643\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0975 - acc: 0.9814 - val_loss: 0.3208 - val_acc: 0.9607\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0827 - acc: 0.9833 - val_loss: 0.2688 - val_acc: 0.9624\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0711 - acc: 0.9845 - val_loss: 0.2657 - val_acc: 0.9631\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0834 - acc: 0.9826 - val_loss: 0.2441 - val_acc: 0.9622\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0786 - acc: 0.9837 - val_loss: 0.3011 - val_acc: 0.9619\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0978 - acc: 0.9802 - val_loss: 0.2977 - val_acc: 0.9618\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0884 - acc: 0.9817 - val_loss: 0.2505 - val_acc: 0.9656\n",
      "======== layers: 2 ; hid_dim: 1024 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 29us/sample - loss: 3.1625 - acc: 0.9072 - val_loss: 0.4239 - val_acc: 0.9427\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.2102 - acc: 0.9623 - val_loss: 0.2804 - val_acc: 0.9557\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.1373 - acc: 0.9715 - val_loss: 0.3444 - val_acc: 0.9526\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.1368 - acc: 0.9737 - val_loss: 0.2872 - val_acc: 0.9598\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1089 - acc: 0.9763 - val_loss: 0.3128 - val_acc: 0.9554\n",
      "======== layers: 3 ; hid_dim: 1 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 2.3017 - acc: 0.1130 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3020 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3022 - val_acc: 0.1060\n",
      "======== layers: 3 ; hid_dim: 2 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 2.3017 - acc: 0.1132 - val_loss: 2.3019 - val_acc: 0.1060\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3021 - val_acc: 0.1060\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.3011 - acc: 0.1140 - val_loss: 2.3022 - val_acc: 0.1060\n",
      "======== layers: 3 ; hid_dim: 4 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 2.6552 - acc: 0.1306 - val_loss: 2.2066 - val_acc: 0.1600\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.1455 - acc: 0.1777 - val_loss: 2.1008 - val_acc: 0.1947\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0677 - acc: 0.2013 - val_loss: 2.0624 - val_acc: 0.1996\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9804 - acc: 0.2376 - val_loss: 1.8305 - val_acc: 0.2873\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.7586 - acc: 0.3002 - val_loss: 1.7008 - val_acc: 0.3169\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - ETA: 0s - loss: 1.6724 - acc: 0.323 - 1s 19us/sample - loss: 1.6720 - acc: 0.3239 - val_loss: 1.6443 - val_acc: 0.3387\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.6167 - acc: 0.3368 - val_loss: 1.6041 - val_acc: 0.3322\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.5823 - acc: 0.3375 - val_loss: 1.5799 - val_acc: 0.3575\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5606 - acc: 0.3385 - val_loss: 1.5619 - val_acc: 0.3378\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5448 - acc: 0.3420 - val_loss: 1.5493 - val_acc: 0.3386\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5352 - acc: 0.3460 - val_loss: 1.5368 - val_acc: 0.3488\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5258 - acc: 0.3532 - val_loss: 1.5398 - val_acc: 0.3458\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5161 - acc: 0.3596 - val_loss: 1.5145 - val_acc: 0.3678\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5058 - acc: 0.3655 - val_loss: 1.5153 - val_acc: 0.3735\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.4916 - acc: 0.3742 - val_loss: 1.4939 - val_acc: 0.3818\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.4702 - acc: 0.3912 - val_loss: 1.4469 - val_acc: 0.4173\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3802 - acc: 0.4570 - val_loss: 1.3297 - val_acc: 0.4837\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3024 - acc: 0.4949 - val_loss: 1.2782 - val_acc: 0.5142\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.2509 - acc: 0.5198 - val_loss: 1.2100 - val_acc: 0.5467\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.2032 - acc: 0.5305 - val_loss: 1.1678 - val_acc: 0.5525\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1717 - acc: 0.5463 - val_loss: 1.1628 - val_acc: 0.5407\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1503 - acc: 0.5526 - val_loss: 1.1299 - val_acc: 0.5802\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.1353 - acc: 0.5626 - val_loss: 1.1269 - val_acc: 0.5794\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.1219 - acc: 0.5629 - val_loss: 1.1093 - val_acc: 0.5714\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1052 - acc: 0.5627 - val_loss: 1.1058 - val_acc: 0.5648\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0919 - acc: 0.5653 - val_loss: 1.0961 - val_acc: 0.5722\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0798 - acc: 0.5750 - val_loss: 1.0778 - val_acc: 0.5865\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0711 - acc: 0.5827 - val_loss: 1.0533 - val_acc: 0.6065\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0647 - acc: 0.5998 - val_loss: 1.0426 - val_acc: 0.6075\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0520 - acc: 0.6054 - val_loss: 1.0460 - val_acc: 0.6122\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0488 - acc: 0.6068 - val_loss: 1.0474 - val_acc: 0.6089\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0429 - acc: 0.6110 - val_loss: 1.0425 - val_acc: 0.6112\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0408 - acc: 0.6117 - val_loss: 1.0529 - val_acc: 0.6198\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0377 - acc: 0.6150 - val_loss: 1.0204 - val_acc: 0.6275\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0333 - acc: 0.6189 - val_loss: 1.0277 - val_acc: 0.6201\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0307 - acc: 0.6219 - val_loss: 1.0335 - val_acc: 0.6201\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.0294 - acc: 0.6242 - val_loss: 1.0468 - val_acc: 0.6173\n",
      "======== layers: 3 ; hid_dim: 8 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 3.2031 - acc: 0.1491 - val_loss: 2.1101 - val_acc: 0.1988\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 2.0425 - acc: 0.2070 - val_loss: 1.9929 - val_acc: 0.2282\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.9175 - acc: 0.2622 - val_loss: 1.8722 - val_acc: 0.2826\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.8359 - acc: 0.2904 - val_loss: 1.8292 - val_acc: 0.2895\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7863 - acc: 0.3072 - val_loss: 1.7782 - val_acc: 0.3134\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7115 - acc: 0.3365 - val_loss: 1.6742 - val_acc: 0.3543\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.6026 - acc: 0.3833 - val_loss: 1.5320 - val_acc: 0.4008\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.4932 - acc: 0.4171 - val_loss: 1.4465 - val_acc: 0.4372\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.4145 - acc: 0.4377 - val_loss: 1.3614 - val_acc: 0.4568\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3218 - acc: 0.4641 - val_loss: 1.2605 - val_acc: 0.4992\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1981 - acc: 0.5325 - val_loss: 1.1400 - val_acc: 0.5809\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 1.0664 - acc: 0.6322 - val_loss: 0.9925 - val_acc: 0.6317\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.9308 - acc: 0.6954 - val_loss: 0.8935 - val_acc: 0.7074\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.8574 - acc: 0.7403 - val_loss: 0.8028 - val_acc: 0.7632\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7922 - acc: 0.7690 - val_loss: 0.7631 - val_acc: 0.7795\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7557 - acc: 0.7807 - val_loss: 0.7295 - val_acc: 0.7883\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7335 - acc: 0.7883 - val_loss: 0.7022 - val_acc: 0.8006\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7125 - acc: 0.7938 - val_loss: 0.7054 - val_acc: 0.7979\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.6983 - acc: 0.7976 - val_loss: 0.6888 - val_acc: 0.8008\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6845 - acc: 0.8012 - val_loss: 0.6869 - val_acc: 0.8043\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.6517 - acc: 0.8107 - val_loss: 0.6271 - val_acc: 0.8219\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.6072 - acc: 0.8224 - val_loss: 0.6074 - val_acc: 0.8261\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5841 - acc: 0.8294 - val_loss: 0.5706 - val_acc: 0.8370\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5628 - acc: 0.8378 - val_loss: 0.5550 - val_acc: 0.8429\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5459 - acc: 0.8442 - val_loss: 0.5425 - val_acc: 0.8464\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5292 - acc: 0.8486 - val_loss: 0.5282 - val_acc: 0.8500\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5219 - acc: 0.8514 - val_loss: 0.5319 - val_acc: 0.8468\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5116 - acc: 0.8531 - val_loss: 0.5110 - val_acc: 0.8558\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4990 - acc: 0.8563 - val_loss: 0.5154 - val_acc: 0.8507\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4898 - acc: 0.8586 - val_loss: 0.4967 - val_acc: 0.8591\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4809 - acc: 0.8625 - val_loss: 0.5101 - val_acc: 0.8611\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4731 - acc: 0.8643 - val_loss: 0.4884 - val_acc: 0.8620\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4663 - acc: 0.8661 - val_loss: 0.4888 - val_acc: 0.8618\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4606 - acc: 0.8666 - val_loss: 0.4769 - val_acc: 0.8652\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4545 - acc: 0.8687 - val_loss: 0.4849 - val_acc: 0.8561\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4457 - acc: 0.8703 - val_loss: 0.4720 - val_acc: 0.8642\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4434 - acc: 0.8702 - val_loss: 0.4652 - val_acc: 0.8692\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4396 - acc: 0.8724 - val_loss: 0.4692 - val_acc: 0.8714\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4366 - acc: 0.8726 - val_loss: 0.4469 - val_acc: 0.8723\n",
      "Epoch 40/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4322 - acc: 0.8731 - val_loss: 0.4516 - val_acc: 0.8712\n",
      "Epoch 41/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4293 - acc: 0.8753 - val_loss: 0.4514 - val_acc: 0.8690\n",
      "Epoch 42/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4254 - acc: 0.8752 - val_loss: 0.4518 - val_acc: 0.8708\n",
      "======== layers: 3 ; hid_dim: 16 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 25us/sample - loss: 2.9982 - acc: 0.1932 - val_loss: 1.9432 - val_acc: 0.2833\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.7823 - acc: 0.3362 - val_loss: 1.6592 - val_acc: 0.3670\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.5862 - acc: 0.3841 - val_loss: 1.5280 - val_acc: 0.4067\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.4503 - acc: 0.4176 - val_loss: 1.3880 - val_acc: 0.4331\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.3334 - acc: 0.4451 - val_loss: 1.2460 - val_acc: 0.4873\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 1.1386 - acc: 0.5385 - val_loss: 1.0333 - val_acc: 0.5897\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.9493 - acc: 0.6344 - val_loss: 0.8235 - val_acc: 0.6885\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7914 - acc: 0.7034 - val_loss: 0.7302 - val_acc: 0.7331\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.7103 - acc: 0.7353 - val_loss: 0.6632 - val_acc: 0.7466\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6592 - acc: 0.7488 - val_loss: 0.6344 - val_acc: 0.7602\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6267 - acc: 0.7595 - val_loss: 0.6066 - val_acc: 0.7767\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5757 - acc: 0.7869 - val_loss: 0.5406 - val_acc: 0.7943\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.5418 - acc: 0.7919 - val_loss: 0.5299 - val_acc: 0.8010\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5228 - acc: 0.7975 - val_loss: 0.5058 - val_acc: 0.8067\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5090 - acc: 0.7996 - val_loss: 0.5065 - val_acc: 0.8077\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4988 - acc: 0.8047 - val_loss: 0.4928 - val_acc: 0.8084\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4835 - acc: 0.8109 - val_loss: 0.4860 - val_acc: 0.8116\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.4586 - acc: 0.8382 - val_loss: 0.3947 - val_acc: 0.8982\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3642 - acc: 0.8999 - val_loss: 0.3575 - val_acc: 0.9047\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3436 - acc: 0.9053 - val_loss: 0.3530 - val_acc: 0.9076\n",
      "Epoch 21/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3269 - acc: 0.9094 - val_loss: 0.3288 - val_acc: 0.9112\n",
      "Epoch 22/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3150 - acc: 0.9130 - val_loss: 0.3424 - val_acc: 0.9096\n",
      "Epoch 23/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3100 - acc: 0.9136 - val_loss: 0.3340 - val_acc: 0.9097\n",
      "Epoch 24/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3051 - acc: 0.9143 - val_loss: 0.3174 - val_acc: 0.9153\n",
      "Epoch 25/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2990 - acc: 0.9158 - val_loss: 0.3324 - val_acc: 0.9103\n",
      "Epoch 26/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2960 - acc: 0.9167 - val_loss: 0.3093 - val_acc: 0.9163\n",
      "Epoch 27/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2862 - acc: 0.9201 - val_loss: 0.3127 - val_acc: 0.9170\n",
      "Epoch 28/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2827 - acc: 0.9206 - val_loss: 0.3170 - val_acc: 0.9113\n",
      "Epoch 29/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2810 - acc: 0.9206 - val_loss: 0.2998 - val_acc: 0.9194\n",
      "Epoch 30/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2719 - acc: 0.9233 - val_loss: 0.2982 - val_acc: 0.9190\n",
      "Epoch 31/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2636 - acc: 0.9256 - val_loss: 0.3074 - val_acc: 0.9144\n",
      "Epoch 32/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2547 - acc: 0.9275 - val_loss: 0.2908 - val_acc: 0.9232\n",
      "Epoch 33/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2538 - acc: 0.9284 - val_loss: 0.2915 - val_acc: 0.9238\n",
      "Epoch 34/100\n",
      "48000/48000 [==============================] - 1s 18us/sample - loss: 0.2503 - acc: 0.9291 - val_loss: 0.3066 - val_acc: 0.9215\n",
      "Epoch 35/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2459 - acc: 0.9304 - val_loss: 0.2881 - val_acc: 0.9226\n",
      "Epoch 36/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2413 - acc: 0.9306 - val_loss: 0.2792 - val_acc: 0.9249\n",
      "Epoch 37/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2399 - acc: 0.9317 - val_loss: 0.2821 - val_acc: 0.9242\n",
      "Epoch 38/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2362 - acc: 0.9324 - val_loss: 0.2825 - val_acc: 0.9227\n",
      "Epoch 39/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2299 - acc: 0.9342 - val_loss: 0.2827 - val_acc: 0.9243\n",
      "======== layers: 3 ; hid_dim: 32 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 2.8379 - acc: 0.4228 - val_loss: 1.1189 - val_acc: 0.6439\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.9002 - acc: 0.7102 - val_loss: 0.6973 - val_acc: 0.7718\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.6353 - acc: 0.8036 - val_loss: 0.5678 - val_acc: 0.8300\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.5243 - acc: 0.8481 - val_loss: 0.4712 - val_acc: 0.8717\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.4525 - acc: 0.8710 - val_loss: 0.4371 - val_acc: 0.8791\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3994 - acc: 0.8861 - val_loss: 0.4013 - val_acc: 0.8933\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3700 - acc: 0.8945 - val_loss: 0.3721 - val_acc: 0.8978\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3416 - acc: 0.9013 - val_loss: 0.3400 - val_acc: 0.9074\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3261 - acc: 0.9067 - val_loss: 0.3412 - val_acc: 0.9082\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3081 - acc: 0.9101 - val_loss: 0.3258 - val_acc: 0.9059\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2925 - acc: 0.9155 - val_loss: 0.3267 - val_acc: 0.9127\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2831 - acc: 0.9179 - val_loss: 0.3077 - val_acc: 0.9158\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2693 - acc: 0.9202 - val_loss: 0.2967 - val_acc: 0.9223\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2633 - acc: 0.9237 - val_loss: 0.3240 - val_acc: 0.9144\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2532 - acc: 0.9267 - val_loss: 0.3145 - val_acc: 0.9180\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2425 - acc: 0.9301 - val_loss: 0.2982 - val_acc: 0.9171\n",
      "======== layers: 3 ; hid_dim: 64 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 1.7864 - acc: 0.7975 - val_loss: 0.4938 - val_acc: 0.8871\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.3695 - acc: 0.9103 - val_loss: 0.3309 - val_acc: 0.9174\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.2407 - acc: 0.9341 - val_loss: 0.2637 - val_acc: 0.9317\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1878 - acc: 0.9471 - val_loss: 0.2366 - val_acc: 0.9392\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1596 - acc: 0.9542 - val_loss: 0.2179 - val_acc: 0.9424\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1334 - acc: 0.9609 - val_loss: 0.2247 - val_acc: 0.9429\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1205 - acc: 0.9634 - val_loss: 0.2150 - val_acc: 0.9488\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1132 - acc: 0.9661 - val_loss: 0.1985 - val_acc: 0.9517\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1032 - acc: 0.9691 - val_loss: 0.1950 - val_acc: 0.9523\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0950 - acc: 0.9710 - val_loss: 0.2041 - val_acc: 0.9496\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0909 - acc: 0.9722 - val_loss: 0.1895 - val_acc: 0.9544\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0833 - acc: 0.9749 - val_loss: 0.1995 - val_acc: 0.9533\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0825 - acc: 0.9747 - val_loss: 0.1998 - val_acc: 0.9547\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0687 - acc: 0.9788 - val_loss: 0.1719 - val_acc: 0.9592\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0617 - acc: 0.9809 - val_loss: 0.1849 - val_acc: 0.9577\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0683 - acc: 0.9793 - val_loss: 0.2032 - val_acc: 0.9588\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0650 - acc: 0.9802 - val_loss: 0.2142 - val_acc: 0.9562\n",
      "======== layers: 3 ; hid_dim: 128 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 1.6711 - acc: 0.8477 - val_loss: 0.4441 - val_acc: 0.9128\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.3282 - acc: 0.9265 - val_loss: 0.3281 - val_acc: 0.9252\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2023 - acc: 0.9488 - val_loss: 0.2595 - val_acc: 0.9433\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1398 - acc: 0.9609 - val_loss: 0.2363 - val_acc: 0.9468\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1124 - acc: 0.9678 - val_loss: 0.2300 - val_acc: 0.9503\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0905 - acc: 0.9729 - val_loss: 0.2305 - val_acc: 0.9495\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1005 - acc: 0.9705 - val_loss: 0.2383 - val_acc: 0.9525\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1031 - acc: 0.9713 - val_loss: 0.2195 - val_acc: 0.9550\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0832 - acc: 0.9770 - val_loss: 0.2058 - val_acc: 0.9592\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0703 - acc: 0.9788 - val_loss: 0.2080 - val_acc: 0.9576\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0820 - acc: 0.9764 - val_loss: 0.1892 - val_acc: 0.9639\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0743 - acc: 0.9785 - val_loss: 0.2187 - val_acc: 0.9584\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0609 - acc: 0.9820 - val_loss: 0.1849 - val_acc: 0.9648\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0554 - acc: 0.9839 - val_loss: 0.1890 - val_acc: 0.9640\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0520 - acc: 0.9843 - val_loss: 0.1804 - val_acc: 0.9672\n",
      "Epoch 16/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0505 - acc: 0.9850 - val_loss: 0.2062 - val_acc: 0.9653\n",
      "Epoch 17/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0598 - acc: 0.9829 - val_loss: 0.1725 - val_acc: 0.9654\n",
      "Epoch 18/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0408 - acc: 0.9875 - val_loss: 0.1984 - val_acc: 0.9679\n",
      "Epoch 19/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0419 - acc: 0.9882 - val_loss: 0.1846 - val_acc: 0.9696\n",
      "Epoch 20/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0460 - acc: 0.9864 - val_loss: 0.2238 - val_acc: 0.9643\n",
      "======== layers: 3 ; hid_dim: 256 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 26us/sample - loss: 1.7435 - acc: 0.8737 - val_loss: 0.4184 - val_acc: 0.9280\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.2661 - acc: 0.9432 - val_loss: 0.3079 - val_acc: 0.9422\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1556 - acc: 0.9607 - val_loss: 0.3154 - val_acc: 0.9390\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.1294 - acc: 0.9678 - val_loss: 0.2570 - val_acc: 0.9490\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.1034 - acc: 0.9724 - val_loss: 0.3176 - val_acc: 0.9413\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0964 - acc: 0.9745 - val_loss: 0.2607 - val_acc: 0.9525\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0925 - acc: 0.9757 - val_loss: 0.2212 - val_acc: 0.9603\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0892 - acc: 0.9773 - val_loss: 0.2409 - val_acc: 0.9553\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0806 - acc: 0.9782 - val_loss: 0.2360 - val_acc: 0.9588\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 19us/sample - loss: 0.0770 - acc: 0.9807 - val_loss: 0.2212 - val_acc: 0.9607\n",
      "======== layers: 3 ; hid_dim: 512 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 1s 28us/sample - loss: 2.0532 - acc: 0.8879 - val_loss: 0.2731 - val_acc: 0.9369\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.1656 - acc: 0.9564 - val_loss: 0.2106 - val_acc: 0.9486\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.1106 - acc: 0.9688 - val_loss: 0.2194 - val_acc: 0.9542\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0896 - acc: 0.9748 - val_loss: 0.1994 - val_acc: 0.9593\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0895 - acc: 0.9765 - val_loss: 0.1816 - val_acc: 0.9616\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0903 - acc: 0.9762 - val_loss: 0.2300 - val_acc: 0.9552\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0784 - acc: 0.9782 - val_loss: 0.1972 - val_acc: 0.9621\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0832 - acc: 0.9781 - val_loss: 0.1815 - val_acc: 0.9615\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0676 - acc: 0.9813 - val_loss: 0.1741 - val_acc: 0.9638\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0691 - acc: 0.9813 - val_loss: 0.2329 - val_acc: 0.9603\n",
      "Epoch 11/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0569 - acc: 0.9840 - val_loss: 0.1729 - val_acc: 0.9655\n",
      "Epoch 12/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0619 - acc: 0.9841 - val_loss: 0.1444 - val_acc: 0.9710\n",
      "Epoch 13/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0567 - acc: 0.9850 - val_loss: 0.1919 - val_acc: 0.9654\n",
      "Epoch 14/100\n",
      "48000/48000 [==============================] - 1s 21us/sample - loss: 0.0628 - acc: 0.9832 - val_loss: 0.1725 - val_acc: 0.9692\n",
      "Epoch 15/100\n",
      "48000/48000 [==============================] - 1s 20us/sample - loss: 0.0466 - acc: 0.9872 - val_loss: 0.1845 - val_acc: 0.9649\n",
      "======== layers: 3 ; hid_dim: 1024 ========\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/100\n",
      "48000/48000 [==============================] - 2s 32us/sample - loss: 2.6175 - acc: 0.8987 - val_loss: 0.2100 - val_acc: 0.9429\n",
      "Epoch 2/100\n",
      "48000/48000 [==============================] - 1s 23us/sample - loss: 0.1208 - acc: 0.9655 - val_loss: 0.1827 - val_acc: 0.9557\n",
      "Epoch 3/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.0928 - acc: 0.9726 - val_loss: 0.1743 - val_acc: 0.9607\n",
      "Epoch 4/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0916 - acc: 0.9744 - val_loss: 0.2168 - val_acc: 0.9517\n",
      "Epoch 5/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0883 - acc: 0.9754 - val_loss: 0.1749 - val_acc: 0.9582\n",
      "Epoch 6/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.0767 - acc: 0.9790 - val_loss: 0.1545 - val_acc: 0.9662\n",
      "Epoch 7/100\n",
      "48000/48000 [==============================] - 1s 24us/sample - loss: 0.0632 - acc: 0.9825 - val_loss: 0.1350 - val_acc: 0.9710\n",
      "Epoch 8/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0673 - acc: 0.9819 - val_loss: 0.1596 - val_acc: 0.9651\n",
      "Epoch 9/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0663 - acc: 0.9822 - val_loss: 0.1573 - val_acc: 0.9681\n",
      "Epoch 10/100\n",
      "48000/48000 [==============================] - 1s 22us/sample - loss: 0.0554 - acc: 0.9845 - val_loss: 0.1455 - val_acc: 0.9707\n"
     ]
    }
   ],
   "source": [
    "dim_hidden_layres = [2**i for i in range(11)]\n",
    "n_layers = range(1, 4)\n",
    "\n",
    "df_accuracy = pd.DataFrame()\n",
    "\n",
    "for layers in n_layers:\n",
    "    for hid_dim in dim_hidden_layres:\n",
    "        print('========', 'layers:', layers, '; hid_dim:', hid_dim, '========')\n",
    "        model = DenseModel(layers=layers, hid_dim=hid_dim)\n",
    "        model = model.build()\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n",
    "        callbacks = [\n",
    "            EarlyStopping(patience=3),\n",
    "            ModelCheckpoint(filepath=os.path.join('models', 'DNN', 'model_{}_{}.h5'.format(layers, hid_dim)), save_best_only=True),\n",
    "        ]\n",
    "        model.fit(x=x_train, y=y_train, batch_size=128, epochs=100, callbacks=callbacks, validation_split=0.2)\n",
    "        acc = accuracy_score(y_test, model.predict(x_test).argmax(axis=-1))\n",
    "        \n",
    "        df_accuracy = pd.concat([df_accuracy, pd.DataFrame([[layers, hid_dim, acc]], columns=['layers', 'hid_dim', 'accuracy'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"11\" halign=\"left\">accuracy</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hid_dim</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>4</th>\n",
       "      <th>8</th>\n",
       "      <th>16</th>\n",
       "      <th>32</th>\n",
       "      <th>64</th>\n",
       "      <th>128</th>\n",
       "      <th>256</th>\n",
       "      <th>512</th>\n",
       "      <th>1024</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2991</td>\n",
       "      <td>0.6508</td>\n",
       "      <td>0.1134</td>\n",
       "      <td>0.8459</td>\n",
       "      <td>0.9019</td>\n",
       "      <td>0.9325</td>\n",
       "      <td>0.9496</td>\n",
       "      <td>0.9499</td>\n",
       "      <td>0.9633</td>\n",
       "      <td>0.9680</td>\n",
       "      <td>0.9636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.1136</td>\n",
       "      <td>0.8395</td>\n",
       "      <td>0.9081</td>\n",
       "      <td>0.9303</td>\n",
       "      <td>0.9421</td>\n",
       "      <td>0.9497</td>\n",
       "      <td>0.9634</td>\n",
       "      <td>0.9659</td>\n",
       "      <td>0.9577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>0.6127</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>0.9253</td>\n",
       "      <td>0.9118</td>\n",
       "      <td>0.9566</td>\n",
       "      <td>0.9649</td>\n",
       "      <td>0.9595</td>\n",
       "      <td>0.9666</td>\n",
       "      <td>0.9724</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        accuracy                                                          \\\n",
       "hid_dim     1       2       4       8       16      32      64      128    \n",
       "layers                                                                     \n",
       "1         0.2991  0.6508  0.1134  0.8459  0.9019  0.9325  0.9496  0.9499   \n",
       "2         0.1135  0.1136  0.1136  0.8395  0.9081  0.9303  0.9421  0.9497   \n",
       "3         0.1135  0.1135  0.6127  0.8702  0.9253  0.9118  0.9566  0.9649   \n",
       "\n",
       "                                 \n",
       "hid_dim    256     512     1024  \n",
       "layers                           \n",
       "1        0.9633  0.9680  0.9636  \n",
       "2        0.9634  0.9659  0.9577  \n",
       "3        0.9595  0.9666  0.9724  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_accuracy.set_index(['layers', 'hid_dim']).unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
